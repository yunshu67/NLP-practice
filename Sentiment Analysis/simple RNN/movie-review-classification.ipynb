{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def3d94e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:33.514780Z",
     "iopub.status.busy": "2022-03-07T23:11:33.514119Z",
     "iopub.status.idle": "2022-03-07T23:11:34.278117Z",
     "shell.execute_reply": "2022-03-07T23:11:34.277438Z",
     "shell.execute_reply.started": "2022-03-07T22:00:26.071243Z"
    },
    "papermill": {
     "duration": 0.799937,
     "end_time": "2022-03-07T23:11:34.278291",
     "exception": false,
     "start_time": "2022-03-07T23:11:33.478354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%ls\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b8f43",
   "metadata": {
    "papermill": {
     "duration": 0.026255,
     "end_time": "2022-03-07T23:11:34.332046",
     "exception": false,
     "start_time": "2022-03-07T23:11:34.305791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1.读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08656d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:34.390498Z",
     "iopub.status.busy": "2022-03-07T23:11:34.389733Z",
     "iopub.status.idle": "2022-03-07T23:11:35.036371Z",
     "shell.execute_reply": "2022-03-07T23:11:35.035612Z",
     "shell.execute_reply.started": "2022-03-07T22:00:26.899196Z"
    },
    "papermill": {
     "duration": 0.678026,
     "end_time": "2022-03-07T23:11:35.036512",
     "exception": false,
     "start_time": "2022-03-07T23:11:34.358486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../input/movie-review-data/reviews.txt\") as f:\n",
    "    reviews = f.read()\n",
    "    \n",
    "with open(\"../input/movie-review-data/labels.txt\") as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8b9cf",
   "metadata": {
    "papermill": {
     "duration": 0.025529,
     "end_time": "2022-03-07T23:11:35.089392",
     "exception": false,
     "start_time": "2022-03-07T23:11:35.063863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.数据预处理和序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6b7240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:35.149020Z",
     "iopub.status.busy": "2022-03-07T23:11:35.148280Z",
     "iopub.status.idle": "2022-03-07T23:11:35.151245Z",
     "shell.execute_reply": "2022-03-07T23:11:35.150677Z",
     "shell.execute_reply.started": "2022-03-07T22:00:27.360152Z"
    },
    "papermill": {
     "duration": 0.035783,
     "end_time": "2022-03-07T23:11:35.151382",
     "exception": false,
     "start_time": "2022-03-07T23:11:35.115599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(w):\n",
    "    # w = unicode_to_ascii(w.lower().strip())\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Záéíóúâêôãõàèìò?.!,¿\\n]+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "    \n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43964719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:35.208705Z",
     "iopub.status.busy": "2022-03-07T23:11:35.208057Z",
     "iopub.status.idle": "2022-03-07T23:11:43.771111Z",
     "shell.execute_reply": "2022-03-07T23:11:43.770495Z",
     "shell.execute_reply.started": "2022-03-07T22:00:27.367090Z"
    },
    "papermill": {
     "duration": 8.592569,
     "end_time": "2022-03-07T23:11:43.771262",
     "exception": false,
     "start_time": "2022-03-07T23:11:35.178693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews = preprocess(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fca1fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:43.830716Z",
     "iopub.status.busy": "2022-03-07T23:11:43.830063Z",
     "iopub.status.idle": "2022-03-07T23:11:43.836496Z",
     "shell.execute_reply": "2022-03-07T23:11:43.836991Z",
     "shell.execute_reply.started": "2022-03-07T22:00:32.481492Z"
    },
    "papermill": {
     "duration": 0.036597,
     "end_time": "2022-03-07T23:11:43.837147",
     "exception": false,
     "start_time": "2022-03-07T23:11:43.800550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02850f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:43.893768Z",
     "iopub.status.busy": "2022-03-07T23:11:43.893102Z",
     "iopub.status.idle": "2022-03-07T23:11:45.382351Z",
     "shell.execute_reply": "2022-03-07T23:11:45.382935Z",
     "shell.execute_reply.started": "2022-03-07T22:00:32.491916Z"
    },
    "papermill": {
     "duration": 1.519531,
     "end_time": "2022-03-07T23:11:45.383126",
     "exception": false,
     "start_time": "2022-03-07T23:11:43.863595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76638\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words = reviews.split(' ')\n",
    "word_count = Counter(words)\n",
    "vocab_size = len(word_count)+2 # 1 for unknow word and 1 for padding\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7278503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:45.447938Z",
     "iopub.status.busy": "2022-03-07T23:11:45.446927Z",
     "iopub.status.idle": "2022-03-07T23:11:45.451426Z",
     "shell.execute_reply": "2022-03-07T23:11:45.450938Z",
     "shell.execute_reply.started": "2022-03-07T22:00:33.854115Z"
    },
    "papermill": {
     "duration": 0.039543,
     "end_time": "2022-03-07T23:11:45.451575",
     "exception": false,
     "start_time": "2022-03-07T23:11:45.412032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'a', 'cartoon']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(word_count.keys())\n",
    "keys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4617ba3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:45.510127Z",
     "iopub.status.busy": "2022-03-07T23:11:45.509098Z",
     "iopub.status.idle": "2022-03-07T23:11:45.572350Z",
     "shell.execute_reply": "2022-03-07T23:11:45.572865Z",
     "shell.execute_reply.started": "2022-03-07T22:00:33.962198Z"
    },
    "papermill": {
     "duration": 0.094365,
     "end_time": "2022-03-07T23:11:45.573035",
     "exception": false,
     "start_time": "2022-03-07T23:11:45.478670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bromwell': 1,\n",
       " 'high': 2,\n",
       " 'is': 3,\n",
       " 'a': 4,\n",
       " 'cartoon': 5,\n",
       " 'comedy': 6,\n",
       " '.': 7,\n",
       " 'it': 8,\n",
       " 'ran': 9,\n",
       " 'at': 10,\n",
       " 'the': 11,\n",
       " 'same': 12,\n",
       " 'time': 13,\n",
       " 'as': 14,\n",
       " 'some': 15,\n",
       " 'other': 16,\n",
       " 'programs': 17,\n",
       " 'about': 18,\n",
       " 'school': 19,\n",
       " 'life': 20,\n",
       " 'such': 21,\n",
       " 'teachers': 22,\n",
       " 'my': 23,\n",
       " 'years': 24,\n",
       " 'in': 25,\n",
       " 'teaching': 26,\n",
       " 'profession': 27,\n",
       " 'lead': 28,\n",
       " 'me': 29,\n",
       " 'to': 30,\n",
       " 'believe': 31,\n",
       " 'that': 32,\n",
       " 's': 33,\n",
       " 'satire': 34,\n",
       " 'much': 35,\n",
       " 'closer': 36,\n",
       " 'reality': 37,\n",
       " 'than': 38,\n",
       " 'scramble': 39,\n",
       " 'survive': 40,\n",
       " 'financially': 41,\n",
       " 'insightful': 42,\n",
       " 'students': 43,\n",
       " 'who': 44,\n",
       " 'can': 45,\n",
       " 'see': 46,\n",
       " 'right': 47,\n",
       " 'through': 48,\n",
       " 'their': 49,\n",
       " 'pathetic': 50,\n",
       " 'pomp': 51,\n",
       " 'pettiness': 52,\n",
       " 'of': 53,\n",
       " 'whole': 54,\n",
       " 'situation': 55,\n",
       " 'all': 56,\n",
       " 'remind': 57,\n",
       " 'schools': 58,\n",
       " 'i': 59,\n",
       " 'knew': 60,\n",
       " 'and': 61,\n",
       " 'when': 62,\n",
       " 'saw': 63,\n",
       " 'episode': 64,\n",
       " 'which': 65,\n",
       " 'student': 66,\n",
       " 'repeatedly': 67,\n",
       " 'tried': 68,\n",
       " 'burn': 69,\n",
       " 'down': 70,\n",
       " 'immediately': 71,\n",
       " 'recalled': 72,\n",
       " 'classic': 73,\n",
       " 'line': 74,\n",
       " 'inspector': 75,\n",
       " 'm': 76,\n",
       " 'here': 77,\n",
       " 'sack': 78,\n",
       " 'one': 79,\n",
       " 'your': 80,\n",
       " 'welcome': 81,\n",
       " 'expect': 82,\n",
       " 'many': 83,\n",
       " 'adults': 84,\n",
       " 'age': 85,\n",
       " 'think': 86,\n",
       " 'far': 87,\n",
       " 'fetched': 88,\n",
       " 'what': 89,\n",
       " 'pity': 90,\n",
       " 'isn': 91,\n",
       " 't': 92,\n",
       " '\\nstory': 93,\n",
       " 'man': 94,\n",
       " 'has': 95,\n",
       " 'unnatural': 96,\n",
       " 'feelings': 97,\n",
       " 'for': 98,\n",
       " 'pig': 99,\n",
       " 'starts': 100,\n",
       " 'out': 101,\n",
       " 'with': 102,\n",
       " 'opening': 103,\n",
       " 'scene': 104,\n",
       " 'terrific': 105,\n",
       " 'example': 106,\n",
       " 'absurd': 107,\n",
       " 'formal': 108,\n",
       " 'orchestra': 109,\n",
       " 'audience': 110,\n",
       " 'turned': 111,\n",
       " 'into': 112,\n",
       " 'an': 113,\n",
       " 'insane': 114,\n",
       " 'violent': 115,\n",
       " 'mob': 116,\n",
       " 'by': 117,\n",
       " 'crazy': 118,\n",
       " 'chantings': 119,\n",
       " 'singers': 120,\n",
       " 'unfortunately': 121,\n",
       " 'stays': 122,\n",
       " 'no': 123,\n",
       " 'general': 124,\n",
       " 'narrative': 125,\n",
       " 'eventually': 126,\n",
       " 'making': 127,\n",
       " 'just': 128,\n",
       " 'too': 129,\n",
       " 'off': 130,\n",
       " 'putting': 131,\n",
       " 'even': 132,\n",
       " 'those': 133,\n",
       " 'from': 134,\n",
       " 'era': 135,\n",
       " 'should': 136,\n",
       " 'be': 137,\n",
       " 'cryptic': 138,\n",
       " 'dialogue': 139,\n",
       " 'would': 140,\n",
       " 'make': 141,\n",
       " 'shakespeare': 142,\n",
       " 'seem': 143,\n",
       " 'easy': 144,\n",
       " 'third': 145,\n",
       " 'grader': 146,\n",
       " 'on': 147,\n",
       " 'technical': 148,\n",
       " 'level': 149,\n",
       " 'better': 150,\n",
       " 'you': 151,\n",
       " 'might': 152,\n",
       " 'good': 153,\n",
       " 'cinematography': 154,\n",
       " 'future': 155,\n",
       " 'great': 156,\n",
       " 'vilmos': 157,\n",
       " 'zsigmond': 158,\n",
       " 'stars': 159,\n",
       " 'sally': 160,\n",
       " 'kirkland': 161,\n",
       " 'frederic': 162,\n",
       " 'forrest': 163,\n",
       " 'seen': 164,\n",
       " 'briefly': 165,\n",
       " '\\nhomelessness': 166,\n",
       " 'or': 167,\n",
       " 'houselessness': 168,\n",
       " 'george': 169,\n",
       " 'carlin': 170,\n",
       " 'stated': 171,\n",
       " 'been': 172,\n",
       " 'issue': 173,\n",
       " 'but': 174,\n",
       " 'never': 175,\n",
       " 'plan': 176,\n",
       " 'help': 177,\n",
       " 'street': 178,\n",
       " 'were': 179,\n",
       " 'once': 180,\n",
       " 'considered': 181,\n",
       " 'human': 182,\n",
       " 'did': 183,\n",
       " 'everything': 184,\n",
       " 'going': 185,\n",
       " 'work': 186,\n",
       " 'vote': 187,\n",
       " 'matter': 188,\n",
       " 'most': 189,\n",
       " 'people': 190,\n",
       " 'homeless': 191,\n",
       " 'lost': 192,\n",
       " 'cause': 193,\n",
       " 'while': 194,\n",
       " 'worrying': 195,\n",
       " 'things': 196,\n",
       " 'racism': 197,\n",
       " 'war': 198,\n",
       " 'iraq': 199,\n",
       " 'pressuring': 200,\n",
       " 'kids': 201,\n",
       " 'succeed': 202,\n",
       " 'technology': 203,\n",
       " 'elections': 204,\n",
       " 'inflation': 205,\n",
       " 'if': 206,\n",
       " 'they': 207,\n",
       " 'll': 208,\n",
       " 'next': 209,\n",
       " 'end': 210,\n",
       " 'up': 211,\n",
       " 'streets': 212,\n",
       " 'br': 213,\n",
       " 'given': 214,\n",
       " 'bet': 215,\n",
       " 'live': 216,\n",
       " 'month': 217,\n",
       " 'without': 218,\n",
       " 'luxuries': 219,\n",
       " 'had': 220,\n",
       " 'home': 221,\n",
       " 'entertainment': 222,\n",
       " 'sets': 223,\n",
       " 'bathroom': 224,\n",
       " 'pictures': 225,\n",
       " 'wall': 226,\n",
       " 'computer': 227,\n",
       " 'treasure': 228,\n",
       " 'like': 229,\n",
       " 'goddard': 230,\n",
       " 'bolt': 231,\n",
       " 'lesson': 232,\n",
       " 'mel': 233,\n",
       " 'brooks': 234,\n",
       " 'directs': 235,\n",
       " 'plays': 236,\n",
       " 'rich': 237,\n",
       " 'world': 238,\n",
       " 'until': 239,\n",
       " 'deciding': 240,\n",
       " 'sissy': 241,\n",
       " 'rival': 242,\n",
       " 'jeffery': 243,\n",
       " 'tambor': 244,\n",
       " 'he': 245,\n",
       " 'thirty': 246,\n",
       " 'days': 247,\n",
       " 'succeeds': 248,\n",
       " 'do': 249,\n",
       " 'wants': 250,\n",
       " 'project': 251,\n",
       " 'more': 252,\n",
       " 'buildings': 253,\n",
       " 'where': 254,\n",
       " 'thrown': 255,\n",
       " 'bracelet': 256,\n",
       " 'his': 257,\n",
       " 'leg': 258,\n",
       " 'monitor': 259,\n",
       " 'every': 260,\n",
       " 'move': 261,\n",
       " 'step': 262,\n",
       " 'sidewalk': 263,\n",
       " 'nickname': 264,\n",
       " 'pepto': 265,\n",
       " 'vagrant': 266,\n",
       " 'after': 267,\n",
       " 'written': 268,\n",
       " 'forehead': 269,\n",
       " 'meets': 270,\n",
       " 'characters': 271,\n",
       " 'including': 272,\n",
       " 'woman': 273,\n",
       " 'name': 274,\n",
       " 'molly': 275,\n",
       " 'lesley': 276,\n",
       " 'ann': 277,\n",
       " 'warren': 278,\n",
       " 'ex': 279,\n",
       " 'dancer': 280,\n",
       " 'got': 281,\n",
       " 'divorce': 282,\n",
       " 'before': 283,\n",
       " 'losing': 284,\n",
       " 'her': 285,\n",
       " 'pals': 286,\n",
       " 'sailor': 287,\n",
       " 'howard': 288,\n",
       " 'morris': 289,\n",
       " 'fumes': 290,\n",
       " 'teddy': 291,\n",
       " 'wilson': 292,\n",
       " 'are': 293,\n",
       " 'already': 294,\n",
       " 'used': 295,\n",
       " 're': 296,\n",
       " 'survivors': 297,\n",
       " 'not': 298,\n",
       " 'reaching': 299,\n",
       " 'mutual': 300,\n",
       " 'agreements': 301,\n",
       " 'being': 302,\n",
       " 'fight': 303,\n",
       " 'flight': 304,\n",
       " 'kill': 305,\n",
       " 'killed': 306,\n",
       " 'love': 307,\n",
       " 'connection': 308,\n",
       " 'between': 309,\n",
       " 'wasn': 310,\n",
       " 'necessary': 311,\n",
       " 'plot': 312,\n",
       " 'found': 313,\n",
       " 'stinks': 314,\n",
       " 'observant': 315,\n",
       " 'films': 316,\n",
       " 'prior': 317,\n",
       " 'shows': 318,\n",
       " 'tender': 319,\n",
       " 'side': 320,\n",
       " 'compared': 321,\n",
       " 'slapstick': 322,\n",
       " 'blazing': 323,\n",
       " 'saddles': 324,\n",
       " 'young': 325,\n",
       " 'frankenstein': 326,\n",
       " 'spaceballs': 327,\n",
       " 'show': 328,\n",
       " 'having': 329,\n",
       " 'something': 330,\n",
       " 'valuable': 331,\n",
       " 'day': 332,\n",
       " 'hand': 333,\n",
       " 'stupid': 334,\n",
       " 'don': 335,\n",
       " 'know': 336,\n",
       " 'money': 337,\n",
       " 'maybe': 338,\n",
       " 'give': 339,\n",
       " 'instead': 340,\n",
       " 'using': 341,\n",
       " 'monopoly': 342,\n",
       " 'this': 343,\n",
       " 'film': 344,\n",
       " 'will': 345,\n",
       " 'inspire': 346,\n",
       " 'others': 347,\n",
       " '\\nairport': 348,\n",
       " 'brand': 349,\n",
       " 'new': 350,\n",
       " 'luxury': 351,\n",
       " 'plane': 352,\n",
       " 'loaded': 353,\n",
       " 'paintings': 354,\n",
       " 'belonging': 355,\n",
       " 'businessman': 356,\n",
       " 'philip': 357,\n",
       " 'stevens': 358,\n",
       " 'james': 359,\n",
       " 'stewart': 360,\n",
       " 'flying': 361,\n",
       " 'them': 362,\n",
       " 'bunch': 363,\n",
       " 'vip': 364,\n",
       " 'estate': 365,\n",
       " 'preparation': 366,\n",
       " 'opened': 367,\n",
       " 'public': 368,\n",
       " 'museum': 369,\n",
       " 'also': 370,\n",
       " 'board': 371,\n",
       " 'daughter': 372,\n",
       " 'julie': 373,\n",
       " 'kathleen': 374,\n",
       " 'quinlan': 375,\n",
       " 'son': 376,\n",
       " 'jetliner': 377,\n",
       " 'takes': 378,\n",
       " 'planned': 379,\n",
       " 'mid': 380,\n",
       " 'air': 381,\n",
       " 'hi': 382,\n",
       " 'jacked': 383,\n",
       " 'co': 384,\n",
       " 'pilot': 385,\n",
       " 'chambers': 386,\n",
       " 'robert': 387,\n",
       " 'foxworth': 388,\n",
       " 'two': 389,\n",
       " 'accomplice': 390,\n",
       " 'banker': 391,\n",
       " 'monte': 392,\n",
       " 'markham': 393,\n",
       " 'michael': 394,\n",
       " 'pataki': 395,\n",
       " 'knock': 396,\n",
       " 'passengers': 397,\n",
       " 'crew': 398,\n",
       " 'sleeping': 399,\n",
       " 'gas': 400,\n",
       " 'steal': 401,\n",
       " 'cargo': 402,\n",
       " 'land': 403,\n",
       " 'disused': 404,\n",
       " 'strip': 405,\n",
       " 'isolated': 406,\n",
       " 'island': 407,\n",
       " 'descent': 408,\n",
       " 'almost': 409,\n",
       " 'hits': 410,\n",
       " 'oil': 411,\n",
       " 'rig': 412,\n",
       " 'ocean': 413,\n",
       " 'loses': 414,\n",
       " 'control': 415,\n",
       " 'sending': 416,\n",
       " 'crashing': 417,\n",
       " 'sea': 418,\n",
       " 'sinks': 419,\n",
       " 'bottom': 420,\n",
       " 'bang': 421,\n",
       " 'middle': 422,\n",
       " 'bermuda': 423,\n",
       " 'triangle': 424,\n",
       " 'short': 425,\n",
       " 'supply': 426,\n",
       " 'water': 427,\n",
       " 'leaking': 428,\n",
       " 'flown': 429,\n",
       " 'over': 430,\n",
       " 'miles': 431,\n",
       " 'course': 432,\n",
       " 'problems': 433,\n",
       " 'mount': 434,\n",
       " 'survivor': 435,\n",
       " 'await': 436,\n",
       " 'fast': 437,\n",
       " 'running': 438,\n",
       " 'known': 439,\n",
       " 'under': 440,\n",
       " 'slightly': 441,\n",
       " 'different': 442,\n",
       " 'tile': 443,\n",
       " 'airport': 444,\n",
       " 'second': 445,\n",
       " 'sequel': 446,\n",
       " 'smash': 447,\n",
       " 'hit': 448,\n",
       " 'disaster': 449,\n",
       " 'thriller': 450,\n",
       " 'was': 451,\n",
       " 'directed': 452,\n",
       " 'jerry': 453,\n",
       " 'jameson': 454,\n",
       " 'again': 455,\n",
       " 'predecessors': 456,\n",
       " 'say': 457,\n",
       " 'any': 458,\n",
       " 'sort': 459,\n",
       " 'forgotten': 460,\n",
       " 'entertaining': 461,\n",
       " 'although': 462,\n",
       " 'necessarily': 463,\n",
       " 'reasons': 464,\n",
       " 'three': 465,\n",
       " 'have': 466,\n",
       " 'so': 467,\n",
       " 'actually': 468,\n",
       " 'liked': 469,\n",
       " 'best': 470,\n",
       " 'favourite': 471,\n",
       " 'nice': 472,\n",
       " 'jacking': 473,\n",
       " 'then': 474,\n",
       " 'didn': 475,\n",
       " 'sinking': 476,\n",
       " 'makers': 477,\n",
       " 'trying': 478,\n",
       " 'cross': 479,\n",
       " 'original': 480,\n",
       " 'another': 481,\n",
       " 'popular': 482,\n",
       " 'flick': 483,\n",
       " 'period': 484,\n",
       " 'poseidon': 485,\n",
       " 'adventure': 486,\n",
       " 'submerged': 487,\n",
       " 'stark': 488,\n",
       " 'dilemma': 489,\n",
       " 'facing': 490,\n",
       " 'trapped': 491,\n",
       " 'inside': 492,\n",
       " 'either': 493,\n",
       " 'suffocate': 494,\n",
       " 'runs': 495,\n",
       " 'drown': 496,\n",
       " 'floods': 497,\n",
       " 'doors': 498,\n",
       " 'decent': 499,\n",
       " 'idea': 500,\n",
       " 'could': 501,\n",
       " 'made': 502,\n",
       " 'little': 503,\n",
       " 'bad': 504,\n",
       " 'unsympathetic': 505,\n",
       " 'character': 506,\n",
       " 'dull': 507,\n",
       " 'lethargic': 508,\n",
       " 'set': 509,\n",
       " 'pieces': 510,\n",
       " 'real': 511,\n",
       " 'lack': 512,\n",
       " 'danger': 513,\n",
       " 'suspense': 514,\n",
       " 'tension': 515,\n",
       " 'means': 516,\n",
       " 'missed': 517,\n",
       " 'opportunity': 518,\n",
       " 'rather': 519,\n",
       " 'sluggish': 520,\n",
       " 'keeps': 521,\n",
       " 'entertained': 522,\n",
       " 'odd': 523,\n",
       " 'minutes': 524,\n",
       " 'happens': 525,\n",
       " 'there': 526,\n",
       " 'urgency': 527,\n",
       " 'thought': 528,\n",
       " 'navy': 529,\n",
       " 'become': 530,\n",
       " 'involved': 531,\n",
       " 'pick': 532,\n",
       " 'few': 533,\n",
       " 'shots': 534,\n",
       " 'huge': 535,\n",
       " 'ships': 536,\n",
       " 'helicopters': 537,\n",
       " 'lacking': 538,\n",
       " 'kennedy': 539,\n",
       " 'jinxed': 540,\n",
       " 'airline': 541,\n",
       " 'worker': 542,\n",
       " 'joe': 543,\n",
       " 'patroni': 544,\n",
       " 'back': 545,\n",
       " 'only': 546,\n",
       " 'gets': 547,\n",
       " 'couple': 548,\n",
       " 'scenes': 549,\n",
       " 'barely': 550,\n",
       " 'says': 551,\n",
       " 'anything': 552,\n",
       " 'preferring': 553,\n",
       " 'look': 554,\n",
       " 'worried': 555,\n",
       " 'background': 556,\n",
       " 'video': 557,\n",
       " 'theatrical': 558,\n",
       " 'version': 559,\n",
       " 'run': 560,\n",
       " 'us': 561,\n",
       " 'tv': 562,\n",
       " 'versions': 563,\n",
       " 'add': 564,\n",
       " 'extra': 565,\n",
       " 'hour': 566,\n",
       " 'footage': 567,\n",
       " 'credits': 568,\n",
       " 'sequence': 569,\n",
       " 'flashbacks': 570,\n",
       " 'flesh': 571,\n",
       " 'longer': 572,\n",
       " 'rescue': 573,\n",
       " 'discovery': 574,\n",
       " 'dead': 575,\n",
       " 'bodies': 576,\n",
       " 'navigator': 577,\n",
       " 'am': 578,\n",
       " 'sure': 579,\n",
       " 'sit': 580,\n",
       " 'near': 581,\n",
       " 'cut': 582,\n",
       " 'expected': 583,\n",
       " 'dated': 584,\n",
       " 'badly': 585,\n",
       " 'horrible': 586,\n",
       " 'fashions': 587,\n",
       " 'interior': 588,\n",
       " 'design': 589,\n",
       " 'choices': 590,\n",
       " 'toy': 591,\n",
       " 'model': 592,\n",
       " 'effects': 593,\n",
       " 'aren': 594,\n",
       " 'along': 595,\n",
       " 'sequels': 596,\n",
       " 'pride': 597,\n",
       " 'place': 598,\n",
       " 'razzie': 599,\n",
       " 'award': 600,\n",
       " 'hall': 601,\n",
       " 'shame': 602,\n",
       " 'lots': 603,\n",
       " 'worse': 604,\n",
       " 'reckon': 605,\n",
       " 'harsh': 606,\n",
       " 'action': 607,\n",
       " 'pace': 608,\n",
       " 'slow': 609,\n",
       " 'excitement': 610,\n",
       " 'generated': 611,\n",
       " 'pretty': 612,\n",
       " 'properly': 613,\n",
       " 'production': 614,\n",
       " 'values': 615,\n",
       " 'alright': 616,\n",
       " 'nothing': 617,\n",
       " 'spectacular': 618,\n",
       " 'acting': 619,\n",
       " 'oscar': 620,\n",
       " 'winner': 621,\n",
       " 'jack': 622,\n",
       " 'lemmon': 623,\n",
       " 'said': 624,\n",
       " 'since': 625,\n",
       " 'mistake': 626,\n",
       " 'star': 627,\n",
       " 'looks': 628,\n",
       " 'old': 629,\n",
       " 'frail': 630,\n",
       " 'lee': 631,\n",
       " 'grant': 632,\n",
       " 'drunk': 633,\n",
       " 'sir': 634,\n",
       " 'christopher': 635,\n",
       " 'plenty': 636,\n",
       " 'familiar': 637,\n",
       " 'faces': 638,\n",
       " 'orientated': 639,\n",
       " 'ideas': 640,\n",
       " 'behind': 641,\n",
       " 'bit': 642,\n",
       " 'silly': 643,\n",
       " 'bland': 644,\n",
       " 'direction': 645,\n",
       " 'doesn': 646,\n",
       " 'though': 647,\n",
       " 'sunken': 648,\n",
       " 'shouldn': 649,\n",
       " 'boring': 650,\n",
       " 'followed': 651,\n",
       " 'concorde': 652,\n",
       " '\\nbrilliant': 653,\n",
       " 'dramatic': 654,\n",
       " 'hobo': 655,\n",
       " 'lady': 656,\n",
       " 'ever': 657,\n",
       " 'clothes': 658,\n",
       " 'warehouse': 659,\n",
       " 'none': 660,\n",
       " 'corn': 661,\n",
       " 'face': 662,\n",
       " 'take': 663,\n",
       " 'lawyers': 664,\n",
       " 'superb': 665,\n",
       " 'accused': 666,\n",
       " 'turncoat': 667,\n",
       " 'selling': 668,\n",
       " 'boss': 669,\n",
       " 'dishonest': 670,\n",
       " 'lawyer': 671,\n",
       " 'shrugs': 672,\n",
       " 'indifferently': 673,\n",
       " 'funny': 674,\n",
       " 'words': 675,\n",
       " 'jeffrey': 676,\n",
       " 'favorite': 677,\n",
       " 'later': 678,\n",
       " 'larry': 679,\n",
       " 'sanders': 680,\n",
       " 'fantastic': 681,\n",
       " 'mad': 682,\n",
       " 'millionaire': 683,\n",
       " 'crush': 684,\n",
       " 'ghetto': 685,\n",
       " 'malevolent': 686,\n",
       " 'usual': 687,\n",
       " 'hospital': 688,\n",
       " 'invade': 689,\n",
       " 'demolition': 690,\n",
       " 'site': 691,\n",
       " 'classics': 692,\n",
       " 'legs': 693,\n",
       " 'big': 694,\n",
       " 'diggers': 695,\n",
       " 'fighting': 696,\n",
       " 'bleeds': 697,\n",
       " 'movie': 698,\n",
       " 'each': 699,\n",
       " 'quite': 700,\n",
       " 'often': 701,\n",
       " '\\nthis': 702,\n",
       " 'lacked': 703,\n",
       " 'couldn': 704,\n",
       " 'put': 705,\n",
       " 'finger': 706,\n",
       " 'first': 707,\n",
       " 'charisma': 708,\n",
       " 'part': 709,\n",
       " 'leading': 710,\n",
       " 'actress': 711,\n",
       " 'inevitably': 712,\n",
       " 'translated': 713,\n",
       " 'chemistry': 714,\n",
       " 'she': 715,\n",
       " 'shared': 716,\n",
       " 'screen': 717,\n",
       " 'romantic': 718,\n",
       " 'came': 719,\n",
       " 'across': 720,\n",
       " 'merely': 721,\n",
       " 'actors': 722,\n",
       " 'play': 723,\n",
       " 'very': 724,\n",
       " 'well': 725,\n",
       " 'director': 726,\n",
       " 'miscalculated': 727,\n",
       " 'needed': 728,\n",
       " 'screenplay': 729,\n",
       " 'exactly': 730,\n",
       " 'chef': 731,\n",
       " 'seemed': 732,\n",
       " 'enamored': 733,\n",
       " 'culinary': 734,\n",
       " 'skills': 735,\n",
       " 'restaurant': 736,\n",
       " 'ultimately': 737,\n",
       " 'himself': 738,\n",
       " 'youthful': 739,\n",
       " 'exploits': 740,\n",
       " 'anybody': 741,\n",
       " 'else': 742,\n",
       " 'convinced': 743,\n",
       " 'princess': 744,\n",
       " 'disappointed': 745,\n",
       " 'forget': 746,\n",
       " 'nominated': 747,\n",
       " 'judge': 748,\n",
       " 'yourself': 749,\n",
       " 'easily': 750,\n",
       " 'underrated': 751,\n",
       " 'inn': 752,\n",
       " 'cannon': 753,\n",
       " 'its': 754,\n",
       " 'flawed': 755,\n",
       " 'does': 756,\n",
       " 'realistic': 757,\n",
       " 'view': 758,\n",
       " 'homelessness': 759,\n",
       " 'unlike': 760,\n",
       " 'how': 761,\n",
       " 'citizen': 762,\n",
       " 'kane': 763,\n",
       " 'gave': 764,\n",
       " 'lounge': 765,\n",
       " 'titanic': 766,\n",
       " 'italians': 767,\n",
       " 'idiots': 768,\n",
       " 'jokes': 769,\n",
       " 'fall': 770,\n",
       " 'flat': 771,\n",
       " 'still': 772,\n",
       " 'lovable': 773,\n",
       " 'way': 774,\n",
       " 'comedies': 775,\n",
       " 'pull': 776,\n",
       " 'story': 777,\n",
       " 'traditionally': 778,\n",
       " 'reviled': 779,\n",
       " 'members': 780,\n",
       " 'society': 781,\n",
       " 'truly': 782,\n",
       " 'impressive': 783,\n",
       " 'fisher': 784,\n",
       " 'king': 785,\n",
       " 'crap': 786,\n",
       " 'complaint': 787,\n",
       " 'cast': 788,\n",
       " 'someone': 789,\n",
       " 'writer': 790,\n",
       " '\\nsorry': 791,\n",
       " 'everyone': 792,\n",
       " 'supposed': 793,\n",
       " 'art': 794,\n",
       " 'wow': 795,\n",
       " 'handed': 796,\n",
       " 'guns': 797,\n",
       " 'screening': 798,\n",
       " 'blow': 799,\n",
       " 'brains': 800,\n",
       " 'watch': 801,\n",
       " 'photographic': 802,\n",
       " 'excellent': 803,\n",
       " 'painful': 804,\n",
       " 'absence': 805,\n",
       " 'sound': 806,\n",
       " 'track': 807,\n",
       " 'brutal': 808,\n",
       " 'loooonnnnng': 809,\n",
       " 'long': 810,\n",
       " 'sitting': 811,\n",
       " 'talking': 812,\n",
       " 'especially': 813,\n",
       " 'complaining': 814,\n",
       " 'really': 815,\n",
       " 'hard': 816,\n",
       " 'getting': 817,\n",
       " 'performances': 818,\n",
       " 'dark': 819,\n",
       " 'sombre': 820,\n",
       " 'uninspired': 821,\n",
       " 'stuff': 822,\n",
       " 'thing': 823,\n",
       " 'maureen': 824,\n",
       " 'stapleton': 825,\n",
       " 'red': 826,\n",
       " 'dress': 827,\n",
       " 'dancing': 828,\n",
       " 'otherwise': 829,\n",
       " 'ripoff': 830,\n",
       " 'bergman': 831,\n",
       " 'fan': 832,\n",
       " 'f': 833,\n",
       " 'anyone': 834,\n",
       " 'enjoyed': 835,\n",
       " 'hours': 836,\n",
       " 'lying': 837,\n",
       " 'typical': 838,\n",
       " 'less': 839,\n",
       " 'movies': 840,\n",
       " 'followable': 841,\n",
       " 'leslie': 842,\n",
       " 'rated': 843,\n",
       " 'moments': 844,\n",
       " 'fleshed': 845,\n",
       " 'probably': 846,\n",
       " 'room': 847,\n",
       " 'worth': 848,\n",
       " 'price': 849,\n",
       " 'rent': 850,\n",
       " 'overall': 851,\n",
       " 'job': 852,\n",
       " 'characteristic': 853,\n",
       " 'speaking': 854,\n",
       " 'directly': 855,\n",
       " 'actor': 856,\n",
       " 'fume': 857,\n",
       " 'both': 858,\n",
       " 'played': 859,\n",
       " 'parts': 860,\n",
       " '\\nwhen': 861,\n",
       " 'parents': 862,\n",
       " 'took': 863,\n",
       " 'theater': 864,\n",
       " 'interiors': 865,\n",
       " 'watched': 866,\n",
       " 'we': 867,\n",
       " 'walked': 868,\n",
       " 'recently': 869,\n",
       " 'lived': 870,\n",
       " 'rest': 871,\n",
       " 'pretentious': 872,\n",
       " 'ponderous': 873,\n",
       " 'painfully': 874,\n",
       " 'piece': 875,\n",
       " 'wine': 876,\n",
       " 'cheese': 877,\n",
       " 'tripe': 878,\n",
       " 'woody': 879,\n",
       " 'allen': 880,\n",
       " 'directors': 881,\n",
       " 'worst': 882,\n",
       " 'career': 883,\n",
       " 'unmistakable': 884,\n",
       " 'style': 885,\n",
       " 'ingmar': 886,\n",
       " 'berman': 887,\n",
       " 'gives': 888,\n",
       " 'angular': 889,\n",
       " 'muted': 890,\n",
       " 'insight': 891,\n",
       " 'lives': 892,\n",
       " 'family': 893,\n",
       " 'wrought': 894,\n",
       " 'psychological': 895,\n",
       " 'damage': 896,\n",
       " 'caused': 897,\n",
       " 'estrangement': 898,\n",
       " 'non': 899,\n",
       " 'halitosis': 900,\n",
       " 'whatever': 901,\n",
       " 'intentionally': 902,\n",
       " 'comic': 903,\n",
       " 'relief': 904,\n",
       " 'music': 905,\n",
       " 'drenched': 906,\n",
       " 'shadowy': 907,\n",
       " 'pathos': 908,\n",
       " 'defined': 909,\n",
       " 'expressionist': 910,\n",
       " 'nature': 911,\n",
       " 'improvisational': 912,\n",
       " 'method': 913,\n",
       " 'illicit': 914,\n",
       " 'pronounced': 915,\n",
       " 'depth': 916,\n",
       " 'meaning': 917,\n",
       " 'truth': 918,\n",
       " 'beyond': 919,\n",
       " 'simply': 920,\n",
       " 'sympathy': 921,\n",
       " 'felt': 922,\n",
       " 'contempt': 923,\n",
       " 'parade': 924,\n",
       " 'shuffling': 925,\n",
       " 'whining': 926,\n",
       " 'nicotine': 927,\n",
       " 'stained': 928,\n",
       " 'martyrs': 929,\n",
       " 'perpetual': 930,\n",
       " 'quest': 931,\n",
       " 'identity': 932,\n",
       " 'amid': 933,\n",
       " 'backdrop': 934,\n",
       " 'cosmopolitan': 935,\n",
       " 'affluence': 936,\n",
       " 'baked': 937,\n",
       " 'brie': 938,\n",
       " 'intelligentsia': 939,\n",
       " 'looms': 940,\n",
       " 'fart': 941,\n",
       " 'speaks': 942,\n",
       " 'affected': 943,\n",
       " 'platitudes': 944,\n",
       " 'elevated': 945,\n",
       " 'language': 946,\n",
       " 'cigarettes': 947,\n",
       " 'struggling': 948,\n",
       " 'desperate': 949,\n",
       " 'find': 950,\n",
       " 'understanding': 951,\n",
       " 'goes': 952,\n",
       " 'point': 953,\n",
       " 'want': 954,\n",
       " 'slap': 955,\n",
       " 'resolution': 956,\n",
       " 'interminable': 957,\n",
       " 'introspective': 958,\n",
       " 'babble': 959,\n",
       " 'drama': 960,\n",
       " 'taken': 961,\n",
       " 'extreme': 962,\n",
       " 'ability': 963,\n",
       " 'connect': 964,\n",
       " 'chose': 965,\n",
       " 'immersed': 966,\n",
       " 'themselves': 967,\n",
       " 'feel': 968,\n",
       " 'left': 969,\n",
       " 'reason': 970,\n",
       " 'self': 971,\n",
       " 'indulgent': 972,\n",
       " 'spiritually': 973,\n",
       " 'draining': 974,\n",
       " 'insistence': 975,\n",
       " 'promoting': 976,\n",
       " 'message': 977,\n",
       " 'prozac': 978,\n",
       " 'prose': 979,\n",
       " 'distorted': 980,\n",
       " 'techniques': 981,\n",
       " 'jettisons': 982,\n",
       " 'past': 983,\n",
       " 'relevance': 984,\n",
       " 'highly': 985,\n",
       " 'recommend': 986,\n",
       " 'feeling': 987,\n",
       " 'happy': 988,\n",
       " 'need': 989,\n",
       " 'death': 990,\n",
       " 'let': 991,\n",
       " 'pretend': 992,\n",
       " 'happened': 993,\n",
       " 'comedic': 994,\n",
       " 'robin': 995,\n",
       " 'williams': 996,\n",
       " 'nor': 997,\n",
       " 'quirky': 998,\n",
       " 'recent': 999,\n",
       " 'fame': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = {keys[i]:i+1 for i in range(len(keys))}\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d5e354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:45.634569Z",
     "iopub.status.busy": "2022-03-07T23:11:45.633663Z",
     "iopub.status.idle": "2022-03-07T23:11:45.638494Z",
     "shell.execute_reply": "2022-03-07T23:11:45.639085Z",
     "shell.execute_reply.started": "2022-03-07T22:00:34.023638Z"
    },
    "papermill": {
     "duration": 0.037333,
     "end_time": "2022-03-07T23:11:45.639252",
     "exception": false,
     "start_time": "2022-03-07T23:11:45.601919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "'i do' -> ['i','do']\n",
    "'''\n",
    "def text2seq(text):\n",
    "    res = []\n",
    "    for word in text.split(' '):\n",
    "        res.append(dictionary.get(word,vocab_size-1))\n",
    "        \n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b15807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:45.700330Z",
     "iopub.status.busy": "2022-03-07T23:11:45.699396Z",
     "iopub.status.idle": "2022-03-07T23:11:45.704204Z",
     "shell.execute_reply": "2022-03-07T23:11:45.704783Z",
     "shell.execute_reply.started": "2022-03-07T22:00:34.029640Z"
    },
    "papermill": {
     "duration": 0.036898,
     "end_time": "2022-03-07T23:11:45.704938",
     "exception": false,
     "start_time": "2022-03-07T23:11:45.668040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad(seq,length):\n",
    "    seqLen = len(seq)\n",
    "    if seqLen < length:\n",
    "        seq.extend([0]*(length-seqLen))\n",
    "        \n",
    "    elif seqLen > length:\n",
    "        seq = seq[:length]\n",
    "        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c7ef4",
   "metadata": {
    "papermill": {
     "duration": 0.028482,
     "end_time": "2022-03-07T23:11:45.762274",
     "exception": false,
     "start_time": "2022-03-07T23:11:45.733792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.准备Dataset和Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3fd98f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:45.827758Z",
     "iopub.status.busy": "2022-03-07T23:11:45.827079Z",
     "iopub.status.idle": "2022-03-07T23:11:47.297911Z",
     "shell.execute_reply": "2022-03-07T23:11:47.297340Z",
     "shell.execute_reply.started": "2022-03-07T22:00:34.042042Z"
    },
    "papermill": {
     "duration": 1.506849,
     "end_time": "2022-03-07T23:11:47.298060",
     "exception": false,
     "start_time": "2022-03-07T23:11:45.791211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class myDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.review_list = reviews.split('\\n')\n",
    "        self.label_list = labels.split('\\n')\n",
    "    def __getitem__(self,idx):\n",
    "        label = 1 if self.label_list[idx]=='positive' else 0\n",
    "        return (pad(text2seq(self.review_list[idx]),255), label)\n",
    "    def __len__(self):\n",
    "        return len(self.review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c78eac2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:47.422718Z",
     "iopub.status.busy": "2022-03-07T23:11:47.421922Z",
     "iopub.status.idle": "2022-03-07T23:11:47.425309Z",
     "shell.execute_reply": "2022-03-07T23:11:47.425824Z",
     "shell.execute_reply.started": "2022-03-07T22:00:35.449554Z"
    },
    "papermill": {
     "duration": 0.098834,
     "end_time": "2022-03-07T23:11:47.425995",
     "exception": false,
     "start_time": "2022-03-07T23:11:47.327161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([759,\n",
       "  167,\n",
       "  168,\n",
       "  14,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  95,\n",
       "  172,\n",
       "  113,\n",
       "  173,\n",
       "  98,\n",
       "  24,\n",
       "  174,\n",
       "  175,\n",
       "  4,\n",
       "  176,\n",
       "  30,\n",
       "  177,\n",
       "  133,\n",
       "  147,\n",
       "  11,\n",
       "  178,\n",
       "  32,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  44,\n",
       "  183,\n",
       "  184,\n",
       "  134,\n",
       "  185,\n",
       "  30,\n",
       "  19,\n",
       "  186,\n",
       "  167,\n",
       "  187,\n",
       "  98,\n",
       "  11,\n",
       "  188,\n",
       "  7,\n",
       "  189,\n",
       "  190,\n",
       "  86,\n",
       "  53,\n",
       "  11,\n",
       "  191,\n",
       "  14,\n",
       "  128,\n",
       "  4,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  18,\n",
       "  196,\n",
       "  21,\n",
       "  14,\n",
       "  197,\n",
       "  11,\n",
       "  198,\n",
       "  147,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  30,\n",
       "  202,\n",
       "  203,\n",
       "  11,\n",
       "  204,\n",
       "  205,\n",
       "  167,\n",
       "  195,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  137,\n",
       "  209,\n",
       "  30,\n",
       "  210,\n",
       "  211,\n",
       "  147,\n",
       "  11,\n",
       "  212,\n",
       "  7,\n",
       "  213,\n",
       "  213,\n",
       "  174,\n",
       "  89,\n",
       "  206,\n",
       "  151,\n",
       "  179,\n",
       "  214,\n",
       "  4,\n",
       "  215,\n",
       "  30,\n",
       "  216,\n",
       "  147,\n",
       "  11,\n",
       "  212,\n",
       "  98,\n",
       "  4,\n",
       "  217,\n",
       "  218,\n",
       "  11,\n",
       "  219,\n",
       "  151,\n",
       "  180,\n",
       "  220,\n",
       "  134,\n",
       "  4,\n",
       "  221,\n",
       "  11,\n",
       "  222,\n",
       "  223,\n",
       "  4,\n",
       "  224,\n",
       "  225,\n",
       "  147,\n",
       "  11,\n",
       "  226,\n",
       "  4,\n",
       "  227,\n",
       "  61,\n",
       "  184,\n",
       "  151,\n",
       "  180,\n",
       "  228,\n",
       "  30,\n",
       "  46,\n",
       "  89,\n",
       "  8,\n",
       "  33,\n",
       "  229,\n",
       "  30,\n",
       "  137,\n",
       "  191,\n",
       "  32,\n",
       "  3,\n",
       "  230,\n",
       "  231,\n",
       "  33,\n",
       "  232,\n",
       "  7,\n",
       "  213,\n",
       "  213,\n",
       "  233,\n",
       "  234,\n",
       "  44,\n",
       "  235,\n",
       "  44,\n",
       "  159,\n",
       "  14,\n",
       "  231,\n",
       "  236,\n",
       "  4,\n",
       "  237,\n",
       "  94,\n",
       "  44,\n",
       "  95,\n",
       "  184,\n",
       "  25,\n",
       "  11,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  30,\n",
       "  141,\n",
       "  4,\n",
       "  215,\n",
       "  102,\n",
       "  4,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  30,\n",
       "  46,\n",
       "  206,\n",
       "  245,\n",
       "  45,\n",
       "  216,\n",
       "  25,\n",
       "  11,\n",
       "  212,\n",
       "  98,\n",
       "  246,\n",
       "  247,\n",
       "  218,\n",
       "  11,\n",
       "  219,\n",
       "  206,\n",
       "  231,\n",
       "  248,\n",
       "  245,\n",
       "  45,\n",
       "  249,\n",
       "  89,\n",
       "  245,\n",
       "  250,\n",
       "  102,\n",
       "  4,\n",
       "  155,\n",
       "  251,\n",
       "  53,\n",
       "  127,\n",
       "  252,\n",
       "  253,\n",
       "  7,\n",
       "  11,\n",
       "  215,\n",
       "  33,\n",
       "  147,\n",
       "  254,\n",
       "  231,\n",
       "  3,\n",
       "  255,\n",
       "  147,\n",
       "  11,\n",
       "  178,\n",
       "  102,\n",
       "  4,\n",
       "  256,\n",
       "  147,\n",
       "  257,\n",
       "  258,\n",
       "  30,\n",
       "  259,\n",
       "  257,\n",
       "  260,\n",
       "  261,\n",
       "  254,\n",
       "  245,\n",
       "  45,\n",
       "  92,\n",
       "  262,\n",
       "  130,\n",
       "  11,\n",
       "  263,\n",
       "  7,\n",
       "  245,\n",
       "  33,\n",
       "  214,\n",
       "  11,\n",
       "  264,\n",
       "  265,\n",
       "  117,\n",
       "  4,\n",
       "  266,\n",
       "  267,\n",
       "  8,\n",
       "  33,\n",
       "  268,\n",
       "  147],\n",
       " 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=myDataset()\n",
    "dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76eac1e",
   "metadata": {
    "papermill": {
     "duration": 0.029134,
     "end_time": "2022-03-07T23:11:47.484800",
     "exception": false,
     "start_time": "2022-03-07T23:11:47.455666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08f61e6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:47.548525Z",
     "iopub.status.busy": "2022-03-07T23:11:47.547898Z",
     "iopub.status.idle": "2022-03-07T23:11:47.552344Z",
     "shell.execute_reply": "2022-03-07T23:11:47.552979Z",
     "shell.execute_reply.started": "2022-03-07T22:00:35.519658Z"
    },
    "papermill": {
     "duration": 0.037909,
     "end_time": "2022-03-07T23:11:47.553150",
     "exception": false,
     "start_time": "2022-03-07T23:11:47.515241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def myfunc(batch_data):\n",
    "    '''\n",
    "    batch_data: 32x2\n",
    "    '''\n",
    "    resData = []\n",
    "    resLabel = []\n",
    "    for i in batch_data:\n",
    "        resData.append(i[0])\n",
    "        resLabel.append(i[1])\n",
    "    return torch.tensor(resData,dtype=torch.int),torch.tensor(resLabel,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f48769",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:47.617806Z",
     "iopub.status.busy": "2022-03-07T23:11:47.616843Z",
     "iopub.status.idle": "2022-03-07T23:11:47.639862Z",
     "shell.execute_reply": "2022-03-07T23:11:47.640424Z",
     "shell.execute_reply.started": "2022-03-07T22:00:35.528224Z"
    },
    "papermill": {
     "duration": 0.056947,
     "end_time": "2022-03-07T23:11:47.640599",
     "exception": false,
     "start_time": "2022-03-07T23:11:47.583652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1301ae7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:47.705123Z",
     "iopub.status.busy": "2022-03-07T23:11:47.704432Z",
     "iopub.status.idle": "2022-03-07T23:11:47.708396Z",
     "shell.execute_reply": "2022-03-07T23:11:47.708936Z",
     "shell.execute_reply.started": "2022-03-07T22:00:35.568544Z"
    },
    "papermill": {
     "duration": 0.038078,
     "end_time": "2022-03-07T23:11:47.709106",
     "exception": false,
     "start_time": "2022-03-07T23:11:47.671028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(train_dataset, batch_size=32,shuffle=True,collate_fn=myfunc)\n",
    "testloader = data.DataLoader(test_dataset, batch_size=32,shuffle=True,collate_fn=myfunc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03ebae95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:47.774387Z",
     "iopub.status.busy": "2022-03-07T23:11:47.772054Z",
     "iopub.status.idle": "2022-03-07T23:11:47.777393Z",
     "shell.execute_reply": "2022-03-07T23:11:47.778007Z",
     "shell.execute_reply.started": "2022-03-07T22:00:35.574214Z"
    },
    "papermill": {
     "duration": 0.03896,
     "end_time": "2022-03-07T23:11:47.778175",
     "exception": false,
     "start_time": "2022-03-07T23:11:47.739215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n<class 'torch.Tensor'>\\n<class 'torch.Tensor'>\\ntorch.Size([32, 255])\\ntorch.Size([32])\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in trainloader:\n",
    "#     trData, trLabel = i\n",
    "#     print(type(trData))\n",
    "#     print(type(trLabel))\n",
    "#     print(trData.size())\n",
    "#     print(trLabel.size())\n",
    "#     break\n",
    "\n",
    "'''\n",
    "<class 'torch.Tensor'>\n",
    "<class 'torch.Tensor'>\n",
    "torch.Size([32, 255])\n",
    "torch.Size([32])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03970e42",
   "metadata": {
    "papermill": {
     "duration": 0.029584,
     "end_time": "2022-03-07T23:11:47.837836",
     "exception": false,
     "start_time": "2022-03-07T23:11:47.808252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fa82a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:47.912943Z",
     "iopub.status.busy": "2022-03-07T23:11:47.912242Z",
     "iopub.status.idle": "2022-03-07T23:11:47.914551Z",
     "shell.execute_reply": "2022-03-07T23:11:47.915056Z",
     "shell.execute_reply.started": "2022-03-07T22:00:35.590398Z"
    },
    "papermill": {
     "duration": 0.047386,
     "end_time": "2022-03-07T23:11:47.915238",
     "exception": false,
     "start_time": "2022-03-07T23:11:47.867852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "emb_dim = 30\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n",
    "        self.rnn = nn.RNN(input_size=emb_dim,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.fc1 = nn.Linear(32,128)\n",
    "        self.fc2 = nn.Linear(128,32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                     # 32,255,30\n",
    "#         h0 = torch.randn(1, x.size()[0], 32)               # D*num_layers, batch_size, hidden_size\n",
    "        _,hns = self.rnn(x)                    # 1,32,32\n",
    "        x = hns.view(hns.size()[1],-1)            # reshape to 32,32\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        outprob = torch.sigmoid(self.fc3(x))      # batch_size, 1\n",
    "        return outprob.view(outprob.size()[0])    # reshape to 32\n",
    "#         pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b6848e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:47.981623Z",
     "iopub.status.busy": "2022-03-07T23:11:47.980700Z",
     "iopub.status.idle": "2022-03-07T23:11:48.045089Z",
     "shell.execute_reply": "2022-03-07T23:11:48.044463Z",
     "shell.execute_reply.started": "2022-03-07T22:00:35.605389Z"
    },
    "papermill": {
     "duration": 0.098194,
     "end_time": "2022-03-07T23:11:48.045236",
     "exception": false,
     "start_time": "2022-03-07T23:11:47.947042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建网络模型\n",
    "myModel = MyModel()\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 优化器\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(myModel.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练的轮数\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01e01149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:48.112607Z",
     "iopub.status.busy": "2022-03-07T23:11:48.111552Z",
     "iopub.status.idle": "2022-03-07T23:11:48.113493Z",
     "shell.execute_reply": "2022-03-07T23:11:48.113998Z",
     "shell.execute_reply.started": "2022-03-07T22:00:35.675339Z"
    },
    "papermill": {
     "duration": 0.038249,
     "end_time": "2022-03-07T23:11:48.114166",
     "exception": false,
     "start_time": "2022-03-07T23:11:48.075917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correct_num(vec1,vec2):\n",
    "    result = (torch.abs(vec1-vec2)) <0.5\n",
    "    return torch.sum(result).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eb7be9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:48.178600Z",
     "iopub.status.busy": "2022-03-07T23:11:48.177599Z",
     "iopub.status.idle": "2022-03-07T23:11:48.182798Z",
     "shell.execute_reply": "2022-03-07T23:11:48.183297Z",
     "shell.execute_reply.started": "2022-03-07T22:00:35.681460Z"
    },
    "papermill": {
     "duration": 0.039317,
     "end_time": "2022-03-07T23:11:48.183490",
     "exception": false,
     "start_time": "2022-03-07T23:11:48.144173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_size = len(test_dataset)\n",
    "test_data_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a713b6",
   "metadata": {
    "papermill": {
     "duration": 0.031195,
     "end_time": "2022-03-07T23:11:48.245055",
     "exception": false,
     "start_time": "2022-03-07T23:11:48.213860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58f9a214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:11:48.309626Z",
     "iopub.status.busy": "2022-03-07T23:11:48.308649Z",
     "iopub.status.idle": "2022-03-08T00:30:55.817177Z",
     "shell.execute_reply": "2022-03-08T00:30:55.817778Z",
     "shell.execute_reply.started": "2022-03-07T22:00:35.698031Z"
    },
    "papermill": {
     "duration": 4747.542611,
     "end_time": "2022-03-08T00:30:55.817952",
     "exception": false,
     "start_time": "2022-03-07T23:11:48.275341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------第 1 轮训练开始-------\n",
      "第1轮第100训练step时的loss: 0.6935737729072571\n",
      "第1轮第200训练step时的loss: 0.6904212236404419\n",
      "第1轮第300训练step时的loss: 0.6965456008911133\n",
      "第1轮第400训练step时的loss: 0.6885498762130737\n",
      "第1轮第500训练step时的loss: 0.6987849473953247\n",
      "第1轮第600训练step时的loss: 0.6940868496894836\n",
      "第1轮整体测试集上的Loss: 108.82200574874878\n",
      "第1轮整体测试集上的Accuracy: 0.5076\n",
      "-------第 2 轮训练开始-------\n",
      "第2轮第100训练step时的loss: 0.691878080368042\n",
      "第2轮第200训练step时的loss: 0.6945547461509705\n",
      "第2轮第300训练step时的loss: 0.6931473612785339\n",
      "第2轮第400训练step时的loss: 0.6953945159912109\n",
      "第2轮第500训练step时的loss: 0.6925694346427917\n",
      "第2轮第600训练step时的loss: 0.6913613080978394\n",
      "第2轮整体测试集上的Loss: 108.81661278009415\n",
      "第2轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 3 轮训练开始-------\n",
      "第3轮第100训练step时的loss: 0.6885122656822205\n",
      "第3轮第200训练step时的loss: 0.6955881118774414\n",
      "第3轮第300训练step时的loss: 0.6860030889511108\n",
      "第3轮第400训练step时的loss: 0.6974500417709351\n",
      "第3轮第500训练step时的loss: 0.6955376863479614\n",
      "第3轮第600训练step时的loss: 0.6973163485527039\n",
      "第3轮整体测试集上的Loss: 108.85424464941025\n",
      "第3轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 4 轮训练开始-------\n",
      "第4轮第100训练step时的loss: 0.6944462656974792\n",
      "第4轮第200训练step时的loss: 0.6920533180236816\n",
      "第4轮第300训练step时的loss: 0.6884130835533142\n",
      "第4轮第400训练step时的loss: 0.6960775256156921\n",
      "第4轮第500训练step时的loss: 0.6922729015350342\n",
      "第4轮第600训练step时的loss: 0.6916186213493347\n",
      "第4轮整体测试集上的Loss: 108.88411068916321\n",
      "第4轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 5 轮训练开始-------\n",
      "第5轮第100训练step时的loss: 0.6938169598579407\n",
      "第5轮第200训练step时的loss: 0.6881540417671204\n",
      "第5轮第300训练step时的loss: 0.6927841305732727\n",
      "第5轮第400训练step时的loss: 0.6908519864082336\n",
      "第5轮第500训练step时的loss: 0.6932022571563721\n",
      "第5轮第600训练step时的loss: 0.6922470927238464\n",
      "第5轮整体测试集上的Loss: 108.81922507286072\n",
      "第5轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 6 轮训练开始-------\n",
      "第6轮第100训练step时的loss: 0.6937062740325928\n",
      "第6轮第200训练step时的loss: 0.6878984570503235\n",
      "第6轮第300训练step时的loss: 0.6985827684402466\n",
      "第6轮第400训练step时的loss: 0.692444920539856\n",
      "第6轮第500训练step时的loss: 0.6933351159095764\n",
      "第6轮第600训练step时的loss: 0.6921517848968506\n",
      "第6轮整体测试集上的Loss: 108.81848192214966\n",
      "第6轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 7 轮训练开始-------\n",
      "第7轮第100训练step时的loss: 0.6929163336753845\n",
      "第7轮第200训练step时的loss: 0.6921917200088501\n",
      "第7轮第300训练step时的loss: 0.6952387094497681\n",
      "第7轮第400训练step时的loss: 0.6939047574996948\n",
      "第7轮第500训练step时的loss: 0.691527247428894\n",
      "第7轮第600训练step时的loss: 0.6937724351882935\n",
      "第7轮整体测试集上的Loss: 108.81137609481812\n",
      "第7轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 8 轮训练开始-------\n",
      "第8轮第100训练step时的loss: 0.6946244239807129\n",
      "第8轮第200训练step时的loss: 0.6950980424880981\n",
      "第8轮第300训练step时的loss: 0.6944680213928223\n",
      "第8轮第400训练step时的loss: 0.6939881443977356\n",
      "第8轮第500训练step时的loss: 0.6930185556411743\n",
      "第8轮第600训练step时的loss: 0.6880701780319214\n",
      "第8轮整体测试集上的Loss: 108.84569776058197\n",
      "第8轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 9 轮训练开始-------\n",
      "第9轮第100训练step时的loss: 0.696921706199646\n",
      "第9轮第200训练step时的loss: 0.6789544820785522\n",
      "第9轮第300训练step时的loss: 0.6909106969833374\n",
      "第9轮第400训练step时的loss: 0.6955666542053223\n",
      "第9轮第500训练step时的loss: 0.6932007670402527\n",
      "第9轮第600训练step时的loss: 0.6932251453399658\n",
      "第9轮整体测试集上的Loss: 108.84201222658157\n",
      "第9轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 10 轮训练开始-------\n",
      "第10轮第100训练step时的loss: 0.6925245523452759\n",
      "第10轮第200训练step时的loss: 0.698462963104248\n",
      "第10轮第300训练step时的loss: 0.6901705861091614\n",
      "第10轮第400训练step时的loss: 0.6933133602142334\n",
      "第10轮第500训练step时的loss: 0.6921379566192627\n",
      "第10轮第600训练step时的loss: 0.6898314952850342\n",
      "第10轮整体测试集上的Loss: 108.81439799070358\n",
      "第10轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 11 轮训练开始-------\n",
      "第11轮第100训练step时的loss: 0.6880000233650208\n",
      "第11轮第200训练step时的loss: 0.6934558153152466\n",
      "第11轮第300训练step时的loss: 0.6945542693138123\n",
      "第11轮第400训练step时的loss: 0.6978969573974609\n",
      "第11轮第500训练step时的loss: 0.6933400630950928\n",
      "第11轮第600训练step时的loss: 0.6945220828056335\n",
      "第11轮整体测试集上的Loss: 108.83472657203674\n",
      "第11轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 12 轮训练开始-------\n",
      "第12轮第100训练step时的loss: 0.6931796669960022\n",
      "第12轮第200训练step时的loss: 0.7017132043838501\n",
      "第12轮第300训练step时的loss: 0.6890140175819397\n",
      "第12轮第400训练step时的loss: 0.6957964301109314\n",
      "第12轮第500训练step时的loss: 0.6930131912231445\n",
      "第12轮第600训练step时的loss: 0.6911945939064026\n",
      "第12轮整体测试集上的Loss: 108.82147431373596\n",
      "第12轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 13 轮训练开始-------\n",
      "第13轮第100训练step时的loss: 0.69452965259552\n",
      "第13轮第200训练step时的loss: 0.6905840039253235\n",
      "第13轮第300训练step时的loss: 0.69191575050354\n",
      "第13轮第400训练step时的loss: 0.6969541907310486\n",
      "第13轮第500训练step时的loss: 0.6940867900848389\n",
      "第13轮第600训练step时的loss: 0.6939249038696289\n",
      "第13轮整体测试集上的Loss: 109.00490981340408\n",
      "第13轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 14 轮训练开始-------\n",
      "第14轮第100训练step时的loss: 0.6958987712860107\n",
      "第14轮第200训练step时的loss: 0.6883299946784973\n",
      "第14轮第300训练step时的loss: 0.693642795085907\n",
      "第14轮第400训练step时的loss: 0.693231463432312\n",
      "第14轮第500训练step时的loss: 0.694271445274353\n",
      "第14轮第600训练step时的loss: 0.6930320858955383\n",
      "第14轮整体测试集上的Loss: 108.85305953025818\n",
      "第14轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 15 轮训练开始-------\n",
      "第15轮第100训练step时的loss: 0.6986598968505859\n",
      "第15轮第200训练step时的loss: 0.6901116371154785\n",
      "第15轮第300训练step时的loss: 0.6947874426841736\n",
      "第15轮第400训练step时的loss: 0.6918478608131409\n",
      "第15轮第500训练step时的loss: 0.6933831572532654\n",
      "第15轮第600训练step时的loss: 0.6935214400291443\n",
      "第15轮整体测试集上的Loss: 108.82053864002228\n",
      "第15轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 16 轮训练开始-------\n",
      "第16轮第100训练step时的loss: 0.694621205329895\n",
      "第16轮第200训练step时的loss: 0.6917160153388977\n",
      "第16轮第300训练step时的loss: 0.6944858431816101\n",
      "第16轮第400训练step时的loss: 0.6920132637023926\n",
      "第16轮第500训练step时的loss: 0.6917270421981812\n",
      "第16轮第600训练step时的loss: 0.6969553828239441\n",
      "第16轮整体测试集上的Loss: 108.81573528051376\n",
      "第16轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 17 轮训练开始-------\n",
      "第17轮第100训练step时的loss: 0.6918213963508606\n",
      "第17轮第200训练step时的loss: 0.6935433149337769\n",
      "第17轮第300训练step时的loss: 0.6908063888549805\n",
      "第17轮第400训练step时的loss: 0.6928591728210449\n",
      "第17轮第500训练step时的loss: 0.6932539343833923\n",
      "第17轮第600训练step时的loss: 0.6919884085655212\n",
      "第17轮整体测试集上的Loss: 108.81790375709534\n",
      "第17轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 18 轮训练开始-------\n",
      "第18轮第100训练step时的loss: 0.6956717371940613\n",
      "第18轮第200训练step时的loss: 0.6934017539024353\n",
      "第18轮第300训练step时的loss: 0.6935951709747314\n",
      "第18轮第400训练step时的loss: 0.701278030872345\n",
      "第18轮第500训练step时的loss: 0.6966768503189087\n",
      "第18轮第600训练step时的loss: 0.6912199258804321\n",
      "第18轮整体测试集上的Loss: 108.82772868871689\n",
      "第18轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 19 轮训练开始-------\n",
      "第19轮第100训练step时的loss: 0.6953800916671753\n",
      "第19轮第200训练step时的loss: 0.6970205307006836\n",
      "第19轮第300训练step时的loss: 0.6975842118263245\n",
      "第19轮第400训练step时的loss: 0.7011667490005493\n",
      "第19轮第500训练step时的loss: 0.6932018399238586\n",
      "第19轮第600训练step时的loss: 0.7003586888313293\n",
      "第19轮整体测试集上的Loss: 108.82456642389297\n",
      "第19轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 20 轮训练开始-------\n",
      "第20轮第100训练step时的loss: 0.6931689977645874\n",
      "第20轮第200训练step时的loss: 0.6971938610076904\n",
      "第20轮第300训练step时的loss: 0.6937457919120789\n",
      "第20轮第400训练step时的loss: 0.6949681639671326\n",
      "第20轮第500训练step时的loss: 0.6947969198226929\n",
      "第20轮第600训练step时的loss: 0.6933295726776123\n",
      "第20轮整体测试集上的Loss: 108.84000378847122\n",
      "第20轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 21 轮训练开始-------\n",
      "第21轮第100训练step时的loss: 0.6851955056190491\n",
      "第21轮第200训练step时的loss: 0.6894634366035461\n",
      "第21轮第300训练step时的loss: 0.6952950954437256\n",
      "第21轮第400训练step时的loss: 0.6834452152252197\n",
      "第21轮第500训练step时的loss: 0.6930140256881714\n",
      "第21轮第600训练step时的loss: 0.6938050985336304\n",
      "第21轮整体测试集上的Loss: 108.84561723470688\n",
      "第21轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 22 轮训练开始-------\n",
      "第22轮第100训练step时的loss: 0.699187695980072\n",
      "第22轮第200训练step时的loss: 0.7015666365623474\n",
      "第22轮第300训练step时的loss: 0.6933143138885498\n",
      "第22轮第400训练step时的loss: 0.6913552284240723\n",
      "第22轮第500训练step时的loss: 0.6932423710823059\n",
      "第22轮第600训练step时的loss: 0.6954637765884399\n",
      "第22轮整体测试集上的Loss: 108.83488523960114\n",
      "第22轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 23 轮训练开始-------\n",
      "第23轮第100训练step时的loss: 0.6917470097541809\n",
      "第23轮第200训练step时的loss: 0.6912054419517517\n",
      "第23轮第300训练step时的loss: 0.6917381286621094\n",
      "第23轮第400训练step时的loss: 0.693151593208313\n",
      "第23轮第500训练step时的loss: 0.691962480545044\n",
      "第23轮第600训练step时的loss: 0.6882622838020325\n",
      "第23轮整体测试集上的Loss: 108.94323927164078\n",
      "第23轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 24 轮训练开始-------\n",
      "第24轮第100训练step时的loss: 0.6935023665428162\n",
      "第24轮第200训练step时的loss: 0.6932316422462463\n",
      "第24轮第300训练step时的loss: 0.6945016384124756\n",
      "第24轮第400训练step时的loss: 0.6931475400924683\n",
      "第24轮第500训练step时的loss: 0.6916981935501099\n",
      "第24轮第600训练step时的loss: 0.6942718625068665\n",
      "第24轮整体测试集上的Loss: 108.8259346485138\n",
      "第24轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 25 轮训练开始-------\n",
      "第25轮第100训练step时的loss: 0.693198561668396\n",
      "第25轮第200训练step时的loss: 0.7012511491775513\n",
      "第25轮第300训练step时的loss: 0.6924254894256592\n",
      "第25轮第400训练step时的loss: 0.6933389902114868\n",
      "第25轮第500训练step时的loss: 0.6931589841842651\n",
      "第25轮第600训练step时的loss: 0.6980432271957397\n",
      "第25轮整体测试集上的Loss: 108.90812665224075\n",
      "第25轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 26 轮训练开始-------\n",
      "第26轮第100训练step时的loss: 0.6932234764099121\n",
      "第26轮第200训练step时的loss: 0.6932014226913452\n",
      "第26轮第300训练step时的loss: 0.6961494088172913\n",
      "第26轮第400训练step时的loss: 0.6929386258125305\n",
      "第26轮第500训练step时的loss: 0.6931060552597046\n",
      "第26轮第600训练step时的loss: 0.693489134311676\n",
      "第26轮整体测试集上的Loss: 108.92036044597626\n",
      "第26轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 27 轮训练开始-------\n",
      "第27轮第100训练step时的loss: 0.6931920051574707\n",
      "第27轮第200训练step时的loss: 0.6914936304092407\n",
      "第27轮第300训练step时的loss: 0.6960775852203369\n",
      "第27轮第400训练step时的loss: 0.6926177144050598\n",
      "第27轮第500训练step时的loss: 0.6966447830200195\n",
      "第27轮第600训练step时的loss: 0.6929564476013184\n",
      "第27轮整体测试集上的Loss: 108.83260387182236\n",
      "第27轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 28 轮训练开始-------\n",
      "第28轮第100训练step时的loss: 0.6887475252151489\n",
      "第28轮第200训练step时的loss: 0.6886414885520935\n",
      "第28轮第300训练step时的loss: 0.6935105323791504\n",
      "第28轮第400训练step时的loss: 0.6964545249938965\n",
      "第28轮第500训练step时的loss: 0.6999961137771606\n",
      "第28轮第600训练step时的loss: 0.6904802918434143\n",
      "第28轮整体测试集上的Loss: 108.81844389438629\n",
      "第28轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 29 轮训练开始-------\n",
      "第29轮第100训练step时的loss: 0.6948038339614868\n",
      "第29轮第200训练step时的loss: 0.6932799816131592\n",
      "第29轮第300训练step时的loss: 0.6914482712745667\n",
      "第29轮第400训练step时的loss: 0.688406229019165\n",
      "第29轮第500训练step时的loss: 0.6935867071151733\n",
      "第29轮第600训练step时的loss: 0.6957190036773682\n",
      "第29轮整体测试集上的Loss: 108.8455640077591\n",
      "第29轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 30 轮训练开始-------\n",
      "第30轮第100训练step时的loss: 0.6921861171722412\n",
      "第30轮第200训练step时的loss: 0.6900278329849243\n",
      "第30轮第300训练step时的loss: 0.6940984725952148\n",
      "第30轮第400训练step时的loss: 0.6941280364990234\n",
      "第30轮第500训练step时的loss: 0.6931692361831665\n",
      "第30轮第600训练step时的loss: 0.6922687292098999\n",
      "第30轮整体测试集上的Loss: 108.81452351808548\n",
      "第30轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 31 轮训练开始-------\n",
      "第31轮第100训练step时的loss: 0.6948121190071106\n",
      "第31轮第200训练step时的loss: 0.6999717950820923\n",
      "第31轮第300训练step时的loss: 0.6926244497299194\n",
      "第31轮第400训练step时的loss: 0.6971989870071411\n",
      "第31轮第500训练step时的loss: 0.6931580305099487\n",
      "第31轮第600训练step时的loss: 0.6882547736167908\n",
      "第31轮整体测试集上的Loss: 108.8712317943573\n",
      "第31轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 32 轮训练开始-------\n",
      "第32轮第100训练step时的loss: 0.6893631219863892\n",
      "第32轮第200训练step时的loss: 0.6911609172821045\n",
      "第32轮第300训练step时的loss: 0.6915491819381714\n",
      "第32轮第400训练step时的loss: 0.6918758749961853\n",
      "第32轮第500训练step时的loss: 0.6946402788162231\n",
      "第32轮第600训练step时的loss: 0.6931649446487427\n",
      "第32轮整体测试集上的Loss: 108.85873359441757\n",
      "第32轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 33 轮训练开始-------\n",
      "第33轮第100训练step时的loss: 0.6915884017944336\n",
      "第33轮第200训练step时的loss: 0.6923577785491943\n",
      "第33轮第300训练step时的loss: 0.6948968768119812\n",
      "第33轮第400训练step时的loss: 0.6901524662971497\n",
      "第33轮第500训练step时的loss: 0.7078614234924316\n",
      "第33轮第600训练step时的loss: 0.693572998046875\n",
      "第33轮整体测试集上的Loss: 108.91271829605103\n",
      "第33轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 34 轮训练开始-------\n",
      "第34轮第100训练step时的loss: 0.6928570866584778\n",
      "第34轮第200训练step时的loss: 0.6938422918319702\n",
      "第34轮第300训练step时的loss: 0.6889563798904419\n",
      "第34轮第400训练step时的loss: 0.6938531398773193\n",
      "第34轮第500训练step时的loss: 0.6943641901016235\n",
      "第34轮第600训练step时的loss: 0.693129301071167\n",
      "第34轮整体测试集上的Loss: 108.84382033348083\n",
      "第34轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 35 轮训练开始-------\n",
      "第35轮第100训练step时的loss: 0.6922429203987122\n",
      "第35轮第200训练step时的loss: 0.6936858892440796\n",
      "第35轮第300训练step时的loss: 0.6937823295593262\n",
      "第35轮第400训练step时的loss: 0.6926388144493103\n",
      "第35轮第500训练step时的loss: 0.6994761824607849\n",
      "第35轮第600训练step时的loss: 0.6934953927993774\n",
      "第35轮整体测试集上的Loss: 108.81497728824615\n",
      "第35轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 36 轮训练开始-------\n",
      "第36轮第100训练step时的loss: 0.6975347995758057\n",
      "第36轮第200训练step时的loss: 0.6931930184364319\n",
      "第36轮第300训练step时的loss: 0.6887341737747192\n",
      "第36轮第400训练step时的loss: 0.6949570178985596\n",
      "第36轮第500训练step时的loss: 0.6988388895988464\n",
      "第36轮第600训练step时的loss: 0.6934555172920227\n",
      "第36轮整体测试集上的Loss: 108.83383470773697\n",
      "第36轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 37 轮训练开始-------\n",
      "第37轮第100训练step时的loss: 0.6933817863464355\n",
      "第37轮第200训练step时的loss: 0.6909331679344177\n",
      "第37轮第300训练step时的loss: 0.6924725770950317\n",
      "第37轮第400训练step时的loss: 0.6935526728630066\n",
      "第37轮第500训练step时的loss: 0.6951295137405396\n",
      "第37轮第600训练step时的loss: 0.6952007412910461\n",
      "第37轮整体测试集上的Loss: 108.81312596797943\n",
      "第37轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 38 轮训练开始-------\n",
      "第38轮第100训练step时的loss: 0.693870484828949\n",
      "第38轮第200训练step时的loss: 0.6925060749053955\n",
      "第38轮第300训练step时的loss: 0.6935769319534302\n",
      "第38轮第400训练step时的loss: 0.6936174035072327\n",
      "第38轮第500训练step时的loss: 0.694423258304596\n",
      "第38轮第600训练step时的loss: 0.6856215000152588\n",
      "第38轮整体测试集上的Loss: 108.88078850507736\n",
      "第38轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 39 轮训练开始-------\n",
      "第39轮第100训练step时的loss: 0.6934646368026733\n",
      "第39轮第200训练step时的loss: 0.7031393051147461\n",
      "第39轮第300训练step时的loss: 0.6941372752189636\n",
      "第39轮第400训练step时的loss: 0.6946612596511841\n",
      "第39轮第500训练step时的loss: 0.6931858062744141\n",
      "第39轮第600训练step时的loss: 0.6921049356460571\n",
      "第39轮整体测试集上的Loss: 108.8196349143982\n",
      "第39轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 40 轮训练开始-------\n",
      "第40轮第100训练step时的loss: 0.6888368725776672\n",
      "第40轮第200训练step时的loss: 0.6931586861610413\n",
      "第40轮第300训练step时的loss: 0.6970391273498535\n",
      "第40轮第400训练step时的loss: 0.6860373020172119\n",
      "第40轮第500训练step时的loss: 0.6952222585678101\n",
      "第40轮第600训练step时的loss: 0.6945958137512207\n",
      "第40轮整体测试集上的Loss: 108.91485249996185\n",
      "第40轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 41 轮训练开始-------\n",
      "第41轮第100训练step时的loss: 0.6927863359451294\n",
      "第41轮第200训练step时的loss: 0.6931590437889099\n",
      "第41轮第300训练step时的loss: 0.7020263671875\n",
      "第41轮第400训练step时的loss: 0.6932101249694824\n",
      "第41轮第500训练step时的loss: 0.6953730583190918\n",
      "第41轮第600训练step时的loss: 0.6725188493728638\n",
      "第41轮整体测试集上的Loss: 108.83340072631836\n",
      "第41轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 42 轮训练开始-------\n",
      "第42轮第100训练step时的loss: 0.6911928653717041\n",
      "第42轮第200训练step时的loss: 0.6941920518875122\n",
      "第42轮第300训练step时的loss: 0.6931477189064026\n",
      "第42轮第400训练step时的loss: 0.6936541795730591\n",
      "第42轮第500训练step时的loss: 0.6892284154891968\n",
      "第42轮第600训练step时的loss: 0.6961291432380676\n",
      "第42轮整体测试集上的Loss: 108.84892427921295\n",
      "第42轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 43 轮训练开始-------\n",
      "第43轮第100训练step时的loss: 0.6941795349121094\n",
      "第43轮第200训练step时的loss: 0.6954126954078674\n",
      "第43轮第300训练step时的loss: 0.6944302320480347\n",
      "第43轮第400训练step时的loss: 0.6972928643226624\n",
      "第43轮第500训练step时的loss: 0.693591296672821\n",
      "第43轮第600训练step时的loss: 0.6924517750740051\n",
      "第43轮整体测试集上的Loss: 108.98105949163437\n",
      "第43轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 44 轮训练开始-------\n",
      "第44轮第100训练step时的loss: 0.691619873046875\n",
      "第44轮第200训练step时的loss: 0.6910793781280518\n",
      "第44轮第300训练step时的loss: 0.6943342685699463\n",
      "第44轮第400训练step时的loss: 0.6981583833694458\n",
      "第44轮第500训练step时的loss: 0.6944982409477234\n",
      "第44轮第600训练step时的loss: 0.6897681951522827\n",
      "第44轮整体测试集上的Loss: 108.83142578601837\n",
      "第44轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 45 轮训练开始-------\n",
      "第45轮第100训练step时的loss: 0.6908001899719238\n",
      "第45轮第200训练step时的loss: 0.6923094391822815\n",
      "第45轮第300训练step时的loss: 0.6872062087059021\n",
      "第45轮第400训练step时的loss: 0.700453519821167\n",
      "第45轮第500训练step时的loss: 0.6892170906066895\n",
      "第45轮第600训练step时的loss: 0.6978970170021057\n",
      "第45轮整体测试集上的Loss: 108.86157953739166\n",
      "第45轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 46 轮训练开始-------\n",
      "第46轮第100训练step时的loss: 0.6919919848442078\n",
      "第46轮第200训练step时的loss: 0.6936845779418945\n",
      "第46轮第300训练step时的loss: 0.6924433708190918\n",
      "第46轮第400训练step时的loss: 0.6990700364112854\n",
      "第46轮第500训练step时的loss: 0.6927415132522583\n",
      "第46轮第600训练step时的loss: 0.6934926509857178\n",
      "第46轮整体测试集上的Loss: 108.83321702480316\n",
      "第46轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 47 轮训练开始-------\n",
      "第47轮第100训练step时的loss: 0.6966157555580139\n",
      "第47轮第200训练step时的loss: 0.6926583051681519\n",
      "第47轮第300训练step时的loss: 0.692877471446991\n",
      "第47轮第400训练step时的loss: 0.6922284960746765\n",
      "第47轮第500训练step时的loss: 0.6932691335678101\n",
      "第47轮第600训练step时的loss: 0.6916601061820984\n",
      "第47轮整体测试集上的Loss: 108.81737607717514\n",
      "第47轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 48 轮训练开始-------\n",
      "第48轮第100训练step时的loss: 0.6922361850738525\n",
      "第48轮第200训练step时的loss: 0.6980787515640259\n",
      "第48轮第300训练step时的loss: 0.6824519038200378\n",
      "第48轮第400训练step时的loss: 0.6935585737228394\n",
      "第48轮第500训练step时的loss: 0.6887399554252625\n",
      "第48轮第600训练step时的loss: 0.6920375227928162\n",
      "第48轮整体测试集上的Loss: 108.84925693273544\n",
      "第48轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 49 轮训练开始-------\n",
      "第49轮第100训练step时的loss: 0.6915419697761536\n",
      "第49轮第200训练step时的loss: 0.6921260356903076\n",
      "第49轮第300训练step时的loss: 0.6947471499443054\n",
      "第49轮第400训练step时的loss: 0.6821005344390869\n",
      "第49轮第500训练step时的loss: 0.6738444566726685\n",
      "第49轮第600训练step时的loss: 0.7021651864051819\n",
      "第49轮整体测试集上的Loss: 108.82510071992874\n",
      "第49轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 50 轮训练开始-------\n",
      "第50轮第100训练step时的loss: 0.6925584673881531\n",
      "第50轮第200训练step时的loss: 0.6947435140609741\n",
      "第50轮第300训练step时的loss: 0.6931480765342712\n",
      "第50轮第400训练step时的loss: 0.6947040557861328\n",
      "第50轮第500训练step时的loss: 0.6931482553482056\n",
      "第50轮第600训练step时的loss: 0.6965095400810242\n",
      "第50轮整体测试集上的Loss: 108.91433227062225\n",
      "第50轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 51 轮训练开始-------\n",
      "第51轮第100训练step时的loss: 0.6915187835693359\n",
      "第51轮第200训练step时的loss: 0.6923312544822693\n",
      "第51轮第300训练step时的loss: 0.6925679445266724\n",
      "第51轮第400训练step时的loss: 0.6794561147689819\n",
      "第51轮第500训练step时的loss: 0.6975139379501343\n",
      "第51轮第600训练step时的loss: 0.6924829483032227\n",
      "第51轮整体测试集上的Loss: 109.01310473680496\n",
      "第51轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 52 轮训练开始-------\n",
      "第52轮第100训练step时的loss: 0.6921404600143433\n",
      "第52轮第200训练step时的loss: 0.6931679248809814\n",
      "第52轮第300训练step时的loss: 0.6852214932441711\n",
      "第52轮第400训练step时的loss: 0.696692705154419\n",
      "第52轮第500训练step时的loss: 0.6892908811569214\n",
      "第52轮第600训练step时的loss: 0.6932095289230347\n",
      "第52轮整体测试集上的Loss: 108.86059045791626\n",
      "第52轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 53 轮训练开始-------\n",
      "第53轮第100训练step时的loss: 0.6926918625831604\n",
      "第53轮第200训练step时的loss: 0.6932668685913086\n",
      "第53轮第300训练step时的loss: 0.6924600601196289\n",
      "第53轮第400训练step时的loss: 0.6912270188331604\n",
      "第53轮第500训练step时的loss: 0.6974091529846191\n",
      "第53轮第600训练step时的loss: 0.695858895778656\n",
      "第53轮整体测试集上的Loss: 108.93322640657425\n",
      "第53轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 54 轮训练开始-------\n",
      "第54轮第100训练step时的loss: 0.6931472420692444\n",
      "第54轮第200训练step时的loss: 0.691351056098938\n",
      "第54轮第300训练step时的loss: 0.6983944177627563\n",
      "第54轮第400训练step时的loss: 0.6927151083946228\n",
      "第54轮第500训练step时的loss: 0.6776472330093384\n",
      "第54轮第600训练step时的loss: 0.6941418647766113\n",
      "第54轮整体测试集上的Loss: 108.81806927919388\n",
      "第54轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 55 轮训练开始-------\n",
      "第55轮第100训练step时的loss: 0.6930689811706543\n",
      "第55轮第200训练step时的loss: 0.6954837441444397\n",
      "第55轮第300训练step时的loss: 0.6932626366615295\n",
      "第55轮第400训练step时的loss: 0.6935389041900635\n",
      "第55轮第500训练step时的loss: 0.6894650459289551\n",
      "第55轮第600训练step时的loss: 0.6961109042167664\n",
      "第55轮整体测试集上的Loss: 108.81937819719315\n",
      "第55轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 56 轮训练开始-------\n",
      "第56轮第100训练step时的loss: 0.6950997114181519\n",
      "第56轮第200训练step时的loss: 0.6920262575149536\n",
      "第56轮第300训练step时的loss: 0.7033753395080566\n",
      "第56轮第400训练step时的loss: 0.6932498216629028\n",
      "第56轮第500训练step时的loss: 0.6938040256500244\n",
      "第56轮第600训练step时的loss: 0.6938141584396362\n",
      "第56轮整体测试集上的Loss: 108.82080137729645\n",
      "第56轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 57 轮训练开始-------\n",
      "第57轮第100训练step时的loss: 0.6932237148284912\n",
      "第57轮第200训练step时的loss: 0.6926171779632568\n",
      "第57轮第300训练step时的loss: 0.6930692791938782\n",
      "第57轮第400训练step时的loss: 0.693400502204895\n",
      "第57轮第500训练step时的loss: 0.693557858467102\n",
      "第57轮第600训练step时的loss: 0.6937903165817261\n",
      "第57轮整体测试集上的Loss: 108.851891040802\n",
      "第57轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 58 轮训练开始-------\n",
      "第58轮第100训练step时的loss: 0.6933230757713318\n",
      "第58轮第200训练step时的loss: 0.6919516324996948\n",
      "第58轮第300训练step时的loss: 0.6873744130134583\n",
      "第58轮第400训练step时的loss: 0.6938323378562927\n",
      "第58轮第500训练step时的loss: 0.6935802698135376\n",
      "第58轮第600训练step时的loss: 0.6915143728256226\n",
      "第58轮整体测试集上的Loss: 108.82159358263016\n",
      "第58轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 59 轮训练开始-------\n",
      "第59轮第100训练step时的loss: 0.6932987570762634\n",
      "第59轮第200训练step时的loss: 0.6976321935653687\n",
      "第59轮第300训练step时的loss: 0.7041918039321899\n",
      "第59轮第400训练step时的loss: 0.6919806599617004\n",
      "第59轮第500训练step时的loss: 0.6835064888000488\n",
      "第59轮第600训练step时的loss: 0.6935861110687256\n",
      "第59轮整体测试集上的Loss: 108.81795006990433\n",
      "第59轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 60 轮训练开始-------\n",
      "第60轮第100训练step时的loss: 0.7002278566360474\n",
      "第60轮第200训练step时的loss: 0.7004403471946716\n",
      "第60轮第300训练step时的loss: 0.6931500434875488\n",
      "第60轮第400训练step时的loss: 0.6946271061897278\n",
      "第60轮第500训练step时的loss: 0.688374400138855\n",
      "第60轮第600训练step时的loss: 0.695859432220459\n",
      "第60轮整体测试集上的Loss: 108.99177485704422\n",
      "第60轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 61 轮训练开始-------\n",
      "第61轮第100训练step时的loss: 0.6972303986549377\n",
      "第61轮第200训练step时的loss: 0.6931933164596558\n",
      "第61轮第300训练step时的loss: 0.6925714612007141\n",
      "第61轮第400训练step时的loss: 0.7020050287246704\n",
      "第61轮第500训练step时的loss: 0.6936963796615601\n",
      "第61轮第600训练step时的loss: 0.6913825273513794\n",
      "第61轮整体测试集上的Loss: 108.82168221473694\n",
      "第61轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 62 轮训练开始-------\n",
      "第62轮第100训练step时的loss: 0.6929352283477783\n",
      "第62轮第200训练step时的loss: 0.6947786211967468\n",
      "第62轮第300训练step时的loss: 0.6921302676200867\n",
      "第62轮第400训练step时的loss: 0.6931609511375427\n",
      "第62轮第500训练step时的loss: 0.6930533647537231\n",
      "第62轮第600训练step时的loss: 0.7004090547561646\n",
      "第62轮整体测试集上的Loss: 109.01219642162323\n",
      "第62轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 63 轮训练开始-------\n",
      "第63轮第100训练step时的loss: 0.6876686215400696\n",
      "第63轮第200训练step时的loss: 0.6918547749519348\n",
      "第63轮第300训练step时的loss: 0.703240692615509\n",
      "第63轮第400训练step时的loss: 0.6934914588928223\n",
      "第63轮第500训练step时的loss: 0.6940454840660095\n",
      "第63轮第600训练step时的loss: 0.6978508234024048\n",
      "第63轮整体测试集上的Loss: 108.842234313488\n",
      "第63轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 64 轮训练开始-------\n",
      "第64轮第100训练step时的loss: 0.6896609663963318\n",
      "第64轮第200训练step时的loss: 0.6931753158569336\n",
      "第64轮第300训练step时的loss: 0.6937804818153381\n",
      "第64轮第400训练step时的loss: 0.6932202577590942\n",
      "第64轮第500训练step时的loss: 0.693151593208313\n",
      "第64轮第600训练step时的loss: 0.6948076486587524\n",
      "第64轮整体测试集上的Loss: 108.86491668224335\n",
      "第64轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 65 轮训练开始-------\n",
      "第65轮第100训练step时的loss: 0.695125937461853\n",
      "第65轮第200训练step时的loss: 0.6891288757324219\n",
      "第65轮第300训练step时的loss: 0.687447726726532\n",
      "第65轮第400训练step时的loss: 0.693381667137146\n",
      "第65轮第500训练step时的loss: 0.6935393214225769\n",
      "第65轮第600训练step时的loss: 0.6851526498794556\n",
      "第65轮整体测试集上的Loss: 108.81757873296738\n",
      "第65轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 66 轮训练开始-------\n",
      "第66轮第100训练step时的loss: 0.693202018737793\n",
      "第66轮第200训练step时的loss: 0.6937414407730103\n",
      "第66轮第300训练step时的loss: 0.694070041179657\n",
      "第66轮第400训练step时的loss: 0.6942804455757141\n",
      "第66轮第500训练step时的loss: 0.6927958726882935\n",
      "第66轮第600训练step时的loss: 0.6911910772323608\n",
      "第66轮整体测试集上的Loss: 109.02537989616394\n",
      "第66轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 67 轮训练开始-------\n",
      "第67轮第100训练step时的loss: 0.6955301761627197\n",
      "第67轮第200训练step时的loss: 0.6971940994262695\n",
      "第67轮第300训练step时的loss: 0.6934623718261719\n",
      "第67轮第400训练step时的loss: 0.6931545734405518\n",
      "第67轮第500训练step时的loss: 0.6903511881828308\n",
      "第67轮第600训练step时的loss: 0.6931562423706055\n",
      "第67轮整体测试集上的Loss: 108.81571424007416\n",
      "第67轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 68 轮训练开始-------\n",
      "第68轮第100训练step时的loss: 0.69324791431427\n",
      "第68轮第200训练step时的loss: 0.6923984885215759\n",
      "第68轮第300训练step时的loss: 0.6932317018508911\n",
      "第68轮第400训练step时的loss: 0.6933401823043823\n",
      "第68轮第500训练step时的loss: 0.6931499242782593\n",
      "第68轮第600训练step时的loss: 0.6940523982048035\n",
      "第68轮整体测试集上的Loss: 108.8130087852478\n",
      "第68轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 69 轮训练开始-------\n",
      "第69轮第100训练step时的loss: 0.6929613351821899\n",
      "第69轮第200训练step时的loss: 0.6972742080688477\n",
      "第69轮第300训练step时的loss: 0.6939594745635986\n",
      "第69轮第400训练step时的loss: 0.6871071457862854\n",
      "第69轮第500训练step时的loss: 0.6953331828117371\n",
      "第69轮第600训练step时的loss: 0.69355309009552\n",
      "第69轮整体测试集上的Loss: 108.83802050352097\n",
      "第69轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 70 轮训练开始-------\n",
      "第70轮第100训练step时的loss: 0.6884788274765015\n",
      "第70轮第200训练step时的loss: 0.6938894391059875\n",
      "第70轮第300训练step时的loss: 0.7021172642707825\n",
      "第70轮第400训练step时的loss: 0.6891801357269287\n",
      "第70轮第500训练step时的loss: 0.6971564292907715\n",
      "第70轮第600训练step时的loss: 0.6855117082595825\n",
      "第70轮整体测试集上的Loss: 108.82886224985123\n",
      "第70轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 71 轮训练开始-------\n",
      "第71轮第100训练step时的loss: 0.692623496055603\n",
      "第71轮第200训练step时的loss: 0.6928215026855469\n",
      "第71轮第300训练step时的loss: 0.6919345259666443\n",
      "第71轮第400训练step时的loss: 0.6878228187561035\n",
      "第71轮第500训练step时的loss: 0.689484179019928\n",
      "第71轮第600训练step时的loss: 0.6931748986244202\n",
      "第71轮整体测试集上的Loss: 108.90861225128174\n",
      "第71轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 72 轮训练开始-------\n",
      "第72轮第100训练step时的loss: 0.6943355202674866\n",
      "第72轮第200训练step时的loss: 0.6911258697509766\n",
      "第72轮第300训练step时的loss: 0.6931798458099365\n",
      "第72轮第400训练step时的loss: 0.6894111037254333\n",
      "第72轮第500训练step时的loss: 0.6871480941772461\n",
      "第72轮第600训练step时的loss: 0.696607232093811\n",
      "第72轮整体测试集上的Loss: 108.83247011899948\n",
      "第72轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 73 轮训练开始-------\n",
      "第73轮第100训练step时的loss: 0.694728672504425\n",
      "第73轮第200训练step时的loss: 0.6927636861801147\n",
      "第73轮第300训练step时的loss: 0.6949576735496521\n",
      "第73轮第400训练step时的loss: 0.6929181218147278\n",
      "第73轮第500训练step时的loss: 0.6924917101860046\n",
      "第73轮第600训练step时的loss: 0.6905670762062073\n",
      "第73轮整体测试集上的Loss: 108.81580793857574\n",
      "第73轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 74 轮训练开始-------\n",
      "第74轮第100训练step时的loss: 0.6884009838104248\n",
      "第74轮第200训练step时的loss: 0.6925307512283325\n",
      "第74轮第300训练step时的loss: 0.6939838528633118\n",
      "第74轮第400训练step时的loss: 0.6914051175117493\n",
      "第74轮第500训练step时的loss: 0.6923614144325256\n",
      "第74轮第600训练step时的loss: 0.692988932132721\n",
      "第74轮整体测试集上的Loss: 108.81571090221405\n",
      "第74轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 75 轮训练开始-------\n",
      "第75轮第100训练step时的loss: 0.692966103553772\n",
      "第75轮第200训练step时的loss: 0.6965934038162231\n",
      "第75轮第300训练step时的loss: 0.6936934590339661\n",
      "第75轮第400训练step时的loss: 0.6931537985801697\n",
      "第75轮第500训练step时的loss: 0.6936058402061462\n",
      "第75轮第600训练step时的loss: 0.69095379114151\n",
      "第75轮整体测试集上的Loss: 109.07162165641785\n",
      "第75轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 76 轮训练开始-------\n",
      "第76轮第100训练step时的loss: 0.6933692097663879\n",
      "第76轮第200训练step时的loss: 0.696333646774292\n",
      "第76轮第300训练step时的loss: 0.6910747289657593\n",
      "第76轮第400训练step时的loss: 0.6986961960792542\n",
      "第76轮第500训练step时的loss: 0.6932517290115356\n",
      "第76轮第600训练step时的loss: 0.689684271812439\n",
      "第76轮整体测试集上的Loss: 108.81675547361374\n",
      "第76轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 77 轮训练开始-------\n",
      "第77轮第100训练step时的loss: 0.6914714574813843\n",
      "第77轮第200训练step时的loss: 0.6978344321250916\n",
      "第77轮第300训练step时的loss: 0.6929147839546204\n",
      "第77轮第400训练step时的loss: 0.693056583404541\n",
      "第77轮第500训练step时的loss: 0.702011227607727\n",
      "第77轮第600训练step时的loss: 0.6945493221282959\n",
      "第77轮整体测试集上的Loss: 108.81862425804138\n",
      "第77轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 78 轮训练开始-------\n",
      "第78轮第100训练step时的loss: 0.6945428848266602\n",
      "第78轮第200训练step时的loss: 0.690071702003479\n",
      "第78轮第300训练step时的loss: 0.6911896467208862\n",
      "第78轮第400训练step时的loss: 0.6931235790252686\n",
      "第78轮第500训练step时的loss: 0.6917198896408081\n",
      "第78轮第600训练step时的loss: 0.6932286620140076\n",
      "第78轮整体测试集上的Loss: 109.14589029550552\n",
      "第78轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 79 轮训练开始-------\n",
      "第79轮第100训练step时的loss: 0.6987666487693787\n",
      "第79轮第200训练step时的loss: 0.6936684846878052\n",
      "第79轮第300训练step时的loss: 0.6931485533714294\n",
      "第79轮第400训练step时的loss: 0.6932371854782104\n",
      "第79轮第500训练step时的loss: 0.6942179799079895\n",
      "第79轮第600训练step时的loss: 0.6930820345878601\n",
      "第79轮整体测试集上的Loss: 108.89874756336212\n",
      "第79轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 80 轮训练开始-------\n",
      "第80轮第100训练step时的loss: 0.6912617087364197\n",
      "第80轮第200训练step时的loss: 0.6862961053848267\n",
      "第80轮第300训练step时的loss: 0.6928735375404358\n",
      "第80轮第400训练step时的loss: 0.6943013668060303\n",
      "第80轮第500训练step时的loss: 0.6953672766685486\n",
      "第80轮第600训练step时的loss: 0.695375919342041\n",
      "第80轮整体测试集上的Loss: 108.82557255029678\n",
      "第80轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 81 轮训练开始-------\n",
      "第81轮第100训练step时的loss: 0.6931476593017578\n",
      "第81轮第200训练step时的loss: 0.6948537826538086\n",
      "第81轮第300训练step时的loss: 0.6907416582107544\n",
      "第81轮第400训练step时的loss: 0.6920087337493896\n",
      "第81轮第500训练step时的loss: 0.6908614635467529\n",
      "第81轮第600训练step时的loss: 0.6944398880004883\n",
      "第81轮整体测试集上的Loss: 108.82414638996124\n",
      "第81轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 82 轮训练开始-------\n",
      "第82轮第100训练step时的loss: 0.693634569644928\n",
      "第82轮第200训练step时的loss: 0.6922321319580078\n",
      "第82轮第300训练step时的loss: 0.6914950609207153\n",
      "第82轮第400训练step时的loss: 0.6933213472366333\n",
      "第82轮第500训练step时的loss: 0.6921573877334595\n",
      "第82轮第600训练step时的loss: 0.6908815503120422\n",
      "第82轮整体测试集上的Loss: 108.8235673904419\n",
      "第82轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 83 轮训练开始-------\n",
      "第83轮第100训练step时的loss: 0.6954519748687744\n",
      "第83轮第200训练step时的loss: 0.6928167939186096\n",
      "第83轮第300训练step时的loss: 0.6896681785583496\n",
      "第83轮第400训练step时的loss: 0.6952131390571594\n",
      "第83轮第500训练step时的loss: 0.6932125687599182\n",
      "第83轮第600训练step时的loss: 0.693574070930481\n",
      "第83轮整体测试集上的Loss: 108.85348010063171\n",
      "第83轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 84 轮训练开始-------\n",
      "第84轮第100训练step时的loss: 0.6936318278312683\n",
      "第84轮第200训练step时的loss: 0.6920928359031677\n",
      "第84轮第300训练step时的loss: 0.694797158241272\n",
      "第84轮第400训练step时的loss: 0.6974905729293823\n",
      "第84轮第500训练step时的loss: 0.6942088603973389\n",
      "第84轮第600训练step时的loss: 0.6939680576324463\n",
      "第84轮整体测试集上的Loss: 108.81922870874405\n",
      "第84轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 85 轮训练开始-------\n",
      "第85轮第100训练step时的loss: 0.692922830581665\n",
      "第85轮第200训练step时的loss: 0.6913387775421143\n",
      "第85轮第300训练step时的loss: 0.6918994784355164\n",
      "第85轮第400训练step时的loss: 0.6978861093521118\n",
      "第85轮第500训练step时的loss: 0.6940259337425232\n",
      "第85轮第600训练step时的loss: 0.6816409826278687\n",
      "第85轮整体测试集上的Loss: 108.83652776479721\n",
      "第85轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 86 轮训练开始-------\n",
      "第86轮第100训练step时的loss: 0.6921675205230713\n",
      "第86轮第200训练step时的loss: 0.6990323066711426\n",
      "第86轮第300训练step时的loss: 0.6933721303939819\n",
      "第86轮第400训练step时的loss: 0.6967666745185852\n",
      "第86轮第500训练step时的loss: 0.6958613991737366\n",
      "第86轮第600训练step时的loss: 0.6979221105575562\n",
      "第86轮整体测试集上的Loss: 108.8259619474411\n",
      "第86轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 87 轮训练开始-------\n",
      "第87轮第100训练step时的loss: 0.6890978217124939\n",
      "第87轮第200训练step时的loss: 0.6907521486282349\n",
      "第87轮第300训练step时的loss: 0.6866527795791626\n",
      "第87轮第400训练step时的loss: 0.6943437457084656\n",
      "第87轮第500训练step时的loss: 0.6917270421981812\n",
      "第87轮第600训练step时的loss: 0.692997932434082\n",
      "第87轮整体测试集上的Loss: 108.86750960350037\n",
      "第87轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 88 轮训练开始-------\n",
      "第88轮第100训练step时的loss: 0.6930917501449585\n",
      "第88轮第200训练step时的loss: 0.6923242807388306\n",
      "第88轮第300训练step时的loss: 0.6921110153198242\n",
      "第88轮第400训练step时的loss: 0.6902884244918823\n",
      "第88轮第500训练step时的loss: 0.6929104924201965\n",
      "第88轮第600训练step时的loss: 0.691612720489502\n",
      "第88轮整体测试集上的Loss: 108.8175373673439\n",
      "第88轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 89 轮训练开始-------\n",
      "第89轮第100训练step时的loss: 0.694458544254303\n",
      "第89轮第200训练step时的loss: 0.686373233795166\n",
      "第89轮第300训练step时的loss: 0.6929954290390015\n",
      "第89轮第400训练step时的loss: 0.689420223236084\n",
      "第89轮第500训练step时的loss: 0.6934795379638672\n",
      "第89轮第600训练step时的loss: 0.6955601572990417\n",
      "第89轮整体测试集上的Loss: 108.89674776792526\n",
      "第89轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 90 轮训练开始-------\n",
      "第90轮第100训练step时的loss: 0.693183422088623\n",
      "第90轮第200训练step时的loss: 0.6910337805747986\n",
      "第90轮第300训练step时的loss: 0.6952564716339111\n",
      "第90轮第400训练step时的loss: 0.6930403113365173\n",
      "第90轮第500训练step时的loss: 0.6932113170623779\n",
      "第90轮第600训练step时的loss: 0.6925175189971924\n",
      "第90轮整体测试集上的Loss: 108.84219628572464\n",
      "第90轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 91 轮训练开始-------\n",
      "第91轮第100训练step时的loss: 0.6918783187866211\n",
      "第91轮第200训练step时的loss: 0.6913444399833679\n",
      "第91轮第300训练step时的loss: 0.6920027732849121\n",
      "第91轮第400训练step时的loss: 0.6888842582702637\n",
      "第91轮第500训练step时的loss: 0.6936402320861816\n",
      "第91轮第600训练step时的loss: 0.696556568145752\n",
      "第91轮整体测试集上的Loss: 108.8685651421547\n",
      "第91轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 92 轮训练开始-------\n",
      "第92轮第100训练step时的loss: 0.6937069892883301\n",
      "第92轮第200训练step时的loss: 0.6923139095306396\n",
      "第92轮第300训练step时的loss: 0.6933836936950684\n",
      "第92轮第400训练step时的loss: 0.6853753328323364\n",
      "第92轮第500训练step时的loss: 0.6925181150436401\n",
      "第92轮第600训练step时的loss: 0.6943629384040833\n",
      "第92轮整体测试集上的Loss: 108.81371170282364\n",
      "第92轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 93 轮训练开始-------\n",
      "第93轮第100训练step时的loss: 0.693437397480011\n",
      "第93轮第200训练step时的loss: 0.6931933164596558\n",
      "第93轮第300训练step时的loss: 0.6931613683700562\n",
      "第93轮第400训练step时的loss: 0.693291187286377\n",
      "第93轮第500训练step时的loss: 0.6933699250221252\n",
      "第93轮第600训练step时的loss: 0.6891588568687439\n",
      "第93轮整体测试集上的Loss: 108.81368070840836\n",
      "第93轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 94 轮训练开始-------\n",
      "第94轮第100训练step时的loss: 0.6883904337882996\n",
      "第94轮第200训练step时的loss: 0.6991214156150818\n",
      "第94轮第300训练step时的loss: 0.6928444504737854\n",
      "第94轮第400训练step时的loss: 0.691545844078064\n",
      "第94轮第500训练step时的loss: 0.7027398347854614\n",
      "第94轮第600训练step时的loss: 0.6924121975898743\n",
      "第94轮整体测试集上的Loss: 108.81998389959335\n",
      "第94轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 95 轮训练开始-------\n",
      "第95轮第100训练step时的loss: 0.6932256817817688\n",
      "第95轮第200训练step时的loss: 0.6932686567306519\n",
      "第95轮第300训练step时的loss: 0.6876670122146606\n",
      "第95轮第400训练step时的loss: 0.6936810612678528\n",
      "第95轮第500训练step时的loss: 0.6929532289505005\n",
      "第95轮第600训练step时的loss: 0.6937357187271118\n",
      "第95轮整体测试集上的Loss: 108.83882182836533\n",
      "第95轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 96 轮训练开始-------\n",
      "第96轮第100训练step时的loss: 0.6929513216018677\n",
      "第96轮第200训练step时的loss: 0.6922042369842529\n",
      "第96轮第300训练step时的loss: 0.6970778703689575\n",
      "第96轮第400训练step时的loss: 0.6931455135345459\n",
      "第96轮第500训练step时的loss: 0.6945540904998779\n",
      "第96轮第600训练step时的loss: 0.6931699514389038\n",
      "第96轮整体测试集上的Loss: 108.91850638389587\n",
      "第96轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 97 轮训练开始-------\n",
      "第97轮第100训练step时的loss: 0.6932626962661743\n",
      "第97轮第200训练step时的loss: 0.6936506032943726\n",
      "第97轮第300训练step时的loss: 0.6989210844039917\n",
      "第97轮第400训练step时的loss: 0.6932321190834045\n",
      "第97轮第500训练step时的loss: 0.6900306344032288\n",
      "第97轮第600训练step时的loss: 0.6947330236434937\n",
      "第97轮整体测试集上的Loss: 108.82523947954178\n",
      "第97轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 98 轮训练开始-------\n",
      "第98轮第100训练step时的loss: 0.6907644867897034\n",
      "第98轮第200训练step时的loss: 0.6950787305831909\n",
      "第98轮第300训练step时的loss: 0.6927638053894043\n",
      "第98轮第400训练step时的loss: 0.6933218240737915\n",
      "第98轮第500训练step时的loss: 0.6998494863510132\n",
      "第98轮第600训练step时的loss: 0.6937800049781799\n",
      "第98轮整体测试集上的Loss: 108.82586282491684\n",
      "第98轮整体测试集上的Accuracy: 0.4948\n",
      "-------第 99 轮训练开始-------\n",
      "第99轮第100训练step时的loss: 0.6863256096839905\n",
      "第99轮第200训练step时的loss: 0.6931052207946777\n",
      "第99轮第300训练step时的loss: 0.694030225276947\n",
      "第99轮第400训练step时的loss: 0.6931433081626892\n",
      "第99轮第500训练step时的loss: 0.6929784417152405\n",
      "第99轮第600训练step时的loss: 0.6935399770736694\n",
      "第99轮整体测试集上的Loss: 108.83398514986038\n",
      "第99轮整体测试集上的Accuracy: 0.5052\n",
      "-------第 100 轮训练开始-------\n",
      "第100轮第100训练step时的loss: 0.6911646723747253\n",
      "第100轮第200训练step时的loss: 0.6898382306098938\n",
      "第100轮第300训练step时的loss: 0.6927143931388855\n",
      "第100轮第400训练step时的loss: 0.6933790445327759\n",
      "第100轮第500训练step时的loss: 0.694649338722229\n",
      "第100轮第600训练step时的loss: 0.6972976326942444\n",
      "第100轮整体测试集上的Loss: 108.81844806671143\n",
      "第100轮整体测试集上的Accuracy: 0.5052\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    print(\"-------第 {} 轮训练开始-------\".format(i+1))\n",
    "\n",
    "    # 训练步骤开始\n",
    "    myModel.train()\n",
    "    step = 0\n",
    "    for data in trainloader:\n",
    "        trData, labels = data\n",
    "        outputs = myModel(trData) # 求模型的输出\n",
    "        loss = loss_fn(outputs, labels)  # 求loss\n",
    "        step += 1\n",
    "        \n",
    "        if (step%100 ==0):\n",
    "            print(f'第{i+1}轮第{step}训练step时的loss: {loss.item()}')\n",
    "        \n",
    "\n",
    "        # 优化器优化模型\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss.backward()       # 求梯度\n",
    "        optimizer.step()      # 更新参数\n",
    "\n",
    "\n",
    "    # 测试步骤开始\n",
    "    myModel.eval()\n",
    "    total_test_loss = 0       # 每一轮总的loss\n",
    "    total_accuracy = 0        # 每一轮总的精确度\n",
    "    with torch.no_grad():     # 不求梯度，不更新参数\n",
    "        for data in testloader:\n",
    "            teData, teLabels = data\n",
    "            outputs = myModel(teData)\n",
    "            loss = loss_fn(outputs, teLabels)\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "            total_accuracy = total_accuracy + correct_num(teLabels,outputs)\n",
    "\n",
    "    print(f\"第{i+1}轮整体测试集上的Loss: {total_test_loss}\")\n",
    "    print(f\"第{i+1}轮整体测试集上的Accuracy: {total_accuracy/test_data_size}\")\n",
    "   \n",
    "\n",
    "#     torch.save(tudui, \"tudui_{}_epoch.pth\".format(i))\n",
    "#     print(\"模型已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d6b8b13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T00:30:56.403194Z",
     "iopub.status.busy": "2022-03-08T00:30:56.400440Z",
     "iopub.status.idle": "2022-03-08T00:30:56.404988Z",
     "shell.execute_reply": "2022-03-08T00:30:56.405454Z",
     "shell.execute_reply.started": "2022-03-07T23:05:47.989062Z"
    },
    "papermill": {
     "duration": 0.309426,
     "end_time": "2022-03-08T00:30:56.405604",
     "exception": false,
     "start_time": "2022-03-08T00:30:56.096178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in testloader:\n",
    "#     trData, trLabel = i\n",
    "#     prob=myModel(trData)\n",
    "#     print(prob)\n",
    "#     print(prob.size())\n",
    "#     print(trLabel.size())\n",
    "    \n",
    "#     print(correct_num(prob,trLabel))\n",
    "#     break\n",
    "\n",
    "# '''\n",
    "# tensor([0.5114, 0.5114, 0.5048, 0.5181, 0.5114, 0.5114, 0.5114, 0.5114, 0.5130,\n",
    "#         0.4825, 0.5268, 0.5114, 0.5114, 0.5114, 0.5114, 0.5114, 0.5114, 0.5114,\n",
    "#         0.5114, 0.5114, 0.5242, 0.5115, 0.5067, 0.4983, 0.5114, 0.5114, 0.5114,\n",
    "#         0.5114, 0.5108, 0.5033, 0.5114, 0.5114], grad_fn=<ViewBackward>)\n",
    "# torch.Size([32])\n",
    "# torch.Size([32])\n",
    "# '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4773.873236,
   "end_time": "2022-03-08T00:30:57.706119",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-07T23:11:23.832883",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
