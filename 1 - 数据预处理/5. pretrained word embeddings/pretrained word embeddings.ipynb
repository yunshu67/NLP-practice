{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\py37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "python 3.7\n",
    "bcolz              1.2.1\n",
    "numpy              1.21.5\n",
    "pytorch  1.11.0\n",
    "\n",
    "'''\n",
    "import os\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "def pretrained_word_embeddings(embed_path:str, over_writte:bool, special_tk:bool=True, freeze:bool=False):\n",
    "    ''' return a torch.nn.Embedding layer, utilizing the pre-trained word vector (e.g., Glove), add 'bos', 'eos', 'unk' and 'pad'.\n",
    "\n",
    "    :param embed_path: the path where pre-trained matrix cached (e.g., './glove.6B.300d.txt').\n",
    "    :param over_writte: force to rewritte the existing matrix.\n",
    "    :param special_tk: whether adding special token -- 'pad', 'unk', bos' and 'eos', at position 0, 1, 2 and 3 by default.\n",
    "    :param freeze: whether trainable.\n",
    "    :return: embed -> nn.Embedding, weights_matrix -> np.array, word2idx -> function, embed_dim -> int\n",
    "    '''\n",
    "    root_dir = embed_path.rsplit(\".\",1)[0]+\".dat\"\n",
    "    out_dir_word = embed_path.rsplit(\".\",1)[0]+\"_words.pkl\"\n",
    "    out_dir_idx = embed_path.rsplit(\".\",1)[0]+\"_idx.pkl\"\n",
    "    if not all([os.path.exists(root_dir),os.path.exists(out_dir_word),os.path.exists(out_dir_idx)]) or over_writte:\n",
    "        ## process and cache glove ===========================================\n",
    "        words = []\n",
    "        idx = 0\n",
    "        _word2idx = {}\n",
    "        vectors = bcolz.carray(np.zeros(1), rootdir=root_dir, mode='w')\n",
    "        with open(os.path.join(embed_path),\"rb\") as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                words.append(word)\n",
    "                _word2idx[word] = idx\n",
    "                idx += 1\n",
    "                vect = np.array(line[1:]).astype(float)\n",
    "                vectors.append(vect)\n",
    "        vectors = bcolz.carray(vectors[1:].reshape((idx, vect.shape[0])), rootdir=root_dir, mode='w')\n",
    "        vectors.flush()\n",
    "        pickle.dump(words, open(out_dir_word, 'wb'))\n",
    "        pickle.dump(_word2idx, open(out_dir_idx, 'wb'))\n",
    "        print(\"dump word/idx at {}\".format(embed_path.rsplit(\"/\",1)[0]))\n",
    "        ## =======================================================\n",
    "    ## load glove\n",
    "    vectors = bcolz.open(root_dir)[:]\n",
    "    words = pickle.load(open(embed_path.rsplit(\".\",1)[0]+\"_words.pkl\", 'rb'))\n",
    "    _word2idx = pickle.load(open(embed_path.rsplit(\".\",1)[0]+\"_idx.pkl\", 'rb'))\n",
    "    print(\"Successfully load Golve from {}, the shape of cached matrix: {}\".format(embed_path.rsplit(\"/\",1)[0],vectors.shape))\n",
    "\n",
    "    word_num, embed_dim = vectors.shape\n",
    "    word_num += 4  if special_tk else 0  ## e.g., 400004\n",
    "    embedding_matrix = np.zeros((word_num, embed_dim))\n",
    "    if special_tk:\n",
    "        embedding_matrix[1] = np.random.normal(scale=0.6, size=(embed_dim, ))\n",
    "        embedding_matrix[2] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "        embedding_matrix[3] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "        embedding_matrix[4:,:] = vectors\n",
    "        weights_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "        pad_idx,unk_idx, bos_idx,eos_idx = 0,1,2,3\n",
    "        embed_layer = torch.nn.Embedding(word_num, embed_dim,padding_idx=pad_idx)\n",
    "        embed_layer.from_pretrained(weights_matrix_tensor,freeze=freeze,padding_idx=pad_idx)\n",
    "        _word2idx = dict([(k,v+4) for k,v in _word2idx.items()])\n",
    "        assert len(_word2idx) + 4 == embedding_matrix.shape[0]\n",
    "    else:\n",
    "        embedding_matrix[:,:] = vectors\n",
    "        weights_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "        embed_layer = torch.nn.Embedding(word_num, embed_dim)\n",
    "        embed_layer.from_pretrained(weights_matrix_tensor,freeze=freeze)\n",
    "        assert len(_word2idx) == embedding_matrix.shape[0]\n",
    "\n",
    "    def word2idx(word:str):\n",
    "        if word == '<bos>': return 2\n",
    "        elif word == '<eos>': return 3\n",
    "        return _word2idx.get(word,1)\n",
    "    return embed_layer, embedding_matrix, word2idx, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump word/idx at ../../word embeddings\n",
      "Successfully load Golve from ../../word embeddings, the shape of cached matrix: (400000, 50)\n",
      "13079\n",
      "[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      " -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "  0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "  0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "  0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      " -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      " -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "  0.67204 ]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([[[-2.3598e+00, -1.0984e+00,  4.8800e-01,  1.8143e+00,  2.3621e-01,\n",
      "          -6.7145e-01,  2.1274e+00, -1.0858e+00,  3.4223e-01, -3.4758e-02,\n",
      "           9.7324e-01,  1.0998e+00,  1.7104e+00,  1.2859e+00, -9.7416e-01,\n",
      "           4.8899e-01, -4.8311e-01,  1.0433e+00,  2.9016e-01, -1.5880e+00,\n",
      "           3.5155e-01, -4.5926e-01,  2.1283e-01, -3.0236e-01,  3.2978e-01,\n",
      "          -1.2428e-01,  3.5332e-01,  1.1280e-01,  6.9725e-01,  9.9673e-01,\n",
      "           1.0105e+00, -7.0299e-01, -8.0097e-01, -2.2247e-02,  1.4739e+00,\n",
      "           8.8918e-01, -9.7568e-01,  4.0473e-01, -6.0935e-01, -6.5659e-01,\n",
      "          -6.4616e-01,  4.8247e-01,  7.7013e-01,  1.0267e+00, -1.0532e+00,\n",
      "          -3.9604e-01, -1.9471e+00,  1.2507e+00, -1.1618e+00, -4.3234e-01],\n",
      "         [-1.1444e-03, -2.1049e-01,  1.5129e-01, -1.2575e+00, -1.3172e+00,\n",
      "          -1.1764e+00,  7.0807e-01,  5.1035e-01,  5.6637e-01, -5.2854e-01,\n",
      "           1.1032e+00,  1.7496e+00,  1.2983e+00, -7.3571e-01,  5.3808e-01,\n",
      "          -2.4419e+00, -7.1550e-01, -6.6178e-01, -4.2396e-01,  6.7374e-01,\n",
      "           1.7193e-01,  1.4313e+00,  9.3238e-01, -8.3419e-02, -1.8915e+00,\n",
      "          -8.8361e-01,  1.0990e+00, -2.3367e-01, -1.2621e+00, -1.6720e+00,\n",
      "          -5.2589e-01, -4.5848e-01,  1.4657e+00, -6.4562e-02,  2.3809e-01,\n",
      "          -3.8170e-01, -7.5255e-01, -2.4091e+00, -1.1024e+00, -3.7124e-01,\n",
      "           3.7878e-01, -5.3374e-01,  1.4259e-01, -3.8014e-01, -7.4171e-02,\n",
      "           2.7321e-01,  4.6238e-01,  1.6305e+00, -1.1902e-01, -1.7382e-01]],\n",
      "\n",
      "        [[ 4.5150e-01, -3.3013e-01,  5.5173e-01,  3.0964e-01,  5.8109e-02,\n",
      "          -5.4118e-01,  6.7341e-01,  7.0149e-01,  1.4874e+00,  2.2417e+00,\n",
      "          -8.6245e-01,  6.8266e-01, -1.6493e+00, -1.9793e+00, -2.0517e-01,\n",
      "           3.0292e-01, -2.1613e+00, -5.6317e-01, -2.2156e+00,  7.6917e-02,\n",
      "           1.6594e+00, -1.8296e-01, -1.3805e+00, -2.6630e-01, -1.3872e+00,\n",
      "          -9.9826e-01, -1.3172e-01, -8.5519e-01, -1.4299e-01,  1.5413e+00,\n",
      "          -2.0761e-01, -4.9662e-03,  6.0754e-01, -2.9866e-01,  1.9927e-01,\n",
      "          -6.2013e-01,  2.1278e-02,  1.5583e-01, -1.1900e+00, -2.1485e-02,\n",
      "           6.3874e-01,  3.5142e-01,  1.2299e+00, -7.9116e-01,  3.8609e-01,\n",
      "          -1.3086e-01,  4.0815e-01, -5.7789e-01, -7.0085e-01, -9.8828e-01],\n",
      "         [-1.0335e+00,  5.8591e-01,  1.7265e+00, -3.1092e-02, -1.9419e+00,\n",
      "           1.2407e-01,  1.7532e+00,  8.3227e-01, -8.9041e-01,  3.4740e-01,\n",
      "          -6.0510e-01, -6.7928e-01,  9.0456e-01,  3.0682e-01, -2.1914e-01,\n",
      "           1.3324e+00, -3.0701e-01,  8.2233e-01,  7.9788e-01, -2.0312e-02,\n",
      "          -9.5555e-01,  2.1901e+00, -2.7942e-01, -8.5376e-01, -9.4155e-01,\n",
      "          -7.9533e-01, -2.9276e-02,  1.0321e+00, -1.4862e+00, -8.4057e-01,\n",
      "           5.4614e-01,  4.1812e-01,  5.9607e-03, -1.5385e+00,  2.8170e-02,\n",
      "          -9.7702e-01, -1.0563e-02, -1.4167e-01,  2.3446e+00, -1.5213e+00,\n",
      "           9.3367e-01, -5.9356e-01,  4.7490e-01, -1.7188e+00,  1.1998e-01,\n",
      "          -1.2149e+00, -3.7553e-01, -4.7853e-01,  2.7928e-01, -8.0231e-01]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_layer, weights_matrix, word2idx, embed_dim = pretrained_word_embeddings('../../word embeddings/glove.6B.50d.txt', True, freeze=True)\n",
    "word = 'hello'\n",
    "idx = word2idx(word)\n",
    "embedding = weights_matrix[idx]\n",
    "\n",
    "print(idx)\n",
    "print(embedding)\n",
    "print(type(embedding))\n",
    "\n",
    "\n",
    "data = torch.tensor([[1,2],[6,7]])\n",
    "output= embed_layer(data)\n",
    "print(output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}