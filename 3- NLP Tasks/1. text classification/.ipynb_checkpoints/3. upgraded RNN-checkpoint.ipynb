{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "norm_reviews = np.load('normalized_reviews.npy')\n",
    "norm_sentiment = np.load('normalized_sentiment.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,) (50000, 256)\n"
     ]
    }
   ],
   "source": [
    "print(norm_sentiment.shape,norm_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     2     52      7      4     72  17965     35   3046     16     53\n",
      "   2645    124    180  12965   1946     85    773     34  12769      6\n",
      "     43     36    252      5     23     41     18   2800    106   1583\n",
      "     21      1      1      1     62    877     16   1873    289     63\n",
      "  12965     19     51  13159      9  45659   3472      7    718      5\n",
      "     46    212     10    252     29      4   1392    246      6   1857\n",
      "    289      5     41     18     40     11    277     14      4  17177\n",
      "  21364     50  25444      6     41    277  11114     88  14775     21\n",
      "   9485      8   1607      5   1743     50    718      6     51     18\n",
      "  16361      5     10      4   2396    238      7      4      1      1\n",
      "      1     18    179  12965     23     16     18      4   7402    458\n",
      "      8      4  17868   2968    198     96      1      6     24   6549\n",
      "   1678     17  19724    119      5     33   6126   1523      7      4\n",
      "   1001    115     68      4   2778     37   2851  11278      9    625\n",
      "  67375      5    104   5314     18     40    156     17      4   2808\n",
      "      6  23612    119     18    167      8    113      1  86437      5\n",
      "   2483      5 146506      5  15547      5   4995      5   8597      5\n",
      "   1842      9     60   6942    104  31999      5    340  31661      5\n",
      "  59477   9608      9  18399   3210     36    336    376      1      1\n",
      "      1     58    207      4    448   1578      7      4    277     18\n",
      "    449      8      4    857     16     24   1436    115     72    974\n",
      "     58     74  12252      6   4462   1926   2494   4609     14   4830\n",
      "   5814      5   4462   9920      5   4462   7200    438  12965    264\n",
      "     74   7566    208      6      4     62   1946     45    665    826\n",
      "   1873    289     23    104  10768     24     19  17663      5     45\n",
      "     98     74    207     45     19   1192]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(norm_reviews[0])\n",
    "print(norm_sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\py37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "python 3.7\n",
    "bcolz              1.2.1\n",
    "numpy              1.21.5\n",
    "pytorch  1.11.0\n",
    "\n",
    "'''\n",
    "import os\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "def pretrained_word_embeddings(embed_path:str, over_writte:bool, special_tk:bool=True, freeze:bool=True):\n",
    "    ''' return a torch.nn.Embedding layer, utilizing the pre-trained word vector (e.g., Glove), add 'bos', 'eos', 'unk' and 'pad'.\n",
    "\n",
    "    :param embed_path: the path where pre-trained matrix cached (e.g., './glove.6B.300d.txt').\n",
    "    :param over_writte: force to rewritte the existing matrix.\n",
    "    :param special_tk: whether adding special token -- 'pad', 'unk', bos' and 'eos', at position 0, 1, 2 and 3 by default.\n",
    "    :param freeze: whether trainable.\n",
    "    :return: embed -> nn.Embedding, weights_matrix -> np.array, word2idx -> function, idx2word -> function, embed_dim -> int\n",
    "    '''\n",
    "    root_dir = embed_path.rsplit(\".\",1)[0]+\".dat\"\n",
    "    out_dir_word = embed_path.rsplit(\".\",1)[0]+\"_words.pkl\"\n",
    "    out_dir_idx = embed_path.rsplit(\".\",1)[0]+\"_idx.pkl\"\n",
    "    out_dir_idx2word = embed_path.rsplit(\".\", 1)[0] + \"_idx2word.pkl\"\n",
    "    if not all([os.path.exists(root_dir),os.path.exists(out_dir_word),os.path.exists(out_dir_idx)]) or over_writte:\n",
    "        ## process and cache glove ===========================================\n",
    "        words = []\n",
    "        idx = 0\n",
    "        _word2idx = {}\n",
    "        _idx2word = {}\n",
    "        vectors = bcolz.carray(np.zeros(1), rootdir=root_dir, mode='w')\n",
    "        with open(os.path.join(embed_path),\"rb\") as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                words.append(word)\n",
    "                _word2idx[word] = idx\n",
    "                _idx2word[idx]=word\n",
    "                idx += 1\n",
    "                vect = np.array(line[1:]).astype(float)\n",
    "                vectors.append(vect)\n",
    "        vectors = bcolz.carray(vectors[1:].reshape((idx, vect.shape[0])), rootdir=root_dir, mode='w')\n",
    "        vectors.flush()\n",
    "        pickle.dump(words, open(out_dir_word, 'wb'))\n",
    "        pickle.dump(_word2idx, open(out_dir_idx, 'wb'))\n",
    "        pickle.dump(_idx2word,open(out_dir_idx2word,'wb'))\n",
    "        print(\"dump word/idx at {}\".format(embed_path.rsplit(\"/\",1)[0]))\n",
    "        ## =======================================================\n",
    "    ## load glove\n",
    "    vectors = bcolz.open(root_dir)[:]\n",
    "    words = pickle.load(open(embed_path.rsplit(\".\",1)[0]+\"_words.pkl\", 'rb'))\n",
    "    _word2idx = pickle.load(open(embed_path.rsplit(\".\",1)[0]+\"_idx.pkl\", 'rb'))\n",
    "    _idx2word=pickle.load(open(embed_path.rsplit(\".\", 1)[0] + \"_idx2word.pkl\",'rb'))\n",
    "    print(\"Successfully load Golve from {}, the shape of cached matrix: {}\".format(embed_path.rsplit(\"/\",1)[0],vectors.shape))\n",
    "\n",
    "    word_num, embed_dim = vectors.shape\n",
    "    word_num += 4  if special_tk else 0  ## e.g., 400004\n",
    "    embedding_matrix = np.zeros((word_num, embed_dim))\n",
    "    if special_tk:\n",
    "        embedding_matrix[1] = np.random.normal(scale=0.6, size=(embed_dim, ))\n",
    "        embedding_matrix[2] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "        embedding_matrix[3] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "        embedding_matrix[4:,:] = vectors\n",
    "        weights_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "        pad_idx,unk_idx, bos_idx,eos_idx = 0,1,2,3\n",
    "        embed_layer= torch.nn.Embedding.from_pretrained(weights_matrix_tensor,freeze=freeze,padding_idx=pad_idx)\n",
    "        _word2idx = dict([(k,v+4) for k,v in _word2idx.items()])\n",
    "        _idx2word = dict([(k+4,v) for k,v in _idx2word.items()])\n",
    "        assert len(_word2idx) + 4 == embedding_matrix.shape[0]\n",
    "    else:\n",
    "        embedding_matrix[:,:] = vectors\n",
    "        weights_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "        embed_layer = torch.nn.Embedding.from_pretrained(weights_matrix_tensor,freeze=freeze)\n",
    "        assert len(_word2idx) == embedding_matrix.shape[0]\n",
    "\n",
    "    def word2idx(word:str):\n",
    "        if word == '<pad>': return 0\n",
    "        elif word == '<bos>': return 2\n",
    "        elif word == '<eos>': return 3\n",
    "        return _word2idx.get(word,1)\n",
    "    def idx2word(idx:int):\n",
    "        if idx == 0: return '<pad>'\n",
    "        elif idx == 1: return '<unk>'\n",
    "        elif idx == 2: return '<bos>'\n",
    "        elif idx == 3: return '<eos>'\n",
    "        return _idx2word.get(idx,'')\n",
    "    return embed_layer, embedding_matrix, word2idx,idx2word, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump word/idx at ../../word embeddings\n",
      "Successfully load Golve from ../../word embeddings, the shape of cached matrix: (400000, 300)\n"
     ]
    }
   ],
   "source": [
    "embed_layer, embedding_matrix, word2idx,idx2word, embed_dim = pretrained_word_embeddings('../../word embeddings/glove.6B.300d.txt',over_writte=True,special_tk=True,freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(embed_layer.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, n_layers,\n",
    "                 bidirectional, dropout):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = embed_layer\n",
    "        self.rnn =  nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,batch_first=True)\n",
    "        D = 2 if bidirectional else 1\n",
    "        self.fc3 = nn.Linear(D*n_layers*hidden_dim,1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                     # 32,256,50\n",
    "        _,(hns,cells) = self.rnn(x)               # D*num_layer,batch size,hidden dim\n",
    "        x = hns.view(hns.size()[1],-1)            # reshape to 32,-1\n",
    "        x = self.dropout(x)\n",
    "        outprob = torch.sigmoid(self.fc3(x))      # batch_size, 1\n",
    "        return outprob.view(outprob.size()[0])    # reshape to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建网络模型\n",
    "myModel = MyModel(300,32,2,True,0.8)\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(myModel.parameters(),0.0001)\n",
    "\n",
    "# 训练的轮数\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyModel                                  --\n",
       "├─Embedding: 1-1                         (120,001,200)\n",
       "├─LSTM: 1-2                              110,592\n",
       "├─Linear: 1-3                            129\n",
       "├─Dropout: 1-4                           --\n",
       "=================================================================\n",
       "Total params: 120,111,921\n",
       "Trainable params: 110,721\n",
       "Non-trainable params: 120,001,200\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(myModel.embedding.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def correct_num(vec1,vec2):\n",
    "    result = (torch.abs(vec1-vec2)) <0.5\n",
    "    return torch.sum(result).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class myDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(myDataset,self).__init__()\n",
    "        self.review_list = norm_reviews\n",
    "        self.label_list = norm_sentiment\n",
    "    def __getitem__(self,idx):\n",
    "        return  self.review_list[idx],self.label_list[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     2,     45,    808,     41,     19,     11,   5209,    183,\n",
       "             8,   2029,     83,     17,     11,    321,   1631,    744,\n",
       "          1179,      5,   2999,     10,      4,    329,  17113,   2252,\n",
       "             9,   2645,     11,    901,     15,  21364,   2845,      6,\n",
       "             4,   2223,     18,  26888,      5,     38,      4,   2473,\n",
       "            18,  18222,      9,      4,   2157,     36,  26081,     27,\n",
       "           155,      4,    147,   5876,   1606,   7270,   4902,     28,\n",
       "             6,    114,     81,    111,     34,   4258,     65,     43,\n",
       "          4227,     41,     18,     40,    554,    393,    236,     49,\n",
       "          1122,  10437,      5,     45,    808,     24,     19,   4299,\n",
       "            16,  10971,   3192,     18,    153,   1861,     10,    428,\n",
       "             7,      4,   1139,    113,      7,     99,     37,   2850,\n",
       "             8,      1,      1,      1,     19,      4,    100,     45,\n",
       "          1155,  11391,     26,     52,      7,  10971,     13,  15406,\n",
       "            10,     86,     27,  12252,     45,    207,     11,   1656,\n",
       "           192,     28,      6,    114,     45,    466,    336,     55,\n",
       "          6064,     21,  17046,  74977,      5,     10,     41,     71,\n",
       "          1768,      8,   4449,    139,     75,     12,  11875,     12,\n",
       "          1963,      9,   3454,    252,     79,     11,    645,      5,\n",
       "            38,  12737,    465,      1,      1,      1,    111,     40,\n",
       "            34,      4,   3124,  14145,      7,     30,    436,      5,\n",
       "            38,     24,     19, 194149,     77,     12,   6701,   9321,\n",
       "         20985,     12,      9,     60,   4005,     77,     12,  12749,\n",
       "            12,     11,    357,   2845,      8,    246,    257,     21,\n",
       "          1099,      6,      3,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0]),\n",
       " 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=myDataset()\n",
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def myfunc(batch_data):\n",
    "    '''\n",
    "    batch_data: 32x2\n",
    "    '''\n",
    "    resData = []\n",
    "    resLabel = []\n",
    "    for i in batch_data:\n",
    "        resData.append(i[0])\n",
    "        resLabel.append(i[1])\n",
    "    return torch.tensor(np.array(resData),dtype=torch.int),torch.tensor(np.array(resLabel),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(train_dataset, batch_size=32,shuffle=True,collate_fn=myfunc,drop_last=True)\n",
    "testloader = data.DataLoader(test_dataset, batch_size=32,shuffle=True,collate_fn=myfunc,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------第 1 轮训练开始-------\n",
      "第1轮第100训练step时的loss: 0.6974122524261475\n",
      "第1轮第200训练step时的loss: 0.694391131401062\n",
      "第1轮第300训练step时的loss: 0.720654308795929\n",
      "第1轮第400训练step时的loss: 0.6841869950294495\n",
      "第1轮第500训练step时的loss: 0.7027180790901184\n",
      "第1轮第600训练step时的loss: 0.6917883157730103\n",
      "第1轮第700训练step时的loss: 0.7033368945121765\n",
      "第1轮第800训练step时的loss: 0.7012732625007629\n",
      "第1轮第900训练step时的loss: 0.685875654220581\n",
      "第1轮第1000训练step时的loss: 0.6887688636779785\n",
      "第1轮第1100训练step时的loss: 0.721399188041687\n",
      "第1轮第1200训练step时的loss: 0.7079036831855774\n",
      "第1轮整体测试集上的Loss: 216.34393936395645\n",
      "第1轮整体测试集上的Accuracy: 0.4977\n",
      "-------第 2 轮训练开始-------\n",
      "第2轮第100训练step时的loss: 0.6865971088409424\n",
      "第2轮第200训练step时的loss: 0.685539186000824\n",
      "第2轮第300训练step时的loss: 0.6983528733253479\n",
      "第2轮第400训练step时的loss: 0.7008312940597534\n",
      "第2轮第500训练step时的loss: 0.6734508275985718\n",
      "第2轮第600训练step时的loss: 0.6960465908050537\n",
      "第2轮第700训练step时的loss: 0.6926600337028503\n",
      "第2轮第800训练step时的loss: 0.6968013048171997\n",
      "第2轮第900训练step时的loss: 0.6772184371948242\n",
      "第2轮第1000训练step时的loss: 0.7002111673355103\n",
      "第2轮第1100训练step时的loss: 0.6932343244552612\n",
      "第2轮第1200训练step时的loss: 0.6955271363258362\n",
      "第2轮整体测试集上的Loss: 216.29730772972107\n",
      "第2轮整体测试集上的Accuracy: 0.4962\n",
      "-------第 3 轮训练开始-------\n",
      "第3轮第100训练step时的loss: 0.6759825944900513\n",
      "第3轮第200训练step时的loss: 0.6927028298377991\n",
      "第3轮第300训练step时的loss: 0.6820002794265747\n",
      "第3轮第400训练step时的loss: 0.6879420280456543\n",
      "第3轮第500训练step时的loss: 0.6947186589241028\n",
      "第3轮第600训练step时的loss: 0.6931604146957397\n",
      "第3轮第700训练step时的loss: 0.7073283195495605\n",
      "第3轮第800训练step时的loss: 0.6907523274421692\n",
      "第3轮第900训练step时的loss: 0.6967679858207703\n",
      "第3轮第1000训练step时的loss: 0.6993924975395203\n",
      "第3轮第1100训练step时的loss: 0.6864680647850037\n",
      "第3轮第1200训练step时的loss: 0.7009907960891724\n",
      "第3轮整体测试集上的Loss: 216.2693753838539\n",
      "第3轮整体测试集上的Accuracy: 0.5046\n",
      "-------第 4 轮训练开始-------\n",
      "第4轮第100训练step时的loss: 0.688755989074707\n",
      "第4轮第200训练step时的loss: 0.6917199492454529\n",
      "第4轮第300训练step时的loss: 0.6901524662971497\n",
      "第4轮第400训练step时的loss: 0.6872975826263428\n",
      "第4轮第500训练step时的loss: 0.6830869913101196\n",
      "第4轮第600训练step时的loss: 0.706430196762085\n",
      "第4轮第700训练step时的loss: 0.698185384273529\n",
      "第4轮第800训练step时的loss: 0.7048952579498291\n",
      "第4轮第900训练step时的loss: 0.6899604201316833\n",
      "第4轮第1000训练step时的loss: 0.7033420205116272\n",
      "第4轮第1100训练step时的loss: 0.6906816959381104\n",
      "第4轮第1200训练step时的loss: 0.694783091545105\n",
      "第4轮整体测试集上的Loss: 216.2968977689743\n",
      "第4轮整体测试集上的Accuracy: 0.5002\n",
      "-------第 5 轮训练开始-------\n",
      "第5轮第100训练step时的loss: 0.6858602166175842\n",
      "第5轮第200训练step时的loss: 0.6814144253730774\n",
      "第5轮第300训练step时的loss: 0.7090276479721069\n",
      "第5轮第400训练step时的loss: 0.6935023069381714\n",
      "第5轮第500训练step时的loss: 0.6911607384681702\n",
      "第5轮第600训练step时的loss: 0.7009530663490295\n",
      "第5轮第700训练step时的loss: 0.6989864110946655\n",
      "第5轮第800训练step时的loss: 0.6989352703094482\n",
      "第5轮第900训练step时的loss: 0.6998785138130188\n",
      "第5轮第1000训练step时的loss: 0.6989474296569824\n",
      "第5轮第1100训练step时的loss: 0.6911072134971619\n",
      "第5轮第1200训练step时的loss: 0.6962182521820068\n",
      "第5轮整体测试集上的Loss: 216.26650476455688\n",
      "第5轮整体测试集上的Accuracy: 0.4975\n",
      "-------第 6 轮训练开始-------\n",
      "第6轮第100训练step时的loss: 0.6956961154937744\n",
      "第6轮第200训练step时的loss: 0.6915566921234131\n"
     ]
    }
   ],
   "source": [
    "train_step_loss = []\n",
    "valid_step_loss = []\n",
    "train_epoch_loss = []\n",
    "valid_epoch_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"-------第 {} 轮训练开始-------\".format(i+1))\n",
    "\n",
    "    # 训练步骤开始\n",
    "    myModel.train()\n",
    "    step = 0\n",
    "    for data in trainloader:\n",
    "        trData, labels = data\n",
    "        outputs = myModel(trData) # 求模型的输出\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss = loss_fn(outputs, labels)  # 求loss\n",
    "        train_step_loss.append(loss.item())\n",
    "        step += 1\n",
    "\n",
    "        if (step%100 ==0):\n",
    "            print(f'第{i+1}轮第{step}训练step时的loss: {loss.item()}')\n",
    "\n",
    "\n",
    "        # 优化器优化模型\n",
    "        loss.backward()       # 求梯度\n",
    "        optimizer.step()      # 更新参数\n",
    "        \n",
    "    train_epoch_loss.append(np.average(train_step_loss))\n",
    "\n",
    "    # 测试步骤开始\n",
    "    myModel.eval()\n",
    "    total_accuracy = 0        # 每一轮总的精确度\n",
    "    with torch.no_grad():     # 不求梯度，不更新参数\n",
    "        for data in testloader:\n",
    "            teData, teLabels = data\n",
    "            outputs = myModel(teData)\n",
    "            loss = loss_fn(outputs, teLabels)\n",
    "            valid_step_loss.append(loss.item())\n",
    "            total_accuracy = total_accuracy + correct_num(teLabels,outputs)\n",
    "\n",
    "    valid_epoch_loss.append(np.average(valid_step_loss))\n",
    "    print(f\"第{i+1}轮整体测试集上的Loss: {valid_epoch_loss[-1]}\")\n",
    "    print(f\"第{i+1}轮整体测试集上的Accuracy: {total_accuracy/len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
