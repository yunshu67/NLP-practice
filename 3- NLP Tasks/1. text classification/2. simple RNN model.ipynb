{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "norm_reviews = np.load('normalized_reviews.npy')\n",
    "norm_sentiment = np.load('normalized_sentiment.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,) (50000, 256)\n"
     ]
    }
   ],
   "source": [
    "print(norm_sentiment.shape,norm_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     2     52      7      4     72  17965     35   3046     16     53\n",
      "   2645    124    180  12965   1946     85    773     34  12769      6\n",
      "     43     36    252      5     23     41     18   2800    106   1583\n",
      "     21      1      1      1     62    877     16   1873    289     63\n",
      "  12965     19     51  13159      9  45659   3472      7    718      5\n",
      "     46    212     10    252     29      4   1392    246      6   1857\n",
      "    289      5     41     18     40     11    277     14      4  17177\n",
      "  21364     50  25444      6     41    277  11114     88  14775     21\n",
      "   9485      8   1607      5   1743     50    718      6     51     18\n",
      "  16361      5     10      4   2396    238      7      4      1      1\n",
      "      1     18    179  12965     23     16     18      4   7402    458\n",
      "      8      4  17868   2968    198     96      1      6     24   6549\n",
      "   1678     17  19724    119      5     33   6126   1523      7      4\n",
      "   1001    115     68      4   2778     37   2851  11278      9    625\n",
      "  67375      5    104   5314     18     40    156     17      4   2808\n",
      "      6  23612    119     18    167      8    113      1  86437      5\n",
      "   2483      5 146506      5  15547      5   4995      5   8597      5\n",
      "   1842      9     60   6942    104  31999      5    340  31661      5\n",
      "  59477   9608      9  18399   3210     36    336    376      1      1\n",
      "      1     58    207      4    448   1578      7      4    277     18\n",
      "    449      8      4    857     16     24   1436    115     72    974\n",
      "     58     74  12252      6   4462   1926   2494   4609     14   4830\n",
      "   5814      5   4462   9920      5   4462   7200    438  12965    264\n",
      "     74   7566    208      6      4     62   1946     45    665    826\n",
      "   1873    289     23    104  10768     24     19  17663      5     45\n",
      "     98     74    207     45     19   1192]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(norm_reviews[0])\n",
    "print(norm_sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\py37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "python 3.7\n",
    "bcolz              1.2.1\n",
    "numpy              1.21.5\n",
    "pytorch  1.11.0\n",
    "\n",
    "'''\n",
    "import os\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "def pretrained_word_embeddings(embed_path:str, over_writte:bool, special_tk:bool=True, freeze:bool=True):\n",
    "    ''' return a torch.nn.Embedding layer, utilizing the pre-trained word vector (e.g., Glove), add 'bos', 'eos', 'unk' and 'pad'.\n",
    "\n",
    "    :param embed_path: the path where pre-trained matrix cached (e.g., './glove.6B.300d.txt').\n",
    "    :param over_writte: force to rewritte the existing matrix.\n",
    "    :param special_tk: whether adding special token -- 'pad', 'unk', bos' and 'eos', at position 0, 1, 2 and 3 by default.\n",
    "    :param freeze: whether trainable.\n",
    "    :return: embed -> nn.Embedding, weights_matrix -> np.array, word2idx -> function, idx2word -> function, embed_dim -> int\n",
    "    '''\n",
    "    root_dir = embed_path.rsplit(\".\",1)[0]+\".dat\"\n",
    "    out_dir_word = embed_path.rsplit(\".\",1)[0]+\"_words.pkl\"\n",
    "    out_dir_idx = embed_path.rsplit(\".\",1)[0]+\"_idx.pkl\"\n",
    "    out_dir_idx2word = embed_path.rsplit(\".\", 1)[0] + \"_idx2word.pkl\"\n",
    "    if not all([os.path.exists(root_dir),os.path.exists(out_dir_word),os.path.exists(out_dir_idx)]) or over_writte:\n",
    "        ## process and cache glove ===========================================\n",
    "        words = []\n",
    "        idx = 0\n",
    "        _word2idx = {}\n",
    "        _idx2word = {}\n",
    "        vectors = bcolz.carray(np.zeros(1), rootdir=root_dir, mode='w')\n",
    "        with open(os.path.join(embed_path),\"rb\") as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                words.append(word)\n",
    "                _word2idx[word] = idx\n",
    "                _idx2word[idx]=word\n",
    "                idx += 1\n",
    "                vect = np.array(line[1:]).astype(float)\n",
    "                vectors.append(vect)\n",
    "        vectors = bcolz.carray(vectors[1:].reshape((idx, vect.shape[0])), rootdir=root_dir, mode='w')\n",
    "        vectors.flush()\n",
    "        pickle.dump(words, open(out_dir_word, 'wb'))\n",
    "        pickle.dump(_word2idx, open(out_dir_idx, 'wb'))\n",
    "        pickle.dump(_idx2word,open(out_dir_idx2word,'wb'))\n",
    "        print(\"dump word/idx at {}\".format(embed_path.rsplit(\"/\",1)[0]))\n",
    "        ## =======================================================\n",
    "    ## load glove\n",
    "    vectors = bcolz.open(root_dir)[:]\n",
    "    words = pickle.load(open(embed_path.rsplit(\".\",1)[0]+\"_words.pkl\", 'rb'))\n",
    "    _word2idx = pickle.load(open(embed_path.rsplit(\".\",1)[0]+\"_idx.pkl\", 'rb'))\n",
    "    _idx2word=pickle.load(open(embed_path.rsplit(\".\", 1)[0] + \"_idx2word.pkl\",'rb'))\n",
    "    print(\"Successfully load Golve from {}, the shape of cached matrix: {}\".format(embed_path.rsplit(\"/\",1)[0],vectors.shape))\n",
    "\n",
    "    word_num, embed_dim = vectors.shape\n",
    "    word_num += 4  if special_tk else 0  ## e.g., 400004\n",
    "    embedding_matrix = np.zeros((word_num, embed_dim))\n",
    "    if special_tk:\n",
    "        embedding_matrix[1] = np.random.normal(scale=0.6, size=(embed_dim, ))\n",
    "        embedding_matrix[2] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "        embedding_matrix[3] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "        embedding_matrix[4:,:] = vectors\n",
    "        weights_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "        pad_idx,unk_idx, bos_idx,eos_idx = 0,1,2,3\n",
    "        embed_layer= torch.nn.Embedding.from_pretrained(weights_matrix_tensor,freeze=freeze,padding_idx=pad_idx)\n",
    "        _word2idx = dict([(k,v+4) for k,v in _word2idx.items()])\n",
    "        _idx2word = dict([(k+4,v) for k,v in _idx2word.items()])\n",
    "        assert len(_word2idx) + 4 == embedding_matrix.shape[0]\n",
    "    else:\n",
    "        embedding_matrix[:,:] = vectors\n",
    "        weights_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "        embed_layer = torch.nn.Embedding.from_pretrained(weights_matrix_tensor,freeze=freeze)\n",
    "        assert len(_word2idx) == embedding_matrix.shape[0]\n",
    "\n",
    "    def word2idx(word:str):\n",
    "        if word == '<pad>': return 0\n",
    "        elif word == '<bos>': return 2\n",
    "        elif word == '<eos>': return 3\n",
    "        return _word2idx.get(word,1)\n",
    "    def idx2word(idx:int):\n",
    "        if idx == 0: return '<pad>'\n",
    "        elif idx == 1: return '<unk>'\n",
    "        elif idx == 2: return '<bos>'\n",
    "        elif idx == 3: return '<eos>'\n",
    "        return _idx2word.get(idx,'')\n",
    "    return embed_layer, embedding_matrix, word2idx,idx2word, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump word/idx at ../../word embeddings\n",
      "Successfully load Golve from ../../word embeddings, the shape of cached matrix: (400000, 100)\n"
     ]
    }
   ],
   "source": [
    "embed_layer, embedding_matrix, word2idx,idx2word, embed_dim = pretrained_word_embeddings('../../word embeddings/glove.6B.100d.txt',over_writte=True,special_tk=True,freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(embed_layer.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = embed_layer\n",
    "        self.rnn = nn.RNN(input_size=embed_dim,hidden_size=32,num_layers=1,batch_first=True)\n",
    "        self.fc1 = nn.Linear(32,128)\n",
    "        self.fc2 = nn.Linear(128,32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                     # 32,256,50\n",
    "        _,hns = self.rnn(x)                       # 1,32,32\n",
    "        x = hns.view(hns.size()[1],-1)            # reshape to 32,32\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        outprob = torch.sigmoid(self.fc3(x))      # batch_size, 1\n",
    "        return outprob.view(outprob.size()[0])    # reshape to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建网络模型\n",
    "myModel = MyModel()\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(myModel.parameters(),0.0001)\n",
    "\n",
    "# 训练的轮数\n",
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyModel                                  --\n",
       "├─Embedding: 1-1                         (40,000,400)\n",
       "├─RNN: 1-2                               4,288\n",
       "├─Linear: 1-3                            4,224\n",
       "├─Linear: 1-4                            4,128\n",
       "├─Linear: 1-5                            33\n",
       "=================================================================\n",
       "Total params: 40,013,073\n",
       "Trainable params: 12,673\n",
       "Non-trainable params: 40,000,400\n",
       "================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(myModel.embedding.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def correct_num(vec1,vec2):\n",
    "    result = (torch.abs(vec1-vec2)) <0.5\n",
    "    return torch.sum(result).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class myDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(myDataset,self).__init__()\n",
    "        self.review_list = norm_reviews\n",
    "        self.label_list = norm_sentiment\n",
    "    def __getitem__(self,idx):\n",
    "        return  self.review_list[idx],self.label_list[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     2,     45,    808,     41,     19,     11,   5209,    183,\n",
       "             8,   2029,     83,     17,     11,    321,   1631,    744,\n",
       "          1179,      5,   2999,     10,      4,    329,  17113,   2252,\n",
       "             9,   2645,     11,    901,     15,  21364,   2845,      6,\n",
       "             4,   2223,     18,  26888,      5,     38,      4,   2473,\n",
       "            18,  18222,      9,      4,   2157,     36,  26081,     27,\n",
       "           155,      4,    147,   5876,   1606,   7270,   4902,     28,\n",
       "             6,    114,     81,    111,     34,   4258,     65,     43,\n",
       "          4227,     41,     18,     40,    554,    393,    236,     49,\n",
       "          1122,  10437,      5,     45,    808,     24,     19,   4299,\n",
       "            16,  10971,   3192,     18,    153,   1861,     10,    428,\n",
       "             7,      4,   1139,    113,      7,     99,     37,   2850,\n",
       "             8,      1,      1,      1,     19,      4,    100,     45,\n",
       "          1155,  11391,     26,     52,      7,  10971,     13,  15406,\n",
       "            10,     86,     27,  12252,     45,    207,     11,   1656,\n",
       "           192,     28,      6,    114,     45,    466,    336,     55,\n",
       "          6064,     21,  17046,  74977,      5,     10,     41,     71,\n",
       "          1768,      8,   4449,    139,     75,     12,  11875,     12,\n",
       "          1963,      9,   3454,    252,     79,     11,    645,      5,\n",
       "            38,  12737,    465,      1,      1,      1,    111,     40,\n",
       "            34,      4,   3124,  14145,      7,     30,    436,      5,\n",
       "            38,     24,     19, 194149,     77,     12,   6701,   9321,\n",
       "         20985,     12,      9,     60,   4005,     77,     12,  12749,\n",
       "            12,     11,    357,   2845,      8,    246,    257,     21,\n",
       "          1099,      6,      3,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0]),\n",
       " 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=myDataset()\n",
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def myfunc(batch_data):\n",
    "    '''\n",
    "    batch_data: 32x2\n",
    "    '''\n",
    "    resData = []\n",
    "    resLabel = []\n",
    "    for i in batch_data:\n",
    "        resData.append(i[0])\n",
    "        resLabel.append(i[1])\n",
    "    return torch.tensor(np.array(resData),dtype=torch.int),torch.tensor(np.array(resLabel),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(train_dataset, batch_size=32,shuffle=True,collate_fn=myfunc,drop_last=True)\n",
    "testloader = data.DataLoader(test_dataset, batch_size=32,shuffle=True,collate_fn=myfunc,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------第 1 轮训练开始-------\n",
      "第1轮第100训练step时的loss: 0.6924270987510681\n",
      "第1轮第200训练step时的loss: 0.7003148794174194\n",
      "第1轮第300训练step时的loss: 0.6890363097190857\n",
      "第1轮第400训练step时的loss: 0.6936812400817871\n",
      "第1轮第500训练step时的loss: 0.6937609910964966\n",
      "第1轮第600训练step时的loss: 0.690085768699646\n",
      "第1轮第700训练step时的loss: 0.6949021816253662\n",
      "第1轮第800训练step时的loss: 0.6945669054985046\n",
      "第1轮第900训练step时的loss: 0.6934731006622314\n",
      "第1轮第1000训练step时的loss: 0.6912417411804199\n",
      "第1轮第1100训练step时的loss: 0.6946747303009033\n",
      "第1轮第1200训练step时的loss: 0.6973342895507812\n",
      "第1轮整体测试集上的Loss: 0.6926158464107758\n",
      "第1轮整体测试集上的Accuracy: 0.5043\n",
      "-------第 2 轮训练开始-------\n",
      "第2轮第100训练step时的loss: 0.6903793215751648\n",
      "第2轮第200训练step时的loss: 0.6859424710273743\n",
      "第2轮第300训练step时的loss: 0.693415641784668\n",
      "第2轮第400训练step时的loss: 0.6960420608520508\n",
      "第2轮第500训练step时的loss: 0.6924784779548645\n",
      "第2轮第600训练step时的loss: 0.6898850202560425\n",
      "第2轮第700训练step时的loss: 0.6881420016288757\n",
      "第2轮第800训练step时的loss: 0.6926108598709106\n",
      "第2轮第900训练step时的loss: 0.6870480179786682\n",
      "第2轮第1000训练step时的loss: 0.6992448568344116\n",
      "第2轮第1100训练step时的loss: 0.6912021636962891\n",
      "第2轮第1200训练step时的loss: 0.6963616013526917\n",
      "第2轮整体测试集上的Loss: 0.6924000348036106\n",
      "第2轮整体测试集上的Accuracy: 0.5037\n",
      "-------第 3 轮训练开始-------\n",
      "第3轮第100训练step时的loss: 0.6941348314285278\n",
      "第3轮第200训练step时的loss: 0.6968032121658325\n",
      "第3轮第300训练step时的loss: 0.6826844215393066\n",
      "第3轮第400训练step时的loss: 0.6862530708312988\n",
      "第3轮第500训练step时的loss: 0.6957263350486755\n",
      "第3轮第600训练step时的loss: 0.6834315061569214\n",
      "第3轮第700训练step时的loss: 0.7043266296386719\n",
      "第3轮第800训练step时的loss: 0.7007613182067871\n",
      "第3轮第900训练step时的loss: 0.6955917477607727\n",
      "第3轮第1000训练step时的loss: 0.7150763869285583\n",
      "第3轮第1100训练step时的loss: 0.6821665167808533\n",
      "第3轮第1200训练step时的loss: 0.7007647752761841\n",
      "第3轮整体测试集上的Loss: 0.6916897190559623\n",
      "第3轮整体测试集上的Accuracy: 0.5236\n",
      "-------第 4 轮训练开始-------\n",
      "第4轮第100训练step时的loss: 0.6851508617401123\n",
      "第4轮第200训练step时的loss: 0.7183064222335815\n",
      "第4轮第300训练step时的loss: 0.7039599418640137\n",
      "第4轮第400训练step时的loss: 0.6870340704917908\n",
      "第4轮第500训练step时的loss: 0.6817759871482849\n",
      "第4轮第600训练step时的loss: 0.6938897967338562\n",
      "第4轮第700训练step时的loss: 0.6760798692703247\n",
      "第4轮第800训练step时的loss: 0.683949887752533\n",
      "第4轮第900训练step时的loss: 0.6816835999488831\n",
      "第4轮第1000训练step时的loss: 0.6813653111457825\n",
      "第4轮第1100训练step时的loss: 0.6872482895851135\n",
      "第4轮第1200训练step时的loss: 0.6953119039535522\n",
      "第4轮整体测试集上的Loss: 0.6909433413201418\n",
      "第4轮整体测试集上的Accuracy: 0.519\n",
      "-------第 5 轮训练开始-------\n",
      "第5轮第100训练step时的loss: 0.6867697238922119\n",
      "第5轮第200训练step时的loss: 0.6919974684715271\n",
      "第5轮第300训练step时的loss: 0.6965385675430298\n",
      "第5轮第400训练step时的loss: 0.6844757199287415\n",
      "第5轮第500训练step时的loss: 0.690924882888794\n",
      "第5轮第600训练step时的loss: 0.6920576691627502\n",
      "第5轮第700训练step时的loss: 0.6778420209884644\n",
      "第5轮第800训练step时的loss: 0.7088625431060791\n",
      "第5轮第900训练step时的loss: 0.7163718938827515\n",
      "第5轮第1000训练step时的loss: 0.7042132019996643\n",
      "第5轮第1100训练step时的loss: 0.676827609539032\n",
      "第5轮第1200训练step时的loss: 0.6879458427429199\n",
      "第5轮整体测试集上的Loss: 0.6900480991754776\n",
      "第5轮整体测试集上的Accuracy: 0.5385\n",
      "-------第 6 轮训练开始-------\n",
      "第6轮第100训练step时的loss: 0.6804494857788086\n",
      "第6轮第200训练step时的loss: 0.674258828163147\n",
      "第6轮第300训练step时的loss: 0.6874133348464966\n",
      "第6轮第400训练step时的loss: 0.6295278668403625\n",
      "第6轮第500训练step时的loss: 0.6987432241439819\n",
      "第6轮第600训练step时的loss: 0.6962470412254333\n",
      "第6轮第700训练step时的loss: 0.694576621055603\n",
      "第6轮第800训练step时的loss: 0.6843026280403137\n",
      "第6轮第900训练step时的loss: 0.6938189268112183\n",
      "第6轮第1000训练step时的loss: 0.6945235729217529\n",
      "第6轮第1100训练step时的loss: 0.6913198828697205\n",
      "第6轮第1200训练step时的loss: 0.6691729426383972\n",
      "第6轮整体测试集上的Loss: 0.6892675916290182\n",
      "第6轮整体测试集上的Accuracy: 0.5476\n",
      "-------第 7 轮训练开始-------\n",
      "第7轮第100训练step时的loss: 0.709384560585022\n",
      "第7轮第200训练step时的loss: 0.6930086612701416\n",
      "第7轮第300训练step时的loss: 0.6604443192481995\n",
      "第7轮第400训练step时的loss: 0.6942722797393799\n",
      "第7轮第500训练step时的loss: 0.6775734424591064\n",
      "第7轮第600训练step时的loss: 0.7038755416870117\n",
      "第7轮第700训练step时的loss: 0.6592578887939453\n",
      "第7轮第800训练step时的loss: 0.6655772924423218\n",
      "第7轮第900训练step时的loss: 0.6883305311203003\n",
      "第7轮第1000训练step时的loss: 0.6574475765228271\n",
      "第7轮第1100训练step时的loss: 0.6894379258155823\n",
      "第7轮第1200训练step时的loss: 0.7058979868888855\n",
      "第7轮整体测试集上的Loss: 0.6871056879833067\n",
      "第7轮整体测试集上的Accuracy: 0.5703\n",
      "-------第 8 轮训练开始-------\n",
      "第8轮第100训练step时的loss: 0.727670431137085\n",
      "第8轮第200训练step时的loss: 0.696570098400116\n",
      "第8轮第300训练step时的loss: 0.6914997100830078\n",
      "第8轮第400训练step时的loss: 0.645500659942627\n",
      "第8轮第500训练step时的loss: 0.6661905646324158\n",
      "第8轮第600训练step时的loss: 0.6089472770690918\n",
      "第8轮第700训练step时的loss: 0.6628745794296265\n",
      "第8轮第800训练step时的loss: 0.7898367047309875\n",
      "第8轮第900训练step时的loss: 0.6612425446510315\n",
      "第8轮第1000训练step时的loss: 0.6197095513343811\n",
      "第8轮第1100训练step时的loss: 0.6282961964607239\n",
      "第8轮第1200训练step时的loss: 0.6930348873138428\n",
      "第8轮整体测试集上的Loss: 0.6883836912038999\n",
      "第8轮整体测试集上的Accuracy: 0.5801\n",
      "-------第 9 轮训练开始-------\n",
      "第9轮第100训练step时的loss: 0.6465586423873901\n",
      "第9轮第200训练step时的loss: 0.7096309661865234\n",
      "第9轮第300训练step时的loss: 0.6934195756912231\n",
      "第9轮第400训练step时的loss: 0.697485625743866\n",
      "第9轮第500训练step时的loss: 0.6858157515525818\n",
      "第9轮第600训练step时的loss: 0.6859287619590759\n",
      "第9轮第700训练step时的loss: 0.6916604042053223\n",
      "第9轮第800训练step时的loss: 0.687851071357727\n",
      "第9轮第900训练step时的loss: 0.6951368451118469\n",
      "第9轮第1000训练step时的loss: 0.6909747123718262\n",
      "第9轮第1100训练step时的loss: 0.701087474822998\n",
      "第9轮第1200训练step时的loss: 0.6894500255584717\n",
      "第9轮整体测试集上的Loss: 0.6887880341492147\n",
      "第9轮整体测试集上的Accuracy: 0.508\n",
      "-------第 10 轮训练开始-------\n",
      "第10轮第100训练step时的loss: 0.6904364228248596\n",
      "第10轮第200训练step时的loss: 0.7024208307266235\n",
      "第10轮第300训练step时的loss: 0.6910033822059631\n",
      "第10轮第400训练step时的loss: 0.6948773860931396\n",
      "第10轮第500训练step时的loss: 0.6865136623382568\n",
      "第10轮第600训练step时的loss: 0.6823928356170654\n",
      "第10轮第700训练step时的loss: 0.7078837156295776\n",
      "第10轮第800训练step时的loss: 0.6793187260627747\n",
      "第10轮第900训练step时的loss: 0.6984602212905884\n",
      "第10轮第1000训练step时的loss: 0.6807847619056702\n",
      "第10轮第1100训练step时的loss: 0.6964911222457886\n",
      "第10轮第1200训练step时的loss: 0.6864287257194519\n",
      "第10轮整体测试集上的Loss: 0.6889980971622162\n",
      "第10轮整体测试集上的Accuracy: 0.5135\n",
      "-------第 11 轮训练开始-------\n",
      "第11轮第100训练step时的loss: 0.6933655142784119\n",
      "第11轮第200训练step时的loss: 0.7063441872596741\n",
      "第11轮第300训练step时的loss: 0.6983369588851929\n",
      "第11轮第400训练step时的loss: 0.6943507194519043\n",
      "第11轮第500训练step时的loss: 0.6818963885307312\n",
      "第11轮第600训练step时的loss: 0.679829478263855\n",
      "第11轮第700训练step时的loss: 0.68851637840271\n",
      "第11轮第800训练step时的loss: 0.6849339604377747\n",
      "第11轮第900训练step时的loss: 0.6904999613761902\n",
      "第11轮第1000训练step时的loss: 0.7074962854385376\n",
      "第11轮第1100训练step时的loss: 0.689338207244873\n",
      "第11轮第1200训练step时的loss: 0.6713438034057617\n",
      "第11轮整体测试集上的Loss: 0.6890819784957212\n",
      "第11轮整体测试集上的Accuracy: 0.5172\n",
      "-------第 12 轮训练开始-------\n",
      "第12轮第100训练step时的loss: 0.7113955020904541\n",
      "第12轮第200训练step时的loss: 0.7095727920532227\n",
      "第12轮第300训练step时的loss: 0.6991257071495056\n",
      "第12轮第400训练step时的loss: 0.6840037703514099\n",
      "第12轮第500训练step时的loss: 0.6817000508308411\n",
      "第12轮第600训练step时的loss: 0.6789481043815613\n",
      "第12轮第700训练step时的loss: 0.6932010054588318\n",
      "第12轮第800训练step时的loss: 0.6849377155303955\n",
      "第12轮第900训练step时的loss: 0.6876764297485352\n",
      "第12轮第1000训练step时的loss: 0.6917902231216431\n",
      "第12轮第1100训练step时的loss: 0.6914077401161194\n",
      "第12轮第1200训练step时的loss: 0.6818400025367737\n",
      "第12轮整体测试集上的Loss: 0.689219608823331\n",
      "第12轮整体测试集上的Accuracy: 0.5141\n",
      "-------第 13 轮训练开始-------\n",
      "第13轮第100训练step时的loss: 0.6867932081222534\n",
      "第13轮第200训练step时的loss: 0.6930364370346069\n",
      "第13轮第300训练step时的loss: 0.676902711391449\n",
      "第13轮第400训练step时的loss: 0.6758596897125244\n",
      "第13轮第500训练step时的loss: 0.7091516852378845\n",
      "第13轮第600训练step时的loss: 0.7073003649711609\n",
      "第13轮第700训练step时的loss: 0.6605879664421082\n",
      "第13轮第800训练step时的loss: 0.6716035604476929\n",
      "第13轮第900训练step时的loss: 0.6859667301177979\n",
      "第13轮第1000训练step时的loss: 0.6741440296173096\n",
      "第13轮第1100训练step时的loss: 0.6825961470603943\n",
      "第13轮第1200训练step时的loss: 0.7071993350982666\n",
      "第13轮整体测试集上的Loss: 0.6891257568341506\n",
      "第13轮整体测试集上的Accuracy: 0.5281\n",
      "-------第 14 轮训练开始-------\n",
      "第14轮第100训练step时的loss: 0.6766506433486938\n",
      "第14轮第200训练step时的loss: 0.6783546209335327\n",
      "第14轮第300训练step时的loss: 0.6929431557655334\n",
      "第14轮第400训练step时的loss: 0.7045828700065613\n",
      "第14轮第500训练step时的loss: 0.6824778318405151\n",
      "第14轮第600训练step时的loss: 0.6956112384796143\n",
      "第14轮第700训练step时的loss: 0.6776695847511292\n",
      "第14轮第800训练step时的loss: 0.6926093697547913\n",
      "第14轮第900训练step时的loss: 0.6380733251571655\n",
      "第14轮第1000训练step时的loss: 0.6984950304031372\n",
      "第14轮第1100训练step时的loss: 0.6536501049995422\n",
      "第14轮第1200训练step时的loss: 0.6913139820098877\n",
      "第14轮整体测试集上的Loss: 0.6876986012299419\n",
      "第14轮整体测试集上的Accuracy: 0.5629\n",
      "-------第 15 轮训练开始-------\n",
      "第15轮第100训练step时的loss: 0.6675305366516113\n",
      "第15轮第200训练step时的loss: 0.5666471123695374\n",
      "第15轮第300训练step时的loss: 0.7049627900123596\n",
      "第15轮第400训练step时的loss: 0.6356614828109741\n",
      "第15轮第500训练step时的loss: 0.6522992253303528\n",
      "第15轮第600训练step时的loss: 0.6841323375701904\n",
      "第15轮第700训练step时的loss: 0.6792060136795044\n",
      "第15轮第800训练step时的loss: 0.694535493850708\n",
      "第15轮第900训练step时的loss: 0.6731918454170227\n",
      "第15轮第1000训练step时的loss: 0.5806459188461304\n",
      "第15轮第1100训练step时的loss: 0.6778107285499573\n",
      "第15轮第1200训练step时的loss: 0.666866660118103\n",
      "第15轮整体测试集上的Loss: 0.6876166654575584\n",
      "第15轮整体测试集上的Accuracy: 0.5413\n",
      "-------第 16 轮训练开始-------\n",
      "第16轮第100训练step时的loss: 0.6902694702148438\n",
      "第16轮第200训练step时的loss: 0.7245043516159058\n",
      "第16轮第300训练step时的loss: 0.6692525148391724\n",
      "第16轮第400训练step时的loss: 0.6696694493293762\n",
      "第16轮第500训练step时的loss: 0.6658731698989868\n",
      "第16轮第600训练step时的loss: 0.6951968669891357\n",
      "第16轮第700训练step时的loss: 0.6849341988563538\n",
      "第16轮第800训练step时的loss: 0.7136505842208862\n",
      "第16轮第900训练step时的loss: 0.6641280055046082\n",
      "第16轮第1000训练step时的loss: 0.6857459545135498\n",
      "第16轮第1100训练step时的loss: 0.6482114791870117\n",
      "第16轮第1200训练step时的loss: 0.7063382267951965\n",
      "第16轮整体测试集上的Loss: 0.6857495416457263\n",
      "第16轮整体测试集上的Accuracy: 0.604\n",
      "-------第 17 轮训练开始-------\n",
      "第17轮第100训练step时的loss: 0.6341574192047119\n",
      "第17轮第200训练step时的loss: 0.637167751789093\n",
      "第17轮第300训练step时的loss: 0.7247635722160339\n",
      "第17轮第400训练step时的loss: 0.6124190092086792\n",
      "第17轮第500训练step时的loss: 0.6981296539306641\n",
      "第17轮第600训练step时的loss: 0.6784325242042542\n",
      "第17轮第700训练step时的loss: 0.6767295002937317\n",
      "第17轮第800训练step时的loss: 0.6908921003341675\n",
      "第17轮第900训练step时的loss: 0.7187190055847168\n",
      "第17轮第1000训练step时的loss: 0.6997071504592896\n",
      "第17轮第1100训练step时的loss: 0.6846116185188293\n",
      "第17轮第1200训练step时的loss: 0.7025703191757202\n",
      "第17轮整体测试集上的Loss: 0.6859771197670186\n",
      "第17轮整体测试集上的Accuracy: 0.524\n",
      "-------第 18 轮训练开始-------\n",
      "第18轮第100训练step时的loss: 0.7074256539344788\n",
      "第18轮第200训练step时的loss: 0.6944522261619568\n",
      "第18轮第300训练step时的loss: 0.6652618646621704\n",
      "第18轮第400训练step时的loss: 0.6650317311286926\n",
      "第18轮第500训练step时的loss: 0.6972315907478333\n",
      "第18轮第600训练step时的loss: 0.6893544793128967\n",
      "第18轮第700训练step时的loss: 0.6806015372276306\n",
      "第18轮第800训练step时的loss: 0.6835256814956665\n",
      "第18轮第900训练step时的loss: 0.681736171245575\n",
      "第18轮第1000训练step时的loss: 0.7192069292068481\n",
      "第18轮第1100训练step时的loss: 0.6140620112419128\n",
      "第18轮第1200训练step时的loss: 0.6257389783859253\n",
      "第18轮整体测试集上的Loss: 0.6857236793365573\n",
      "第18轮整体测试集上的Accuracy: 0.534\n",
      "-------第 19 轮训练开始-------\n",
      "第19轮第100训练step时的loss: 0.718001127243042\n",
      "第19轮第200训练step时的loss: 0.6987136602401733\n",
      "第19轮第300训练step时的loss: 0.6640954613685608\n",
      "第19轮第400训练step时的loss: 0.6995629668235779\n",
      "第19轮第500训练step时的loss: 0.6657350659370422\n",
      "第19轮第600训练step时的loss: 0.6761343479156494\n",
      "第19轮第700训练step时的loss: 0.6996638774871826\n",
      "第19轮第800训练step时的loss: 0.707844614982605\n",
      "第19轮第900训练step时的loss: 0.6831267476081848\n",
      "第19轮第1000训练step时的loss: 0.6861011385917664\n",
      "第19轮第1100训练step时的loss: 0.6924927830696106\n",
      "第19轮第1200训练step时的loss: 0.695463240146637\n",
      "第19轮整体测试集上的Loss: 0.6858748970063109\n",
      "第19轮整体测试集上的Accuracy: 0.519\n",
      "-------第 20 轮训练开始-------\n",
      "第20轮第100训练step时的loss: 0.6882417798042297\n",
      "第20轮第200训练step时的loss: 0.6680884957313538\n",
      "第20轮第300训练step时的loss: 0.6582860350608826\n",
      "第20轮第400训练step时的loss: 0.7028336524963379\n",
      "第20轮第500训练step时的loss: 0.6644837260246277\n",
      "第20轮第600训练step时的loss: 0.7070003747940063\n",
      "第20轮第700训练step时的loss: 0.691254198551178\n",
      "第20轮第800训练step时的loss: 0.6572570204734802\n",
      "第20轮第900训练step时的loss: 0.6992334127426147\n",
      "第20轮第1000训练step时的loss: 0.6860609650611877\n",
      "第20轮第1100训练step时的loss: 0.6951956152915955\n",
      "第20轮第1200训练step时的loss: 0.7059956789016724\n",
      "第20轮整体测试集上的Loss: 0.6858792054443024\n",
      "第20轮整体测试集上的Accuracy: 0.5245\n",
      "-------第 21 轮训练开始-------\n",
      "第21轮第100训练step时的loss: 0.6831773519515991\n",
      "第21轮第200训练step时的loss: 0.667769730091095\n",
      "第21轮第300训练step时的loss: 0.680582582950592\n",
      "第21轮第400训练step时的loss: 0.6889070868492126\n",
      "第21轮第500训练step时的loss: 0.6871204972267151\n",
      "第21轮第600训练step时的loss: 0.6825059652328491\n",
      "第21轮第700训练step时的loss: 0.7101424932479858\n",
      "第21轮第800训练step时的loss: 0.5521951913833618\n",
      "第21轮第900训练step时的loss: 0.6334788799285889\n",
      "第21轮第1000训练step时的loss: 0.6904036402702332\n",
      "第21轮第1100训练step时的loss: 0.5899540185928345\n",
      "第21轮第1200训练step时的loss: 0.6642431020736694\n",
      "第21轮整体测试集上的Loss: 0.6836639082875735\n",
      "第21轮整体测试集上的Accuracy: 0.6539\n",
      "-------第 22 轮训练开始-------\n",
      "第22轮第100训练step时的loss: 0.6907928586006165\n",
      "第22轮第200训练step时的loss: 0.6741185188293457\n",
      "第22轮第300训练step时的loss: 0.6374165415763855\n",
      "第22轮第400训练step时的loss: 0.6706647276878357\n",
      "第22轮第500训练step时的loss: 0.7292311787605286\n",
      "第22轮第600训练step时的loss: 0.6496869921684265\n",
      "第22轮第700训练step时的loss: 0.7186344861984253\n",
      "第22轮第800训练step时的loss: 0.660039484500885\n",
      "第22轮第900训练step时的loss: 0.6911613345146179\n",
      "第22轮第1000训练step时的loss: 0.6658337116241455\n",
      "第22轮第1100训练step时的loss: 0.6940781474113464\n",
      "第22轮第1200训练step时的loss: 0.6788114309310913\n",
      "第22轮整体测试集上的Loss: 0.6838822623667045\n",
      "第22轮整体测试集上的Accuracy: 0.5184\n",
      "-------第 23 轮训练开始-------\n",
      "第23轮第100训练step时的loss: 0.690112829208374\n",
      "第23轮第200训练step时的loss: 0.6744235157966614\n",
      "第23轮第300训练step时的loss: 0.6916939616203308\n",
      "第23轮第400训练step时的loss: 0.6783358454704285\n",
      "第23轮第500训练step时的loss: 0.6805572509765625\n",
      "第23轮第600训练step时的loss: 0.6672203540802002\n",
      "第23轮第700训练step时的loss: 0.7290676832199097\n",
      "第23轮第800训练step时的loss: 0.6879112124443054\n",
      "第23轮第900训练step时的loss: 0.6922531127929688\n",
      "第23轮第1000训练step时的loss: 0.7244192957878113\n",
      "第23轮第1100训练step时的loss: 0.6758632063865662\n",
      "第23轮第1200训练step时的loss: 0.6974434852600098\n",
      "第23轮整体测试集上的Loss: 0.6839765512707374\n",
      "第23轮整体测试集上的Accuracy: 0.5254\n",
      "-------第 24 轮训练开始-------\n",
      "第24轮第100训练step时的loss: 0.6438397765159607\n",
      "第24轮第200训练step时的loss: 0.667691707611084\n",
      "第24轮第300训练step时的loss: 0.6498249769210815\n",
      "第24轮第400训练step时的loss: 0.652861475944519\n",
      "第24轮第500训练step时的loss: 0.7509070634841919\n",
      "第24轮第600训练step时的loss: 0.6813722252845764\n",
      "第24轮第700训练step时的loss: 0.6012714505195618\n",
      "第24轮第800训练step时的loss: 0.6162912845611572\n",
      "第24轮第900训练step时的loss: 0.704099178314209\n",
      "第24轮第1000训练step时的loss: 0.6700401306152344\n",
      "第24轮第1100训练step时的loss: 0.6089451909065247\n",
      "第24轮第1200训练step时的loss: 0.6938529014587402\n",
      "第24轮整体测试集上的Loss: 0.6828845637555943\n",
      "第24轮整体测试集上的Accuracy: 0.6021\n",
      "-------第 25 轮训练开始-------\n",
      "第25轮第100训练step时的loss: 0.633332371711731\n",
      "第25轮第200训练step时的loss: 0.6178839206695557\n",
      "第25轮第300训练step时的loss: 0.6027124524116516\n",
      "第25轮第400训练step时的loss: 0.6490059494972229\n",
      "第25轮第500训练step时的loss: 0.6581360101699829\n",
      "第25轮第600训练step时的loss: 0.6605938076972961\n",
      "第25轮第700训练step时的loss: 0.737725019454956\n",
      "第25轮第800训练step时的loss: 0.6275888681411743\n",
      "第25轮第900训练step时的loss: 0.6799755692481995\n",
      "第25轮第1000训练step时的loss: 0.684359073638916\n",
      "第25轮第1100训练step时的loss: 0.6922786235809326\n",
      "第25轮第1200训练step时的loss: 0.6343642473220825\n",
      "第25轮整体测试集上的Loss: 0.682764306137195\n",
      "第25轮整体测试集上的Accuracy: 0.538\n",
      "-------第 26 轮训练开始-------\n",
      "第26轮第100训练step时的loss: 0.7325950264930725\n",
      "第26轮第200训练step时的loss: 0.6385419368743896\n",
      "第26轮第300训练step时的loss: 0.6103096008300781\n",
      "第26轮第400训练step时的loss: 0.6885005235671997\n",
      "第26轮第500训练step时的loss: 0.6505171656608582\n",
      "第26轮第600训练step时的loss: 0.6692768931388855\n",
      "第26轮第700训练step时的loss: 0.6963319182395935\n",
      "第26轮第800训练step时的loss: 0.6645624041557312\n",
      "第26轮第900训练step时的loss: 0.6182091236114502\n",
      "第26轮第1000训练step时的loss: 0.6744112968444824\n",
      "第26轮第1100训练step时的loss: 0.5729638934135437\n",
      "第26轮第1200训练step时的loss: 0.6801556944847107\n",
      "第26轮整体测试集上的Loss: 0.6829066623950145\n",
      "第26轮整体测试集上的Accuracy: 0.5196\n",
      "-------第 27 轮训练开始-------\n",
      "第27轮第100训练step时的loss: 0.660879909992218\n",
      "第27轮第200训练step时的loss: 0.6598977446556091\n",
      "第27轮第300训练step时的loss: 0.7964165806770325\n",
      "第27轮第400训练step时的loss: 0.5833138823509216\n",
      "第27轮第500训练step时的loss: 0.6692566871643066\n",
      "第27轮第600训练step时的loss: 0.6333181262016296\n",
      "第27轮第700训练step时的loss: 0.6400609016418457\n",
      "第27轮第800训练step时的loss: 0.6224123239517212\n",
      "第27轮第900训练step时的loss: 0.6343410015106201\n",
      "第27轮第1000训练step时的loss: 0.6981801986694336\n",
      "第27轮第1100训练step时的loss: 0.6228049993515015\n",
      "第27轮第1200训练step时的loss: 0.6050894260406494\n",
      "第27轮整体测试集上的Loss: 0.6816402658225804\n",
      "第27轮整体测试集上的Accuracy: 0.6096\n",
      "-------第 28 轮训练开始-------\n",
      "第28轮第100训练step时的loss: 0.6053272485733032\n",
      "第28轮第200训练step时的loss: 0.5661325454711914\n",
      "第28轮第300训练step时的loss: 0.6147764921188354\n",
      "第28轮第400训练step时的loss: 0.6931084394454956\n",
      "第28轮第500训练step时的loss: 0.6981087923049927\n",
      "第28轮第600训练step时的loss: 0.6934577822685242\n",
      "第28轮第700训练step时的loss: 0.7003251314163208\n",
      "第28轮第800训练step时的loss: 0.7112606763839722\n",
      "第28轮第900训练step时的loss: 0.5491223931312561\n",
      "第28轮第1000训练step时的loss: 0.6402995586395264\n",
      "第28轮第1100训练step时的loss: 0.6210047602653503\n",
      "第28轮第1200训练step时的loss: 0.6342251896858215\n",
      "第28轮整体测试集上的Loss: 0.6805810793661154\n",
      "第28轮整体测试集上的Accuracy: 0.6439\n",
      "-------第 29 轮训练开始-------\n",
      "第29轮第100训练step时的loss: 0.6207072138786316\n",
      "第29轮第200训练step时的loss: 0.6988668441772461\n",
      "第29轮第300训练step时的loss: 0.6878513097763062\n",
      "第29轮第400训练step时的loss: 0.702149510383606\n",
      "第29轮第500训练step时的loss: 0.6883920431137085\n",
      "第29轮第600训练step时的loss: 0.6247769594192505\n",
      "第29轮第700训练step时的loss: 0.6291759014129639\n",
      "第29轮第800训练step时的loss: 0.6161325573921204\n",
      "第29轮第900训练step时的loss: 0.6602951884269714\n",
      "第29轮第1000训练step时的loss: 0.634395956993103\n",
      "第29轮第1100训练step时的loss: 0.6487090587615967\n",
      "第29轮第1200训练step时的loss: 0.6134873628616333\n",
      "第29轮整体测试集上的Loss: 0.6792938287718226\n",
      "第29轮整体测试集上的Accuracy: 0.6418\n",
      "-------第 30 轮训练开始-------\n",
      "第30轮第100训练step时的loss: 0.658769965171814\n",
      "第30轮第200训练step时的loss: 0.5803848505020142\n",
      "第30轮第300训练step时的loss: 0.5491795539855957\n",
      "第30轮第400训练step时的loss: 0.6643356680870056\n",
      "第30轮第500训练step时的loss: 0.6842238903045654\n",
      "第30轮第600训练step时的loss: 0.7505000233650208\n",
      "第30轮第700训练step时的loss: 0.6507140398025513\n",
      "第30轮第800训练step时的loss: 0.6343699097633362\n",
      "第30轮第900训练step时的loss: 0.6295260787010193\n",
      "第30轮第1000训练step时的loss: 0.6661164164543152\n",
      "第30轮第1100训练step时的loss: 0.6396491527557373\n",
      "第30轮第1200训练step时的loss: 0.5831701755523682\n",
      "第30轮整体测试集上的Loss: 0.6787130080863961\n",
      "第30轮整体测试集上的Accuracy: 0.5891\n",
      "-------第 31 轮训练开始-------\n",
      "第31轮第100训练step时的loss: 0.6462185382843018\n",
      "第31轮第200训练step时的loss: 0.6947506666183472\n",
      "第31轮第300训练step时的loss: 0.599051296710968\n",
      "第31轮第400训练step时的loss: 0.6172715425491333\n",
      "第31轮第500训练step时的loss: 0.6574152708053589\n",
      "第31轮第600训练step时的loss: 0.6367915272712708\n",
      "第31轮第700训练step时的loss: 0.6837619543075562\n",
      "第31轮第800训练step时的loss: 0.5774985551834106\n",
      "第31轮第900训练step时的loss: 0.7315412163734436\n",
      "第31轮第1000训练step时的loss: 0.6387466192245483\n",
      "第31轮第1100训练step时的loss: 0.6197799444198608\n",
      "第31轮第1200训练step时的loss: 0.63297039270401\n",
      "第31轮整体测试集上的Loss: 0.6772567902632347\n",
      "第31轮整体测试集上的Accuracy: 0.6685\n",
      "-------第 32 轮训练开始-------\n",
      "第32轮第100训练step时的loss: 0.665520191192627\n",
      "第32轮第200训练step时的loss: 0.7336974143981934\n",
      "第32轮第300训练step时的loss: 0.6855978965759277\n",
      "第32轮第400训练step时的loss: 0.6824986338615417\n",
      "第32轮第500训练step时的loss: 0.6744149327278137\n",
      "第32轮第600训练step时的loss: 0.7175570130348206\n",
      "第32轮第700训练step时的loss: 0.6086876392364502\n",
      "第32轮第800训练step时的loss: 0.547076404094696\n",
      "第32轮第900训练step时的loss: 0.6295706629753113\n",
      "第32轮第1000训练step时的loss: 0.5602648258209229\n",
      "第32轮第1100训练step时的loss: 0.6001361012458801\n",
      "第32轮第1200训练step时的loss: 0.579472541809082\n",
      "第32轮整体测试集上的Loss: 0.6770454042423995\n",
      "第32轮整体测试集上的Accuracy: 0.6079\n",
      "-------第 33 轮训练开始-------\n",
      "第33轮第100训练step时的loss: 0.6811025738716125\n",
      "第33轮第200训练step时的loss: 0.6268227100372314\n",
      "第33轮第300训练step时的loss: 0.6971036791801453\n",
      "第33轮第400训练step时的loss: 0.6896790266036987\n",
      "第33轮第500训练step时的loss: 0.6982157230377197\n",
      "第33轮第600训练step时的loss: 0.662264347076416\n",
      "第33轮第700训练step时的loss: 0.6872952580451965\n",
      "第33轮第800训练step时的loss: 0.6764000654220581\n",
      "第33轮第900训练step时的loss: 0.6863917112350464\n",
      "第33轮第1000训练step时的loss: 0.6924031376838684\n",
      "第33轮第1100训练step时的loss: 0.6793638467788696\n",
      "第33轮第1200训练step时的loss: 0.6736968755722046\n",
      "第33轮整体测试集上的Loss: 0.6772096649148063\n",
      "第33轮整体测试集上的Accuracy: 0.5319\n",
      "-------第 34 轮训练开始-------\n",
      "第34轮第100训练step时的loss: 0.737923264503479\n",
      "第34轮第200训练step时的loss: 0.6675174236297607\n",
      "第34轮第300训练step时的loss: 0.6448293924331665\n",
      "第34轮第400训练step时的loss: 0.7096946835517883\n",
      "第34轮第500训练step时的loss: 0.6337916254997253\n",
      "第34轮第600训练step时的loss: 0.6237255334854126\n",
      "第34轮第700训练step时的loss: 0.6420971751213074\n",
      "第34轮第800训练step时的loss: 0.6299133896827698\n",
      "第34轮第900训练step时的loss: 0.5318833589553833\n",
      "第34轮第1000训练step时的loss: 0.6835066676139832\n",
      "第34轮第1100训练step时的loss: 0.6769236326217651\n",
      "第34轮第1200训练step时的loss: 0.5441603660583496\n",
      "第34轮整体测试集上的Loss: 0.675526546477305\n",
      "第34轮整体测试集上的Accuracy: 0.6774\n",
      "-------第 35 轮训练开始-------\n",
      "第35轮第100训练step时的loss: 0.6948360204696655\n",
      "第35轮第200训练step时的loss: 0.612582802772522\n",
      "第35轮第300训练step时的loss: 0.6493803858757019\n",
      "第35轮第400训练step时的loss: 0.5895100831985474\n",
      "第35轮第500训练step时的loss: 0.6878387331962585\n",
      "第35轮第600训练step时的loss: 0.6990135312080383\n",
      "第35轮第700训练step时的loss: 0.6532158851623535\n",
      "第35轮第800训练step时的loss: 0.6647459268569946\n",
      "第35轮第900训练step时的loss: 0.6844725608825684\n",
      "第35轮第1000训练step时的loss: 0.6762772798538208\n",
      "第35轮第1100训练step时的loss: 0.6876051425933838\n",
      "第35轮第1200训练step时的loss: 0.6752490997314453\n",
      "第35轮整体测试集上的Loss: 0.6759325658636434\n",
      "第35轮整体测试集上的Accuracy: 0.5234\n",
      "-------第 36 轮训练开始-------\n",
      "第36轮第100训练step时的loss: 0.704272985458374\n",
      "第36轮第200训练step时的loss: 0.7032743692398071\n",
      "第36轮第300训练step时的loss: 0.6749662160873413\n",
      "第36轮第400训练step时的loss: 0.7009591460227966\n",
      "第36轮第500训练step时的loss: 0.7076852321624756\n",
      "第36轮第600训练step时的loss: 0.6732832193374634\n",
      "第36轮第700训练step时的loss: 0.7150289416313171\n",
      "第36轮第800训练step时的loss: 0.6834895014762878\n",
      "第36轮第900训练step时的loss: 0.6929643750190735\n",
      "第36轮第1000训练step时的loss: 0.6976004838943481\n",
      "第36轮第1100训练step时的loss: 0.6899021863937378\n",
      "第36轮第1200训练step时的loss: 0.6578878164291382\n",
      "第36轮整体测试集上的Loss: 0.6757957583526812\n",
      "第36轮整体测试集上的Accuracy: 0.5689\n",
      "-------第 37 轮训练开始-------\n",
      "第37轮第100训练step时的loss: 0.6729766130447388\n",
      "第37轮第200训练step时的loss: 0.6214891076087952\n",
      "第37轮第300训练step时的loss: 0.6457955837249756\n",
      "第37轮第400训练step时的loss: 0.675869882106781\n",
      "第37轮第500训练step时的loss: 0.6876574158668518\n",
      "第37轮第600训练step时的loss: 0.6676726341247559\n",
      "第37轮第700训练step时的loss: 0.6078713536262512\n",
      "第37轮第800训练step时的loss: 0.5524506568908691\n",
      "第37轮第900训练step时的loss: 0.5878357887268066\n",
      "第37轮第1000训练step时的loss: 0.5465410351753235\n",
      "第37轮第1100训练step时的loss: 0.6773679256439209\n",
      "第37轮第1200训练step时的loss: 0.6008140444755554\n",
      "第37轮整体测试集上的Loss: 0.6749756188826972\n",
      "第37轮整体测试集上的Accuracy: 0.6236\n",
      "-------第 38 轮训练开始-------\n",
      "第38轮第100训练step时的loss: 0.6782177090644836\n",
      "第38轮第200训练step时的loss: 0.7106224298477173\n",
      "第38轮第300训练step时的loss: 0.7147055864334106\n",
      "第38轮第400训练step时的loss: 0.6765224933624268\n",
      "第38轮第500训练step时的loss: 0.66139155626297\n",
      "第38轮第600训练step时的loss: 0.6708157062530518\n",
      "第38轮第700训练step时的loss: 0.669521152973175\n",
      "第38轮第800训练step时的loss: 0.6552937626838684\n",
      "第38轮第900训练step时的loss: 0.7229530215263367\n",
      "第38轮第1000训练step时的loss: 0.7029761672019958\n",
      "第38轮第1100训练step时的loss: 0.7020081281661987\n",
      "第38轮第1200训练step时的loss: 0.6964584589004517\n",
      "第38轮整体测试集上的Loss: 0.675239770098837\n",
      "第38轮整体测试集上的Accuracy: 0.5256\n",
      "-------第 39 轮训练开始-------\n",
      "第39轮第100训练step时的loss: 0.696854829788208\n",
      "第39轮第200训练step时的loss: 0.6885325908660889\n",
      "第39轮第300训练step时的loss: 0.6559759378433228\n",
      "第39轮第400训练step时的loss: 0.6838500499725342\n",
      "第39轮第500训练step时的loss: 0.6499248743057251\n",
      "第39轮第600训练step时的loss: 0.6559364199638367\n",
      "第39轮第700训练step时的loss: 0.7084541320800781\n",
      "第39轮第800训练step时的loss: 0.6830233931541443\n",
      "第39轮第900训练step时的loss: 0.652560293674469\n",
      "第39轮第1000训练step时的loss: 0.6677200198173523\n",
      "第39轮第1100训练step时的loss: 0.6869344115257263\n",
      "第39轮第1200训练step时的loss: 0.6789025068283081\n",
      "第39轮整体测试集上的Loss: 0.6749061204904517\n",
      "第39轮整体测试集上的Accuracy: 0.584\n",
      "-------第 40 轮训练开始-------\n",
      "第40轮第100训练step时的loss: 0.6008033156394958\n",
      "第40轮第200训练step时的loss: 0.6308644413948059\n",
      "第40轮第300训练step时的loss: 0.6855661273002625\n",
      "第40轮第400训练step时的loss: 0.5344017744064331\n",
      "第40轮第500训练step时的loss: 0.6072535514831543\n",
      "第40轮第600训练step时的loss: 0.5652705430984497\n",
      "第40轮第700训练step时的loss: 0.7383900880813599\n",
      "第40轮第800训练step时的loss: 0.7185584902763367\n",
      "第40轮第1100训练step时的loss: 0.6705486178398132\n",
      "第40轮第1200训练step时的loss: 0.6765281558036804\n",
      "第40轮整体测试集上的Loss: 0.675299515405622\n",
      "第40轮整体测试集上的Accuracy: 0.5147\n",
      "-------第 41 轮训练开始-------\n",
      "第41轮第100训练step时的loss: 0.6953914761543274\n",
      "第41轮第200训练step时的loss: 0.6858339309692383\n",
      "第41轮第300训练step时的loss: 0.6651151776313782\n",
      "第41轮第400训练step时的loss: 0.6843159198760986\n",
      "第41轮第500训练step时的loss: 0.68076491355896\n",
      "第41轮第600训练step时的loss: 0.7064496874809265\n",
      "第41轮第700训练step时的loss: 0.6531885862350464\n",
      "第41轮第800训练step时的loss: 0.6667487621307373\n",
      "第41轮第900训练step时的loss: 0.682180643081665\n",
      "第41轮第1000训练step时的loss: 0.6577248573303223\n",
      "第41轮第1100训练step时的loss: 0.6413915753364563\n",
      "第41轮第1200训练step时的loss: 0.6791530847549438\n",
      "第41轮整体测试集上的Loss: 0.6753072048362119\n",
      "第41轮整体测试集上的Accuracy: 0.5482\n",
      "-------第 42 轮训练开始-------\n",
      "第42轮第100训练step时的loss: 0.7855886220932007\n",
      "第42轮第200训练step时的loss: 0.6416794061660767\n",
      "第42轮第300训练step时的loss: 0.6791965365409851\n",
      "第42轮第400训练step时的loss: 0.6797785758972168\n",
      "第42轮第500训练step时的loss: 0.6541747450828552\n",
      "第42轮第600训练step时的loss: 0.6803423762321472\n",
      "第42轮第700训练step时的loss: 0.659288763999939\n",
      "第42轮第800训练step时的loss: 0.7029279470443726\n",
      "第42轮第900训练step时的loss: 0.6832847595214844\n",
      "第42轮第1000训练step时的loss: 0.6964892745018005\n",
      "第42轮第1100训练step时的loss: 0.6838304996490479\n",
      "第42轮第1200训练step时的loss: 0.7231678366661072\n",
      "第42轮整体测试集上的Loss: 0.6756331052367321\n",
      "第42轮整体测试集上的Accuracy: 0.5157\n",
      "-------第 43 轮训练开始-------\n",
      "第43轮第100训练step时的loss: 0.6860144138336182\n",
      "第43轮第200训练step时的loss: 0.677891194820404\n",
      "第43轮第300训练step时的loss: 0.7071054577827454\n",
      "第43轮第400训练step时的loss: 0.6855963468551636\n",
      "第43轮第500训练step时的loss: 0.6967635154724121\n",
      "第43轮第600训练step时的loss: 0.6828566193580627\n",
      "第43轮第700训练step时的loss: 0.6798747181892395\n",
      "第43轮第800训练step时的loss: 0.6709136962890625\n",
      "第43轮第900训练step时的loss: 0.6944161653518677\n",
      "第43轮第1000训练step时的loss: 0.6710727214813232\n",
      "第43轮第1100训练step时的loss: 0.712028980255127\n",
      "第43轮第1200训练step时的loss: 0.65777987241745\n",
      "第43轮整体测试集上的Loss: 0.6759490028010391\n",
      "第43轮整体测试集上的Accuracy: 0.525\n",
      "-------第 44 轮训练开始-------\n",
      "第44轮第100训练step时的loss: 0.665401816368103\n",
      "第44轮第200训练step时的loss: 0.654332160949707\n",
      "第44轮第300训练step时的loss: 0.725795328617096\n",
      "第44轮第400训练step时的loss: 0.6773446202278137\n",
      "第44轮第500训练step时的loss: 0.6791639924049377\n",
      "第44轮第600训练step时的loss: 0.6787145733833313\n",
      "第44轮第700训练step时的loss: 0.6535035371780396\n",
      "第44轮第800训练step时的loss: 0.6471560001373291\n",
      "第44轮第900训练step时的loss: 0.6656673550605774\n",
      "第44轮第1000训练step时的loss: 0.6591153740882874\n",
      "第44轮第1100训练step时的loss: 0.6951026916503906\n",
      "第44轮第1200训练step时的loss: 0.675274133682251\n",
      "第44轮整体测试集上的Loss: 0.6762416330524362\n",
      "第44轮整体测试集上的Accuracy: 0.526\n",
      "-------第 45 轮训练开始-------\n",
      "第45轮第100训练step时的loss: 0.6591360569000244\n",
      "第45轮第200训练step时的loss: 0.6639246344566345\n",
      "第45轮第300训练step时的loss: 0.6685115098953247\n",
      "第45轮第400训练step时的loss: 0.7008212804794312\n",
      "第45轮第500训练step时的loss: 0.6546528935432434\n",
      "第45轮第600训练step时的loss: 0.6814591288566589\n",
      "第45轮第700训练step时的loss: 0.6983562707901001\n",
      "第45轮第800训练step时的loss: 0.6500551104545593\n",
      "第45轮第900训练step时的loss: 0.6936379075050354\n",
      "第45轮第1000训练step时的loss: 0.703596293926239\n",
      "第45轮第1100训练step时的loss: 0.6974313855171204\n",
      "第45轮第1200训练step时的loss: 0.7215647101402283\n",
      "第45轮整体测试集上的Loss: 0.676596195157352\n",
      "第45轮整体测试集上的Accuracy: 0.5188\n",
      "-------第 46 轮训练开始-------\n",
      "第46轮第100训练step时的loss: 0.6634781956672668\n",
      "第46轮第200训练step时的loss: 0.7002909779548645\n",
      "第46轮第300训练step时的loss: 0.6914380192756653\n",
      "第46轮第400训练step时的loss: 0.6919099688529968\n",
      "第46轮第500训练step时的loss: 0.6986502408981323\n",
      "第46轮第600训练step时的loss: 0.7111586332321167\n",
      "第46轮第700训练step时的loss: 0.6500717401504517\n",
      "第46轮第800训练step时的loss: 0.7321279048919678\n",
      "第46轮第900训练step时的loss: 0.6596682071685791\n",
      "第46轮第1000训练step时的loss: 0.6808738708496094\n",
      "第46轮第1100训练step时的loss: 0.6655725836753845\n",
      "第46轮第1200训练step时的loss: 0.683769702911377\n",
      "第46轮整体测试集上的Loss: 0.6768180263792491\n",
      "第46轮整体测试集上的Accuracy: 0.5298\n",
      "-------第 47 轮训练开始-------\n",
      "第47轮第100训练step时的loss: 0.6469419598579407\n",
      "第47轮第200训练step时的loss: 0.6112260818481445\n",
      "第47轮第300训练step时的loss: 0.693081259727478\n",
      "第47轮第400训练step时的loss: 0.6668840646743774\n",
      "第47轮第500训练step时的loss: 0.713064432144165\n",
      "第47轮第600训练step时的loss: 0.6744504570960999\n",
      "第47轮第700训练step时的loss: 0.6581299304962158\n",
      "第47轮第800训练step时的loss: 0.6539302468299866\n",
      "第47轮第900训练step时的loss: 0.5707750916481018\n",
      "第47轮第1000训练step时的loss: 0.631304919719696\n",
      "第47轮第1100训练step时的loss: 0.6369794607162476\n",
      "第47轮第1200训练step时的loss: 0.6828592419624329\n",
      "第47轮整体测试集上的Loss: 0.6762825397665847\n",
      "第47轮整体测试集上的Accuracy: 0.6193\n",
      "-------第 48 轮训练开始-------\n",
      "第48轮第100训练step时的loss: 0.6705427765846252\n",
      "第48轮第200训练step时的loss: 0.6310126781463623\n",
      "第48轮第300训练step时的loss: 0.6838303208351135\n",
      "第48轮第400训练step时的loss: 0.5044676065444946\n",
      "第48轮第500训练step时的loss: 0.7520169019699097\n",
      "第48轮第600训练step时的loss: 0.53053218126297\n",
      "第48轮第700训练step时的loss: 0.711087167263031\n",
      "第48轮第800训练step时的loss: 0.6059197783470154\n",
      "第48轮第900训练step时的loss: 0.5653960704803467\n",
      "第48轮第1000训练step时的loss: 0.6664465069770813\n",
      "第48轮第1100训练step时的loss: 0.5851285457611084\n",
      "第48轮第1200训练step时的loss: 0.5893992185592651\n",
      "第48轮整体测试集上的Loss: 0.6750810190151708\n",
      "第48轮整体测试集上的Accuracy: 0.6686\n",
      "-------第 49 轮训练开始-------\n",
      "第49轮第100训练step时的loss: 0.5307208895683289\n",
      "第49轮第200训练step时的loss: 0.6228809356689453\n",
      "第49轮第300训练step时的loss: 0.6987258791923523\n",
      "第49轮第400训练step时的loss: 0.6294307112693787\n",
      "第49轮第500训练step时的loss: 0.5728833675384521\n",
      "第49轮第600训练step时的loss: 0.6378167867660522\n",
      "第49轮第700训练step时的loss: 0.4909324049949646\n",
      "第49轮第800训练step时的loss: 0.6306796073913574\n",
      "第49轮第900训练step时的loss: 0.6615934371948242\n",
      "第49轮第1000训练step时的loss: 0.6492423415184021\n",
      "第49轮第1100训练step时的loss: 0.58924400806427\n",
      "第49轮第1200训练step时的loss: 0.6916534304618835\n",
      "第49轮整体测试集上的Loss: 0.6738752434307964\n",
      "第49轮整体测试集上的Accuracy: 0.6753\n",
      "-------第 50 轮训练开始-------\n",
      "第50轮第100训练step时的loss: 0.6149300336837769\n",
      "第50轮第200训练step时的loss: 0.6031676530838013\n",
      "第50轮第300训练step时的loss: 0.5605226159095764\n",
      "第50轮第400训练step时的loss: 0.5494837760925293\n",
      "第50轮第500训练step时的loss: 0.6427802443504333\n",
      "第50轮第600训练step时的loss: 0.6117035150527954\n",
      "第50轮第700训练step时的loss: 0.6705272197723389\n",
      "第50轮第800训练step时的loss: 0.5562770366668701\n",
      "第50轮第900训练step时的loss: 0.6765597462654114\n",
      "第50轮第1000训练step时的loss: 0.6465442180633545\n",
      "第50轮第1100训练step时的loss: 0.6402568817138672\n",
      "第50轮第1200训练step时的loss: 0.6734172701835632\n",
      "第50轮整体测试集上的Loss: 0.6734609940265998\n",
      "第50轮整体测试集上的Accuracy: 0.602\n"
     ]
    }
   ],
   "source": [
    "train_step_loss = []\n",
    "valid_step_loss = []\n",
    "train_epoch_loss = []\n",
    "valid_epoch_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"-------第 {} 轮训练开始-------\".format(i+1))\n",
    "\n",
    "    # 训练步骤开始\n",
    "    myModel.train()\n",
    "    step = 0\n",
    "    for data in trainloader:\n",
    "        trData, labels = data\n",
    "        outputs = myModel(trData) # 求模型的输出\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss = loss_fn(outputs, labels)  # 求loss\n",
    "        train_step_loss.append(loss.item())\n",
    "        step += 1\n",
    "\n",
    "        if (step%100 ==0):\n",
    "            print(f'第{i+1}轮第{step}训练step时的loss: {loss.item()}')\n",
    "\n",
    "\n",
    "        # 优化器优化模型\n",
    "        loss.backward()       # 求梯度\n",
    "        optimizer.step()      # 更新参数\n",
    "        \n",
    "    train_epoch_loss.append(np.average(train_step_loss))\n",
    "\n",
    "    # 测试步骤开始\n",
    "    myModel.eval()\n",
    "    total_accuracy = 0        # 每一轮总的精确度\n",
    "    with torch.no_grad():     # 不求梯度，不更新参数\n",
    "        for data in testloader:\n",
    "            teData, teLabels = data\n",
    "            outputs = myModel(teData)\n",
    "            loss = loss_fn(outputs, teLabels)\n",
    "            valid_step_loss.append(loss.item())\n",
    "            total_accuracy = total_accuracy + correct_num(teLabels,outputs)\n",
    "\n",
    "    valid_epoch_loss.append(np.average(valid_step_loss))\n",
    "    print(f\"第{i+1}轮整体测试集上的Loss: {valid_epoch_loss[-1]}\")\n",
    "    print(f\"第{i+1}轮整体测试集上的Accuracy: {total_accuracy/len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
