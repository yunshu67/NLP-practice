{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\py37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 3.62kB [00:00, 1.81MB/s]                   \n",
      "Downloading metadata: 3.28kB [00:00, 1.64MB/s]                   \n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to C:\\Users\\adapting\\.cache\\huggingface\\datasets\\emotion\\default\\0.0.0\\348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.66M/1.66M [00:00<00:00, 5.23MB/s]\n",
      "Downloading data: 100%|██████████| 204k/204k [00:00<00:00, 2.69MB/s]\n",
      "Downloading data: 100%|██████████| 207k/207k [00:00<00:00, 2.65MB/s]\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset emotion downloaded and prepared to C:\\Users\\adapting\\.cache\\huggingface\\datasets\\emotion\\default\\0.0.0\\348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 599.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "emotions = load_dataset(\"emotion\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label'],\n    num_rows: 16000\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = emotions[\"train\"]\n",
    "train_ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 'i didnt feel humiliated', 'label': 0}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0] #训练集的的第一条数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['text', 'label']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.column_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy'], 'label': [0, 0, 3, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 14.0kB/s]\n",
      "Downloading: 100%|██████████| 483/483 [00:00<00:00, 483kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 467kB/s] \n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 949kB/s] \n"
     ]
    }
   ],
   "source": [
    "# 加载distilbert模型\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "text = 'Tokenizing text is a core task of NLP'\n",
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "30522"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "512"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['input_ids', 'attention_mask']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.19ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.04ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.19ba/s]\n"
     ]
    }
   ],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(emotions_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label', 'input_ids', 'attention_mask'],\n    num_rows: 16000\n})"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded['train']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 'i didnt feel humiliated',\n 'label': 0,\n 'input_ids': [101,\n  1045,\n  2134,\n  2102,\n  2514,\n  26608,\n  102,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded['train'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer as feature extractor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 256M/256M [00:22<00:00, 11.8MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# hide_output\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
      "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
      "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
      "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
      "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
      "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 6, 768])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "'''\n",
    "由于我们的模型需要张量作为输入，接下来要做的是将 input_ids 和 attention_mask 列转换为 \"torch\" 格式，如下所示：\n",
    "'''\n",
    "emotions_encoded.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [18:11<00:00, 68.24s/ba]\n",
      "100%|██████████| 2/2 [01:47<00:00, 53.55s/ba]\n",
      "100%|██████████| 2/2 [01:44<00:00, 52.21s/ba]\n"
     ]
    }
   ],
   "source": [
    "'''请注意，在这种情况下，我们没有设置 batch_size=None，因此使用默认的 batch_size=1000'''\n",
    "\n",
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "emotions_hidden[\"train\"].column_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask', 'hidden_state'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask', 'hidden_state'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask', 'hidden_state'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(emotions_hidden)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label', 'input_ids', 'attention_mask', 'hidden_state'],\n    num_rows: 16000\n})"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_hidden['train']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "{'label': tensor(0),\n 'input_ids': tensor([  101,  1045,  2134,  2102,  2514, 26608,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 'hidden_state': tensor([-1.1675e-01,  9.8571e-02, -1.2963e-01, -9.5018e-02, -2.8083e-01,\n         -1.3222e-01,  2.2748e-01,  2.3100e-01,  1.6752e-01, -2.4663e-01,\n          2.1755e-02, -1.0200e-01, -1.8098e-01,  2.2351e-01,  2.0275e-01,\n          1.1693e-01, -6.9531e-04,  1.2025e-01, -1.4269e-04, -6.0743e-02,\n         -1.4382e-01, -3.1201e-01, -1.0877e-01,  1.1822e-01, -7.3939e-02,\n          7.2635e-02,  2.1319e-01, -2.9570e-01,  1.0432e-01, -1.4980e-01,\n          1.1913e-01,  1.3559e-01, -2.0512e-01,  7.9983e-02, -1.2340e-01,\n          2.4995e-02,  1.1948e-02,  1.0090e-02,  1.3740e-01, -1.3440e-02,\n         -1.1266e-01, -7.7647e-02, -1.3007e-01, -7.1688e-02,  8.5297e-02,\n         -5.4750e-02, -2.1881e+00, -5.0468e-02, -3.2211e-01, -1.6245e-01,\n          3.1700e-01, -1.9576e-01,  1.3216e-01,  3.7076e-01, -5.4104e-02,\n          3.5674e-01, -1.9445e-01,  3.1695e-01, -2.2347e-02,  9.1786e-02,\n          2.9930e-01,  2.1970e-01, -1.4083e-01, -7.3253e-03, -7.7053e-02,\n          2.2294e-01, -8.2599e-02,  3.5193e-01, -1.1242e-01,  3.1867e-01,\n         -1.4907e-01, -1.1449e-01,  1.0741e-01, -1.2594e-01,  2.2771e-01,\n         -1.9048e-01,  2.4370e-02,  2.4283e-03, -1.4327e-01,  2.5054e-01,\n          2.5349e-02,  3.1793e-01,  1.8465e-01,  2.0792e-01,  1.1736e-01,\n          1.3521e-01, -1.5004e-01, -4.5670e-02,  2.1835e-01,  2.3856e-01,\n         -2.2072e-01,  1.9323e-03,  8.0047e-02,  1.8498e-01,  2.0746e-01,\n         -2.1523e-01, -6.9961e-03, -1.0508e-01,  3.2646e-01,  1.9682e-01,\n          1.6128e-02, -1.5245e-01, -1.0679e-03, -2.9763e-01, -1.0012e-01,\n         -1.2040e-01, -1.0624e-01, -2.5459e-01, -3.0247e-02, -2.4362e+00,\n          1.7284e-01,  1.0362e-01, -2.2759e-01, -4.6082e-01, -1.3025e-01,\n          4.8602e-01,  2.8673e-01, -3.7155e-02, -1.3221e-01, -2.8163e-02,\n         -4.3641e-02,  2.4612e-01,  6.2065e-02, -1.2164e-01,  8.2862e-03,\n          1.3128e-01,  9.3803e-02, -2.1647e-01, -4.1917e-02,  2.6513e-01,\n          2.8144e-01,  4.5992e-01, -3.9946e-02,  8.7393e-02, -2.1318e-01,\n         -1.2098e-02,  1.3406e-01, -7.0123e-02, -2.5529e-02, -1.7133e-01,\n         -1.7314e-01, -2.7216e-02, -2.9234e+00,  3.5353e-01,  4.1371e-01,\n         -8.3737e-02,  1.2897e-01, -5.6390e-02, -5.7990e-02,  2.1370e-01,\n          5.9577e-02,  1.4522e-01, -1.7512e-01,  6.2899e-03, -1.6081e-01,\n          1.4830e-01, -1.8694e-01,  1.0339e-01,  3.3771e-01,  2.4423e-01,\n          3.4423e-02, -1.1541e-01, -1.2788e-01,  6.0679e-03,  1.3770e-01,\n          1.4553e-01,  1.4629e-01,  2.2852e-01,  1.8765e-01, -1.0844e-01,\n          8.6822e-02, -1.6373e-01,  2.4815e-01, -8.6249e-02,  9.9065e-02,\n         -1.9758e-01,  2.9334e-01,  2.3066e-01, -7.9578e-02,  1.5847e-02,\n         -8.5749e-02,  2.3549e-01,  8.7989e-02,  1.3007e-02,  1.0539e-01,\n          1.4956e-01,  2.9645e-01, -2.9444e-01, -1.0499e-01,  2.9548e-01,\n         -3.7255e-01, -3.4156e-01, -1.2231e-01, -2.4674e-02,  3.7060e-01,\n         -1.7973e-01, -2.5541e-02, -2.5343e-01,  2.5999e-01,  1.4390e-01,\n         -2.0943e-01, -3.1637e-01,  3.1725e-03,  2.0085e-01,  9.2304e-02,\n          3.6130e+00, -6.6018e-02, -1.0338e-01,  1.6810e-01,  2.7515e-01,\n         -6.4263e-02, -3.5365e-02, -4.3326e-02, -9.8998e-02,  1.1921e-01,\n         -5.7456e-02,  1.9335e-01, -8.8367e-02,  7.8326e-02, -2.8946e-02,\n          2.6095e-01,  1.9681e-02, -1.1885e-01,  1.1066e-02, -1.3633e-01,\n          1.7394e-01,  7.5209e-03,  1.8244e-01, -1.2067e-03, -1.2601e+00,\n         -1.2663e-01, -8.0744e-02, -3.6575e-02,  2.9806e-01, -2.4624e-01,\n         -4.0191e-02,  1.9576e-01, -2.0116e-03,  4.4442e-02,  1.5877e-01,\n         -1.5320e-01,  1.6113e-01,  3.3971e-01,  2.3434e-01, -4.0033e-01,\n          4.2836e-01,  3.2571e-01,  1.7000e-02,  1.1251e-01, -3.0598e-02,\n          3.0774e-01, -2.6341e-02, -1.3959e-01, -1.5008e-01, -8.3282e-03,\n          8.5082e-02,  1.9154e-01, -3.9110e-02, -2.4176e-01, -3.0225e-01,\n         -1.2867e-01,  1.4362e-01,  3.3107e-01,  2.0546e-01, -1.6152e-01,\n         -1.1428e-01,  5.1518e-02, -2.0517e-01,  2.1981e-01,  1.1049e-01,\n          9.3059e-03, -7.7614e-02, -2.4772e-01, -3.7827e+00, -1.6221e-01,\n         -3.9238e-03,  2.9230e-01,  3.3593e-01, -2.4887e-01, -5.5248e-02,\n          7.0810e-02,  3.3218e-01, -4.9164e-01,  3.3249e-01,  2.5237e-01,\n          1.0302e-01,  8.0644e-02, -5.0636e-01,  3.0274e-01, -1.0587e-01,\n         -3.8474e-02,  7.9369e-02,  9.6534e-02, -2.4516e-02,  2.7989e-01,\n         -2.2132e-02,  9.4404e-02,  4.0250e-02,  7.7165e-02,  3.7470e-02,\n         -2.4932e-01,  7.0242e-02, -8.8952e-02,  2.6193e-02, -2.2170e-01,\n          1.9273e-01, -1.0276e-01, -1.0124e-01, -2.6291e+00, -1.9206e-01,\n          7.4273e-02, -1.8577e-01,  1.7684e-01, -6.8023e-02,  7.8900e-02,\n         -1.3103e-01, -2.6625e-01,  1.5779e-01,  1.9868e-01, -2.7853e-01,\n          9.3142e-02,  2.1580e-01,  4.0338e-01, -1.4951e-02,  3.3463e-01,\n         -3.3864e-01,  7.5545e-02,  2.7307e-01,  1.2786e-03,  1.0240e-01,\n         -3.8401e-02, -1.5953e-01,  1.9380e-01,  3.9973e-01, -2.1533e-01,\n          8.1784e-02, -2.3262e-01, -1.1567e-02, -2.4127e-02, -1.6590e-01,\n          1.2437e-01,  7.0064e-02, -4.4675e-02, -1.9401e-01,  1.2804e-01,\n          1.7199e-01,  9.4083e-02, -1.2295e-01, -1.5975e-01,  3.0723e-01,\n          6.7893e-04,  1.4044e-01,  3.0890e-01,  7.9523e-02, -8.0768e-02,\n         -1.0552e-01, -7.2691e-02,  2.0906e-01,  8.0737e-02,  1.5862e-01,\n          1.0828e+00,  3.6031e-02,  2.8098e-01, -2.1936e-01,  1.3105e-01,\n          1.2379e-01, -9.4350e-02,  1.0612e-01,  1.5117e-01, -2.0615e-02,\n          1.0589e-01, -9.1424e-02, -2.0041e-03, -1.7689e-01,  1.6056e-01,\n         -3.1062e-01,  4.0972e-02,  2.9513e-01,  4.0659e-02,  3.2724e-02,\n         -3.4728e-02, -8.8261e-01, -2.4540e-01,  3.3993e-01, -1.0448e-01,\n         -4.5713e-02, -7.8249e-02, -2.9215e-02, -3.2579e-01,  5.1095e-02,\n         -1.0748e-01,  1.5851e-01, -1.3380e-01,  4.6656e-02, -1.2103e-02,\n          2.1974e-02, -2.9675e-01, -5.9242e-02,  1.4628e-02,  2.8186e-01,\n          2.0580e-01,  2.3218e-01,  1.2978e-01,  2.5792e-02,  2.5988e-01,\n         -6.6988e-01,  5.8503e-02, -2.1245e-01, -2.7290e-02, -2.0058e-01,\n         -2.4012e-01, -2.4584e-02, -2.6802e-01, -2.5739e-01, -3.3969e-01,\n          4.3345e-01,  2.0909e-02,  1.2613e-01, -8.7278e-02,  1.1038e-01,\n          1.7168e-02,  2.2594e-01,  9.0269e-01, -1.5640e-01,  6.0410e-02,\n          2.3355e-01,  3.6283e-02,  3.4390e-01,  3.2939e-01,  1.7398e-01,\n         -2.8532e-01, -3.9076e-02, -1.8436e-01,  1.4601e-01, -8.3952e-02,\n         -4.3313e-01, -3.7668e-01, -1.4026e-01,  3.6117e-01,  1.2553e-01,\n         -2.4964e-01, -5.6666e-01,  8.8820e-02, -3.6136e-01, -5.4826e-02,\n          1.0458e-01,  4.3815e-02,  1.0217e-01,  3.8961e-01, -1.8932e-02,\n         -1.4536e-01,  2.5490e-01, -9.3461e-02,  4.4798e-01,  1.1848e-02,\n         -8.3822e-02, -4.0043e-02,  4.4775e-01, -1.4895e-01, -3.0659e-01,\n          2.0100e-01, -1.4515e-01,  1.4737e-01,  2.1594e-01,  1.4363e-02,\n         -1.3897e-01, -9.5100e-02, -4.7474e-03,  5.4959e-02,  4.5180e-02,\n         -1.2655e+00,  4.2146e-01,  2.5284e-01, -1.4166e-01,  1.3394e-01,\n         -9.8006e-02, -2.3300e-01,  2.7161e-01,  1.8300e-01, -5.6430e-02,\n         -7.3742e-02,  2.7368e-02, -1.3798e-01, -4.4316e-02, -1.3424e-01,\n          1.2901e-01, -1.3098e-02, -6.6707e-02, -8.6357e-02,  7.8332e-02,\n          1.2239e-02,  1.4193e-01,  2.4964e-01, -1.3928e-01, -8.1319e-02,\n         -3.3757e-02,  3.2082e-02,  2.4475e-01,  5.9170e-02, -1.0659e-01,\n          8.2136e-02, -3.4662e-01, -4.0611e-01, -1.0537e-01,  2.2505e-01,\n         -1.5104e-01, -5.5448e-02,  1.8810e-01,  2.5277e-01,  3.6688e-02,\n         -2.8979e-01,  2.7794e-01,  6.1990e-02, -1.0782e-01,  3.5069e-01,\n          1.8230e-01, -2.0973e-01,  2.7432e-01,  2.6626e-02, -1.9963e-01,\n         -2.3292e-01, -5.3275e-02,  1.9217e-02, -1.3250e-01,  2.0585e-03,\n          2.5716e-02,  6.3876e-02,  1.6857e-01, -1.4395e-01,  5.7093e-02,\n          5.3942e-02, -2.0077e-01, -1.0080e-01,  1.6863e-01, -3.3170e-01,\n         -5.3853e-01, -2.8106e-01, -1.9976e-01, -1.4294e-01, -1.3859e-01,\n          3.8894e-01,  1.6479e-01, -5.3734e-02,  1.3637e-01,  3.0068e-02,\n          3.0893e-01,  1.4328e-02,  1.6129e-01,  3.1139e-01, -1.6568e-01,\n         -7.9658e-03, -2.0958e-01, -1.0281e-01,  3.5324e-01, -1.0567e-01,\n         -5.8556e-02,  9.3369e-02,  1.1648e-01, -1.3798e-01, -2.3851e-01,\n         -3.8207e-01, -9.8543e-02, -9.3467e-02,  1.2520e-01, -1.1062e-01,\n          1.6582e-01,  1.0818e-01, -1.9717e-01, -1.3488e-01, -5.4721e-02,\n         -9.3402e-02,  2.6360e-01,  1.2385e-01,  1.8985e-01,  1.4471e-01,\n          3.2649e-01,  2.9378e-01,  1.8079e-01, -5.2757e-01, -2.1517e-01,\n         -2.0087e-01, -1.6715e-02, -1.5241e-01, -9.6169e-02, -1.5351e-01,\n         -1.9263e-01,  2.7597e-02, -3.5116e-01,  1.9531e+00,  2.1804e-01,\n          2.7455e-01, -3.0382e-02,  2.3042e-02, -1.1040e-01,  6.8084e-02,\n          1.6819e-01, -2.3985e-01,  2.7489e-01, -1.1664e-01,  9.8606e-02,\n         -2.6994e-02,  9.6058e-02,  4.1689e-01,  1.1756e-01, -8.4058e-02,\n         -9.0461e-03, -7.1376e-01, -1.9147e-01, -2.6122e-01,  7.8427e-02,\n          4.5298e-01, -1.3170e-02,  1.6693e-01,  2.4969e-01,  6.1676e-02,\n         -2.0383e-01,  6.2522e-02,  9.2148e-02, -7.6351e-02, -3.1085e-02,\n          1.3661e-01,  5.7010e-02, -3.3863e-01, -4.9668e-02,  1.0856e-01,\n         -3.8452e-01, -2.1216e-01, -2.5531e-02, -6.1672e-02, -1.1058e-01,\n          4.3710e-01,  1.5805e-01, -2.1765e-02,  3.3705e-01, -1.4778e-01,\n         -2.0211e-01,  3.8775e-01,  2.1390e-01,  1.2601e-02, -7.4676e-02,\n         -1.5167e-01,  2.6140e-01, -6.3670e-03, -3.6041e-02,  1.2627e-01,\n         -1.3348e-01,  1.1298e-01,  2.3918e-01,  4.6505e-02,  2.7387e-01,\n          2.6779e-01, -1.3219e-01, -4.0080e-02,  2.2250e-01, -3.3824e-02,\n         -1.5152e-01,  1.3421e-01, -5.9236e-02, -9.0817e-03,  2.5004e-01,\n          1.5880e-01,  2.3925e-01, -1.4099e-01,  9.5128e-02,  4.6482e-01,\n          6.5462e-02, -1.3512e-01, -2.8705e+00,  1.6414e-01,  6.5447e-02,\n          8.0043e-02,  1.4024e-01,  2.4770e-01,  1.0595e-01, -1.6517e-01,\n          2.5845e-01, -9.2091e-02,  1.2284e-01,  2.6322e-01,  2.9493e-01,\n          1.0018e-01,  1.9750e-01,  1.7573e-01,  1.6417e-01,  5.8261e-02,\n         -2.6669e-01, -2.3926e-01, -1.7427e-01,  1.8758e-01, -4.5343e-02,\n         -2.7610e-01, -3.8304e-01,  2.2491e-01, -1.4778e-01, -1.9875e-02,\n          2.3241e-01,  1.8489e-01, -5.0066e-02,  3.7768e-01,  2.7042e-02,\n          2.4892e-01,  4.1690e-02, -3.0728e-01, -2.4694e-01, -7.6801e-02,\n          1.0235e-02,  1.6753e-02, -1.2469e-01,  2.5393e-01,  1.6792e-01,\n         -6.4098e-02,  6.9654e-02, -1.6327e-01,  3.7457e-01, -8.9203e-02,\n          1.6170e-01, -2.9383e-01, -2.3540e-02,  1.8495e-02,  4.3380e-02,\n         -3.0954e-02,  2.0399e-01,  5.0104e-02,  7.0994e-02, -4.5955e-02,\n         -9.4303e-03, -1.8107e-01,  1.5580e-01,  1.4979e-01,  1.7684e-01,\n          2.7868e-04,  3.7581e-01, -1.5818e-01, -1.0548e-01,  2.7775e-01,\n          7.5982e-03, -2.9627e-02, -9.5440e-02, -1.7093e-01,  6.6050e-02,\n          1.2896e-01, -4.7792e-03, -1.5916e-02,  3.1259e-01,  1.7665e-01,\n          6.7563e-02,  9.1883e-04, -1.7901e-01, -1.0783e-02,  4.9254e-02,\n         -2.6314e-01,  2.9999e-01, -7.6389e+00, -1.7126e-01,  8.9796e-02,\n         -4.5319e-01, -1.0781e-01, -1.0774e-01,  2.9627e-01, -1.7547e-01,\n          1.3433e-01, -2.7305e-01,  2.3752e-01, -1.0240e-02, -6.7025e-02,\n          5.8711e-02,  3.5433e-01,  4.0421e-01])}"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_hidden['train'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "{'label': tensor(0),\n 'input_ids': tensor([  101,  1045,  2064,  2175,  2013,  3110,  2061, 20625,  2000,  2061,\n          9636, 17772,  2074,  2013,  2108,  2105,  2619,  2040, 14977,  1998,\n          2003,  8300,   102,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 'hidden_state': tensor([-3.2363e-02, -3.2315e-02, -1.9573e-01, -3.4685e-01, -1.4316e-01,\n         -1.8670e-01,  2.5069e-01,  4.1114e-01, -5.9232e-02, -4.2388e-01,\n         -2.0157e-02, -3.9641e-02, -1.0070e-01,  4.1125e-01,  2.4159e-01,\n          2.3955e-01,  2.1609e-01,  2.3275e-01,  1.1953e-01, -1.1994e-01,\n          1.3943e-02, -4.5771e-02, -1.1776e-01,  1.0547e-01,  5.8089e-03,\n         -9.5805e-02,  9.9802e-02, -1.3250e-01,  2.4869e-01, -1.2159e-01,\n          8.7682e-02,  8.4267e-02, -1.7338e-01, -2.6159e-01,  1.1753e-01,\n         -1.7384e-01,  6.9733e-02, -3.5587e-01, -5.5494e-02,  8.8954e-03,\n         -6.7488e-02, -5.9667e-02, -1.4449e-02, -9.3042e-02, -1.0468e-01,\n         -2.1275e-01, -2.4294e+00, -1.9339e-01,  1.5907e-02, -1.2355e-01,\n          2.2788e-01, -1.3114e-01,  2.2353e-01,  2.7558e-01,  4.4254e-01,\n          3.5904e-01, -2.0713e-01,  2.9906e-01, -8.8740e-02,  1.7581e-01,\n          2.9491e-01,  1.1377e-01, -1.9814e-01, -2.1774e-01, -1.2698e-01,\n          1.3146e-01, -8.2443e-02,  6.7246e-02,  1.1928e-02,  2.4722e-01,\n         -2.9221e-01, -1.5208e-01,  2.1010e-01, -2.2425e-01, -6.6253e-02,\n          7.1991e-02, -1.2079e-01,  1.9187e-01,  2.3885e-02,  1.6900e-01,\n         -3.2450e-02,  5.3360e-01,  2.1752e-01,  1.2463e-01,  1.6518e-01,\n         -4.9254e-03, -1.9452e-01,  1.5104e-02,  2.7150e-01,  3.2401e-01,\n         -3.2098e-01, -1.4943e-01,  4.1249e-02,  4.2086e-01,  3.7692e-01,\n         -3.7616e-01,  6.3367e-02,  5.0809e-03,  1.1479e-01,  2.6863e-01,\n          1.9308e-01,  1.3918e-02, -2.5699e-02, -3.3208e-01,  1.8180e-01,\n         -2.0033e-02, -2.1946e-02, -3.1180e-01,  9.5574e-02, -2.4643e+00,\n         -1.3006e-02,  5.8032e-02, -3.8411e-01, -2.5247e-01, -1.4174e-01,\n          2.0100e-01,  2.9318e-01, -1.4643e-02,  1.7975e-01, -4.7881e-02,\n          1.0032e-01,  3.8879e-02, -1.1792e-01, -2.5434e-01,  2.1956e-02,\n          8.7548e-02,  2.7915e-04, -3.4929e-01,  1.1710e-01,  3.1832e-01,\n          2.8725e-01,  5.6664e-01, -3.5749e-02, -3.9447e-01, -2.7608e-01,\n          3.0512e-01,  2.2496e-01, -8.3492e-02, -1.3302e-01, -1.0493e-01,\n         -3.4879e-01, -1.3447e-01, -2.9961e+00,  3.8708e-01,  3.5667e-01,\n         -2.9118e-03,  9.6836e-02,  9.8761e-02,  4.9789e-02,  1.9608e-01,\n          2.1150e-02, -3.7135e-02, -1.5095e-01, -1.6561e-01, -1.5036e-01,\n         -1.8409e-01, -4.2267e-01,  7.5131e-02,  5.1064e-01,  1.2389e-01,\n          1.4039e-01, -2.2593e-01, -1.5750e-01, -6.3807e-02, -3.2204e-03,\n          1.4111e-01,  3.6827e-01,  2.3586e-01, -1.1572e-02, -2.7649e-02,\n          1.6840e-01,  9.8716e-02,  3.0096e-01, -1.9226e-01,  1.8726e-01,\n         -1.7671e-01,  1.8455e-01,  3.6911e-01,  1.6331e-01, -1.2193e-01,\n          3.8474e-02,  4.0548e-01,  1.4172e-01,  1.1013e-01, -3.1064e-02,\n         -1.9506e-01,  1.8380e-02, -1.9592e-01, -2.1119e-01,  3.0078e-01,\n         -1.4430e-01, -3.6476e-01,  5.5558e-03, -1.9080e-02,  2.4370e-01,\n         -2.8861e-01,  3.0572e-02, -3.4631e-01,  2.2572e-01,  9.5500e-02,\n         -2.7844e-01, -1.3220e-01, -2.1319e-01,  2.8476e-01, -1.3547e-01,\n          3.9232e+00,  3.4839e-01, -5.9003e-02, -2.6205e-02, -2.9968e-03,\n          2.2502e-02,  9.4241e-02, -1.6755e-01, -8.2101e-02, -9.6528e-03,\n         -2.4379e-01, -1.2945e-02,  1.4131e-01,  9.9184e-02,  2.0106e-02,\n          2.9429e-01,  2.8387e-01, -1.5112e-01,  7.8345e-02,  5.7670e-02,\n          2.0591e-01, -1.6702e-01,  1.8255e-02,  5.3760e-02, -1.2703e+00,\n          1.7877e-02, -3.3600e-03, -4.1509e-01,  4.2608e-01, -2.3226e-01,\n         -1.4717e-02,  1.4548e-01, -5.7566e-02,  1.6313e-01,  1.3694e-01,\n         -2.3794e-01,  9.5089e-02,  4.5699e-01,  2.7264e-01, -3.2696e-01,\n          5.5342e-01, -6.2474e-02,  5.6503e-02, -9.1001e-02,  1.5822e-01,\n          2.5594e-01, -1.4641e-03, -1.8601e-01, -1.5978e-01, -4.0995e-02,\n          1.9788e-01, -6.5133e-03,  1.3075e-01, -2.9486e-01, -1.6574e-01,\n         -3.0278e-01, -5.2985e-02,  2.0301e-01,  1.3135e-01, -5.0210e-01,\n         -5.5230e-02, -2.5141e-01, -1.5812e-01,  5.4874e-02,  2.0389e-01,\n         -6.7808e-02, -6.4947e-03, -3.2346e-01, -3.5327e+00, -2.7854e-01,\n         -6.1734e-02,  2.0776e-01,  3.2139e-01, -1.1180e-01,  2.3447e-02,\n          3.3499e-01,  1.7371e-01, -3.3498e-01,  3.0946e-01,  2.2188e-01,\n          3.7211e-02,  2.7146e-01, -3.5864e-01,  1.7339e-01,  7.6873e-02,\n         -1.9965e-01,  8.1900e-02, -4.5180e-02, -9.8939e-02,  2.6206e-01,\n         -2.3374e-01,  2.8962e-01,  9.5962e-02,  1.8446e-01, -2.5489e-01,\n         -1.1605e-01, -7.7801e-02,  9.6565e-02,  2.0052e-03, -1.2453e-01,\n          2.3056e-01,  1.3386e-02, -2.6207e-01, -2.8089e+00,  1.4496e-01,\n         -2.7916e-01, -2.2885e-01,  1.0184e-01,  4.2645e-02,  2.1380e-01,\n         -1.5723e-01, -3.0251e-01,  4.8683e-02,  1.7105e-01,  2.1128e-02,\n          1.2515e-01,  5.9567e-02,  1.7588e-01, -1.6883e-01,  6.7289e-02,\n         -1.6565e-01,  9.5153e-02,  1.9953e-01, -3.2374e-02,  2.8306e-01,\n          1.6084e-03, -8.4044e-02,  1.1721e-01,  3.9836e-01, -3.0307e-01,\n         -2.6540e-02, -9.9892e-02,  4.8432e-02, -6.1414e-02, -2.2307e-01,\n          7.9354e-02, -1.6981e-02,  2.8046e-02,  4.7983e-03, -1.3553e-01,\n          1.7427e-01,  2.5030e-01, -8.4254e-02, -1.2365e-01,  1.8300e-01,\n          8.0413e-02,  2.2101e-01,  6.9440e-01,  1.2489e-01,  5.3940e-02,\n         -2.5113e-02,  2.9862e-03,  1.5615e-01,  2.8373e-01,  3.3189e-02,\n          1.2066e+00, -2.9998e-03,  3.1911e-01, -2.2009e-01,  3.0803e-01,\n         -8.6546e-02, -2.3242e-01,  2.1859e-01,  2.3041e-01, -1.0725e-01,\n          1.2996e-01, -9.8805e-02,  1.0791e-01, -4.7234e-01,  1.6917e-01,\n         -4.9113e-01,  8.6275e-02,  2.4879e-01, -1.2433e-02,  9.8178e-02,\n         -1.5901e-01, -8.0131e-01,  9.0409e-02,  7.3910e-02, -1.1814e-01,\n          1.4955e-01,  6.1251e-02,  3.4894e-02, -2.1147e-01,  6.6339e-02,\n         -7.6250e-02,  1.3709e-01, -9.9186e-02, -1.2864e-01, -7.5221e-02,\n          2.5280e-01, -1.2432e-01, -1.1130e-02, -7.9391e-02,  2.3811e-01,\n          2.9994e-01,  2.4303e-02,  2.4630e-01,  1.4831e-01,  1.4405e-01,\n         -7.9035e-01,  8.2464e-02, -1.3351e-01,  6.8420e-02, -2.2729e-01,\n         -3.2650e-01, -1.3800e-01, -2.0347e-01, -3.9290e-01, -2.6333e-01,\n          4.4223e-01, -3.9947e-02,  2.7687e-02, -1.9609e-01, -4.2425e-02,\n          1.8013e-01, -2.2207e-01,  8.1898e-01, -2.5018e-01, -1.6685e-02,\n          5.7030e-01,  9.8938e-03,  9.1869e-02,  1.8714e-01,  1.6144e-01,\n         -1.1553e-01, -1.0705e-01, -3.5834e-01,  5.5081e-02, -1.3225e-01,\n         -2.7339e-01, -2.2988e-01,  1.6887e-02,  1.3962e-01,  6.6491e-02,\n         -2.1425e-01, -4.2997e-01, -2.5190e-01, -3.5628e-01, -1.8398e-01,\n          1.4981e-01,  1.3693e-01,  1.4936e-01,  3.7565e-01,  1.1498e-01,\n         -1.4190e-01,  9.5016e-02, -3.3526e-01,  4.6075e-01, -1.8627e-02,\n          1.0298e-01, -1.8657e-01,  1.0041e-01, -8.9180e-02, -9.3249e-02,\n          9.0621e-02, -4.0994e-01,  3.2926e-01,  2.1664e-01, -8.4818e-02,\n         -1.3015e-02,  1.0069e-01,  1.8197e-01,  1.4961e-01, -1.6361e-02,\n         -1.2735e+00,  1.5583e-01,  2.3457e-01,  9.5818e-02,  8.0692e-02,\n         -6.9856e-02, -4.9208e-02,  4.7123e-01,  2.2048e-01, -2.3500e-01,\n         -4.6125e-02, -1.7010e-01, -9.1555e-02,  2.6261e-01, -7.7647e-02,\n         -1.0787e-01,  3.7506e-02, -9.2822e-03, -1.9176e-01, -1.1883e-01,\n          5.8560e-03,  1.3160e-01,  2.5325e-01,  1.4179e-01,  1.0937e-01,\n         -9.7511e-02,  4.1766e-02,  1.3216e-01,  2.1235e-02,  2.0424e-01,\n          3.2712e-02, -5.3709e-01, -5.5918e-01, -4.0925e-01,  2.1721e-01,\n          5.3103e-02,  1.5933e-01, -3.7019e-02,  3.4710e-01, -1.2423e-02,\n         -5.7815e-01,  3.1686e-01,  3.4354e-01, -1.8817e-01,  5.1454e-01,\n          2.6232e-01, -8.2333e-02,  2.6699e-01, -1.4747e-01, -2.2389e-01,\n         -1.4730e-02,  9.1033e-02, -3.9581e-02,  8.6602e-02,  1.5251e-03,\n         -1.2806e-01, -1.1854e-01,  3.2068e-01, -1.5429e-02,  1.2745e-01,\n          2.6673e-01, -1.6171e-01, -3.0541e-01,  1.3028e-01, -1.3319e-01,\n         -4.5360e-01, -2.1711e-01, -1.2285e-01,  9.5393e-02, -5.7908e-02,\n          3.9164e-01, -1.7015e-01, -1.9278e-01, -7.6761e-02, -6.3805e-02,\n          1.1381e-01, -3.4599e-02,  2.8761e-01,  1.7027e-01, -2.0973e-01,\n          7.8664e-02, -2.2743e-01, -1.1671e-01,  3.5298e-01, -7.6573e-02,\n          6.4515e-02, -2.4224e-01, -2.2167e-01,  1.1938e-02, -2.3110e-01,\n         -2.5532e-01, -1.0334e-01,  2.0210e-01,  7.5716e-02, -3.3214e-01,\n          6.7232e-03,  1.6973e-01, -2.3449e-01,  2.3183e-01, -1.4061e-01,\n         -2.3034e-01,  3.3035e-01,  3.6622e-01,  2.7070e-01, -1.9717e-01,\n          4.9775e-01,  2.1718e-01,  1.3704e-02, -2.7897e-01,  6.8965e-02,\n         -3.8398e-02, -3.3899e-02, -1.0665e-01,  1.4025e-01, -6.4827e-03,\n          8.0744e-02, -5.5055e-02, -5.0765e-01,  1.8079e+00,  1.9107e-01,\n          1.5398e-01, -1.0896e-02,  3.9533e-01,  6.8298e-03,  1.5448e-02,\n          1.6804e-01, -1.5718e-01,  2.0427e-01,  5.3636e-02,  5.8618e-02,\n         -7.4269e-02,  1.0862e-01,  3.4117e-01,  1.1464e-01,  1.4032e-01,\n         -6.2577e-02, -2.2473e-01, -1.5737e-01, -3.6006e-01,  2.7460e-01,\n          2.7903e-01, -1.2135e-01,  5.6340e-02,  1.5499e-01,  1.0939e-01,\n         -1.9718e-01, -7.1753e-02,  1.6505e-01,  1.4730e-01,  1.7620e-01,\n          1.9170e-01,  2.0688e-01, -1.6068e-01, -1.1722e-02,  2.3808e-01,\n         -2.2527e-01,  2.5158e-04,  1.3600e-01, -8.2352e-02, -2.0649e-01,\n          4.3867e-01, -6.5850e-04,  7.4870e-03,  3.0635e-01, -1.9334e-01,\n         -1.3173e-01,  2.6370e-01,  3.1385e-01, -9.6601e-02, -9.8468e-02,\n         -5.2094e-02,  5.9465e-01, -1.0858e-01, -6.5256e-02, -3.3894e-02,\n         -1.4234e-01, -2.1142e-01,  2.5278e-01, -1.4968e-01,  6.4407e-02,\n          2.1339e-01,  2.5200e-01,  6.7391e-02,  1.7276e-01, -9.8558e-02,\n          2.1815e-01,  1.5861e-01, -1.9862e-01, -6.0561e-02,  2.9042e-01,\n          4.3507e-01, -6.1583e-02, -2.2120e-01,  2.4134e-01,  7.2879e-01,\n          2.4233e-01, -2.2077e-01, -2.6636e+00,  6.4031e-02, -7.6370e-02,\n          1.8341e-01,  2.4225e-02,  3.2587e-01,  9.6426e-02,  6.7694e-03,\n          1.5096e-02, -2.0244e-01,  3.4721e-02,  1.7958e-01,  5.9637e-01,\n          7.2577e-02,  2.8754e-01,  8.0446e-02,  1.0961e-01, -1.0235e-01,\n         -1.7555e-01, -2.1619e-01, -1.0625e-01,  2.1853e-01,  2.9135e-02,\n         -2.8051e-01, -4.6491e-01, -8.6035e-03, -4.2722e-02, -5.9642e-02,\n          8.9532e-02,  2.8561e-01, -1.3805e-02,  4.5919e-01, -9.1658e-02,\n          4.2887e-02, -1.3990e-02, -7.6439e-02, -2.8554e-01, -2.1884e-01,\n         -1.7759e-01,  2.4502e-01, -1.2247e-01,  2.4560e-01,  1.0852e-01,\n         -8.1113e-02,  2.3885e-01, -1.8960e-01,  4.5441e-01, -1.7379e-01,\n          2.1045e-01, -5.5977e-01, -6.8152e-02,  1.5432e-01,  3.0034e-01,\n         -1.3819e-01,  1.4856e-01,  1.2730e-01,  2.4443e-01,  1.7630e-01,\n          4.9472e-02, -1.4877e-01, -1.2619e-01,  2.5861e-01,  1.1439e-02,\n          9.9326e-02,  4.6417e-01, -2.2476e-01, -3.8831e-01,  1.4358e-01,\n         -2.0881e-01, -1.0024e-01, -1.5332e-01, -4.3518e-01,  2.0353e-01,\n          1.3124e-01,  4.9282e-02, -5.1823e-02,  1.6797e-01,  4.8978e-01,\n          8.7836e-02,  5.1876e-02, -9.7926e-02,  5.8313e-04, -4.3655e-03,\n         -2.4450e-01,  1.0515e-01, -7.0325e+00, -8.3830e-02, -1.5478e-01,\n         -2.6934e-01, -1.2383e-01, -2.7802e-01,  2.1056e-01, -3.0141e-01,\n          1.8936e-01, -2.7609e-01,  2.3780e-01,  4.1381e-02, -5.0164e-02,\n         -1.7466e-01,  3.5464e-01,  3.0277e-01])}"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_hidden['train'][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Classification Layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "((16000, 768), (2000, 768))"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "X_train.shape, X_valid.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=3000)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We increase `max_iter` to guarantee convergence\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "0.633"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(X_valid,y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}